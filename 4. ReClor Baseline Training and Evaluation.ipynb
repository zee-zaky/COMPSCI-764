{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71614c8c-099c-4c44-91f3-5cd0ea933243",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebea513d-21f8-4f2a-b64e-e52e6acbed93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project libraries imported!\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "Device:cuda\n"
     ]
    }
   ],
   "source": [
    "from project_imports import *\n",
    "import use_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95e38-f23f-4972-a8cb-b4c1849cb726",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb17c692-f274-4f42-8985-1b42f4f1cfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Arguments and global vriables\n",
    "dataset_name=\"ReClor\"\n",
    "pretrained_model_name = \"microsoft/deberta-v3-base\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "data_collator = DefaultDataCollator()\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = right_padding = tokenizer.padding_side == 'right'\n",
    "global_counter = 0\n",
    "traing_answer_mismatches = []\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5d0e7-f055-4058-986f-f82edc6fce8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Prepare the AR-LSAT Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38eef2e-4a62-434a-a3df-31a558fc1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1072514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 118521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 200566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('cleaned_dataset')\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b14e4b-8c9a-4cf4-817e-e8b7099bf1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the dataset to only include AR-LSAT data\n",
    "reclor_train = combined_dataset['train'].filter(lambda x: x['Source Dataset'] == 'ReClor')\n",
    "reclor_val = combined_dataset['validation'].filter(lambda x: x['Source Dataset'] == 'ReClor')\n",
    "reclor_test = combined_dataset['test'].filter(lambda x: x['Source Dataset'] == 'ReClor')\n",
    "\n",
    "\n",
    "# Concatenate test data into the training data\n",
    "reclor_train = concatenate_datasets([reclor_train, reclor_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8169543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for multiple-choice tasks\n",
    "def mcqa_preprocess_function(examples):\n",
    "    num_choices = num_choices = len(examples['Options'][0])    \n",
    "    first_sentences = [[context] * num_choices for context in examples['Context']]  # Repeat context for each option\n",
    "    question_headers = examples['Question']\n",
    "    options_list = examples['Options']\n",
    "    \n",
    "    second_sentences = []\n",
    "    for question, options in zip(question_headers, options_list):\n",
    "        # Combine question with each option\n",
    "        second_sentences.append([f\"{question} {option}\" for option in options])\n",
    "    \n",
    "    # Flatten the lists\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    \n",
    "    # Tokenize the inputs\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    # Un-flatten the tokenized inputs to have shape (num_examples, num_choices, seq_length)\n",
    "    tokenized_inputs = {k: [v[i:i + num_choices] for i in range(0, len(v), num_choices)] for k, v in tokenized_examples.items()}\n",
    "    \n",
    "    # Labels\n",
    "    tokenized_inputs[\"labels\"] = examples[\"Label\"]\n",
    "    \n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b105a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcqa_preprocess_function(examples):\n",
    "    # Determine the maximum number of choices\n",
    "    max_num_choices = 5  \n",
    "    contexts = examples['Context']\n",
    "    questions = examples['Question']\n",
    "    options_list = examples['Options']\n",
    "    labels = examples['Label']\n",
    "    \n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    labels_adjusted = []\n",
    "    \n",
    "    for context, question, options, label in zip(contexts, questions, options_list, labels):\n",
    "        num_choices = len(options)\n",
    "        # Pad options to have max_num_choices\n",
    "        if num_choices < max_num_choices:\n",
    "            options += [''] * (max_num_choices - num_choices)\n",
    "        first_sentences.append([context] * max_num_choices)\n",
    "        second_sentences.append([f\"{question} {option}\" for option in options])\n",
    "        labels_adjusted.append(label)\n",
    "    \n",
    "    # Flatten the lists\n",
    "    first_sentences = [item for sublist in first_sentences for item in sublist]\n",
    "    second_sentences = [item for sublist in second_sentences for item in sublist]\n",
    "    \n",
    "    # Tokenize the inputs\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    # Un-flatten to shape (num_examples, max_num_choices, seq_length)\n",
    "    tokenized_inputs = {\n",
    "        k: [v[i:i + max_num_choices] for i in range(0, len(v), max_num_choices)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Labels\n",
    "    tokenized_inputs[\"labels\"] = labels_adjusted\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944988bb-97a9-4daf-80a6-743daea9b62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply the preprocessing function to the combined datasets\n",
    "encoded_reclor_train = reclor_train.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_reclor_val = reclor_val.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_reclor_test = reclor_test.map(mcqa_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27035bf4-f8bd-4e10-8b8e-80e6c33cb9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the format of the datasets to PyTorch tensors\n",
    "encoded_reclor_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_reclor_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_reclor_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "def get_train_encoded():\n",
    "    return encoded_reclor_train\n",
    "\n",
    "def get_val_encoded():\n",
    "    return encoded_reclor_val\n",
    "\n",
    "def get_test_encoded():\n",
    "    return encoded_reclor_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1af4-7b57-4c74-bcd9-48edefba6747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20dd2c65-8677-41a6-b771-c8bbf786a233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'eval_accuracy': acc, 'eval_f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5911d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_args(run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=3):\n",
    "    \"\"\"\n",
    "    Generates training arguments for training a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): The name of the dataset.\n",
    "    - run_name (str): The name of the run, useful for logging and saving models.\n",
    "    - model_name (str): The name of the model, typically including its configuration.\n",
    "    - num_train_epochs (int): The number of epochs to train for.\n",
    "    - learning_rate (float): The learning rate for training.\n",
    "    - batch_size (int): The batch size used for training.\n",
    "\n",
    "    Returns:\n",
    "    - TrainingArguments: A configured TrainingArguments instance.\n",
    "    \"\"\"    \n",
    "    output_dir = f\"./{dataset_name}/{run_name}/{normalized_model_name}\"\n",
    "\n",
    "    if run_name==\"Optuna\":\n",
    "        training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=1.5807103066634623e-05,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,        \n",
    "        warmup_ratio = 0.5994150649377659,\n",
    "        weight_decay=0.12506835879573128,\n",
    "        adam_beta1=0.8136227307274486,\n",
    "        adam_beta2=0.9924116710027883,\n",
    "        adam_epsilon=1.9858068243318367e-07,\n",
    "        lr_scheduler_type='cosine_with_restarts',\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    )\n",
    "    else:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            report_to=\"none\",  # Disable all integrations\n",
    "            overwrite_output_dir=True,\n",
    "            metric_for_best_model='eval_accuracy',\n",
    "            greater_is_better=True,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=3,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=4,\n",
    "            warmup_steps=398,\n",
    "            weight_decay=0.194,\n",
    "            adam_beta1=0.837,\n",
    "            adam_beta2=0.997,\n",
    "            adam_epsilon=5.87e-07,\n",
    "            lr_scheduler_type='cosine',\n",
    "            fp16=True,  # Enable mixed-precision training\n",
    "        )\n",
    "    \n",
    "    return training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05874a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to stop training when either the performance falls below a certain threshold\n",
    "    or if there is no improvement over a set number of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_name, patience):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience        \n",
    "        self.best_score = None\n",
    "        self.no_improve_epochs = 0\n",
    "        self.config_file = \"early_stopping_config.json\"  # Config file for early stopping values\n",
    "\n",
    "    def read_early_stopping_config(self):\n",
    "        \"\"\"\n",
    "        Reads the early stopping configuration from the file system.\n",
    "        Returns the configuration as a dictionary.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.config_file):\n",
    "            with open(self.config_file, 'r') as file:\n",
    "                config = json.load(file)\n",
    "            return config\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Config file not found: {self.config_file}\")\n",
    "    def reset_manual_stop_flag(self):\n",
    "        \"\"\"\n",
    "        Resets the manual stop flag to False in the early stopping config file.\n",
    "        \"\"\"\n",
    "        config = self.read_early_stopping_config()\n",
    "        config['manual_stop'] = False\n",
    "        with open(self.config_file, 'w') as file:\n",
    "            json.dump(config, file, indent=4)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric_value = kwargs['metrics'].get(self.metric_name)\n",
    "\n",
    "        if self.best_score is None or metric_value > self.best_score:\n",
    "            self.best_score = metric_value\n",
    "            self.no_improve_epochs = 0\n",
    "        else:\n",
    "            self.no_improve_epochs += 1\n",
    "\n",
    "        # Check if no improvement has been seen over the allowed patience\n",
    "        if self.no_improve_epochs >= self.patience:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: No improvement in {self.metric_name} for {self.patience} epochs\")\n",
    "\n",
    "\n",
    "        # Read the early stopping configuration\n",
    "        config = self.read_early_stopping_config()\n",
    "        min_accuracy = config.get(\"min_accuracy\", 0.35)                \n",
    "        num_epochs_min_acc = config.get(\"num_epochs_min_acc\", 2)  \n",
    "        max_variance = config.get(\"max_variance\", 0.2)  \n",
    "\n",
    "        # Check if performance is below the threshold\n",
    "        if metric_value < min_accuracy:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: {self.metric_name} below manual min_acc of {min_accuracy}\")\n",
    "\n",
    "         # Manual stop from config\n",
    "        if config.get(\"manual_stop\", False):\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Manual early stopping triggered!!\")\n",
    "            self.reset_manual_stop_flag()  # Reset the flag for future runs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac33cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name=pretrained_model_name):\n",
    "    return AutoModelForMultipleChoice.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42b49d87-8773-4644-a995-44e141ccea06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(model_name=pretrained_model_name,run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=4):\n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name),\n",
    "        args=create_training_args(run_name=run_name, num_train_epochs=num_train_epochs, learning_rate=learning_rate, batch_size=batch_size),\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[AdvancedEarlyStoppingCallback(metric_name='eval_accuracy', patience=1)]\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f94ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Clear any cached memory to start fresh for each trial\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    #model_name = trial.suggest_categorical('model_name', [pretrained_model_name, \"./squad-trained-model\", './MCQA-Combined/Optuna/trial_5/checkpoint-22865', './MCQA-Combined/Optuna/trial_0/checkpoint-30485','./MCQA-Combined/Optuna/trial_6/checkpoint-30485' ]) \n",
    "    model_name =\"./MCQA-Combined/Optuna/trial_5/checkpoint-22865\"\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [3, 4])\n",
    "    #warmup_steps = trial.suggest_int('warmup_steps', 0, 1000)\n",
    "    warmup_ratio= trial.suggest_float('warmup_ratio', 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.25)\n",
    "    adam_beta1 = trial.suggest_float('adam_beta1', 0.8, 0.95)\n",
    "    adam_beta2 = trial.suggest_float('adam_beta2', 0.990, 0.999)\n",
    "    adam_epsilon = trial.suggest_float('adam_epsilon', 1e-8, 1e-6)\n",
    "    lr_scheduler_type = trial.suggest_categorical('lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts']) #,'constant_with_warmup'   \n",
    "    \n",
    "\n",
    "    output_dir = f\"./{dataset_name}/Optuna3/trial_{trial.number}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=30,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    ) \n",
    "    \n",
    "    # Print trial parameters\n",
    "    print(f\"Current Trial {trial.number} parameters: {trial.params}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name),\n",
    "        args=training_args,\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[AdvancedEarlyStoppingCallback(metric_name='eval_accuracy', patience=1)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "        \n",
    "    torch.cuda.empty_cache()  # Clear cache after evaluation\n",
    "    gc.collect()  # Collect garbage\n",
    "\n",
    "    return eval_results['eval_accuracy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9b86e-63c3-4ee8-bb91-bbb43e5a6481",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Fine-tuning DeBERTa on MCQA task (AR-LSAT Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5960c1-4b7d-4871-9029-bb724849439c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.1 Evaluate Vanilla DeBERTa (Acc= 27.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920052b1-e4d5-45c3-bb80-a471f35d5c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241015_000838-yo29c9mb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/yo29c9mb' target=\"_blank\">./ReClor/microsoft-deberta-v3-base-best_model</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/yo29c9mb' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/yo29c9mb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.276, 'eval_f1': 0.2767257999541722, 'eval_loss': 1.3862577676773071, 'eval_model_preparation_time': 0.002, 'eval_runtime': 15.3247, 'eval_samples_per_second': 32.627, 'eval_steps_per_second': 32.627}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96f4d5-67f3-4d84-b110-42b283a66ce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2 Fine-Tune and Evaluate Vanilla DeBERTa (Acc=38.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcbb144a-c79b-4747-99c9-efd8f6131fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3105' max='3105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3105/3105 42:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.386800</td>\n",
       "      <td>1.385791</td>\n",
       "      <td>0.342685</td>\n",
       "      <td>0.342227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.391000</td>\n",
       "      <td>1.386014</td>\n",
       "      <td>0.296593</td>\n",
       "      <td>0.297131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.305200</td>\n",
       "      <td>1.325571</td>\n",
       "      <td>0.372745</td>\n",
       "      <td>0.370938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.382, 'eval_f1': 0.38155725956214587, 'eval_loss': 1.3181953430175781, 'eval_runtime': 14.3702, 'eval_samples_per_second': 34.794, 'eval_steps_per_second': 34.794, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(pretrained_model_name)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02783c0c-6c08-42fc-a741-705ff1bac74b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.3 Evaluate SQUAD DeBERTa (Acc=25.2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90877cd6-3ca5-4cc4-8a77-a755a0d6896a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.252, 'eval_f1': 0.2527663131875577, 'eval_loss': 1.386056661605835, 'eval_model_preparation_time': 0.002, 'eval_runtime': 14.2706, 'eval_samples_per_second': 35.037, 'eval_steps_per_second': 35.037}\n"
     ]
    }
   ],
   "source": [
    "path = \"./squad-trained-model\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"Squad-Run\")\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33aceb-a791-4e83-a2f4-cec16d086dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.4 Fine-Tune and Evaluate  SQUAD DeBERTa (Acc=46.8%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63fad85-dd7b-4143-aefe-ad0995cca802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241015_010835-cmd6ekui</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/cmd6ekui' target=\"_blank\">./results/ReClor/Squad-Run/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/cmd6ekui' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/cmd6ekui</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3105' max='3105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3105/3105 23:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.372300</td>\n",
       "      <td>1.320340</td>\n",
       "      <td>0.404810</td>\n",
       "      <td>0.404691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.119300</td>\n",
       "      <td>1.094201</td>\n",
       "      <td>0.537074</td>\n",
       "      <td>0.537099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>1.609691</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>0.550685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.468, 'eval_f1': 0.4681988749254519, 'eval_loss': 1.9973200559616089, 'eval_runtime': 14.2601, 'eval_samples_per_second': 35.063, 'eval_steps_per_second': 35.063, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "path = \"./squad-trained-model\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"Squad-Run\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb7338-7ad6-4009-bc86-8319b477a506",
   "metadata": {},
   "source": [
    "## 4.5 Evaluate Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2498f18-3d8c-428b-b887-338d38439725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.4, 'eval_f1': 0.4006366456726491, 'eval_loss': 1.3058828115463257, 'eval_model_preparation_time': 0.0, 'eval_runtime': 14.525, 'eval_samples_per_second': 34.423, 'eval_steps_per_second': 34.423}\n"
     ]
    }
   ],
   "source": [
    "path = \"./LogiQA/Squad-Run/microsoft-deberta-v3-base/checkpoint-12567\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"LogiQA-Run\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07fcc55e-0bb5-4983-a59c-75057a5ae2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375a17723d434ccfbb402ff78037caa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241016_001554-lmmm3npr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/lmmm3npr' target=\"_blank\">./ReClor/MCQA-Combined-Run/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/lmmm3npr' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/lmmm3npr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.3340\n",
      "F1 Score: 0.3335\n"
     ]
    }
   ],
   "source": [
    "path = \"./MCQA-Combined/Squad-Run/microsoft-deberta-v3-base/checkpoint-18291\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"MCQA-Combined-Run\", batch_size=3,  num_train_epochs=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3b0ef3f-8928-4d0b-acd8-6dd49f5a28f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#Create new train/validate split from the validation split as the train split is already used in the combined dataset\n",
    "\n",
    "# Create new train and validation splits from the existing validation set\n",
    "train_size = 0.8  # Assuming 80% of the data should be used for training\n",
    "val_size = 0.2    # Remaining 20% for validation\n",
    "\n",
    "# Perform the split\n",
    "new_train_dataset, new_val_dataset = train_test_split(encoded_reclor_val, test_size=val_size, train_size=train_size)\n",
    "\n",
    "# Since the split returns lists, you need to convert them back to datasets\n",
    "new_train_dataset = Dataset.from_dict(new_train_dataset)\n",
    "new_val_dataset = Dataset.from_dict(new_val_dataset)\n",
    "\n",
    "\n",
    "def get_train_encoded():\n",
    "    return new_train_dataset \n",
    "\n",
    "def get_val_encoded():\n",
    "    return new_val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fbfc152-a388-409f-abf2-3dfd5b3ff837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769f0732761e42a0a0f7aa2ee8a70536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3947, 'grad_norm': 1.515221357345581, 'learning_rate': 4.9164785591849574e-05, 'epoch': 0.48}\n",
      "{'loss': 1.3877, 'grad_norm': 1.8046776056289673, 'learning_rate': 4.7921767366118935e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5eda0c88a44e529effb2392fa7499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.2985971943887776, 'eval_f1': 0.2984327577081756, 'eval_loss': 1.385642409324646, 'eval_runtime': 17.1848, 'eval_samples_per_second': 29.037, 'eval_steps_per_second': 29.037, 'epoch': 1.0}\n",
      "{'loss': 1.3857, 'grad_norm': 1.6376599073410034, 'learning_rate': 4.4985562567144186e-05, 'epoch': 1.45}\n",
      "{'loss': 1.3774, 'grad_norm': 1.8705437183380127, 'learning_rate': 4.0569342969768665e-05, 'epoch': 1.93}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5674d1df25b545ae8ad6ea8944e60353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23446893787575152, 'eval_f1': 0.08906774588137314, 'eval_loss': 1.38671875, 'eval_runtime': 18.0805, 'eval_samples_per_second': 27.599, 'eval_steps_per_second': 27.599, 'epoch': 2.0}\n",
      "{'loss': 1.385, 'grad_norm': 1.643894076347351, 'learning_rate': 3.4993731089832955e-05, 'epoch': 2.42}\n",
      "{'loss': 1.3834, 'grad_norm': 1.6159354448318481, 'learning_rate': 2.8663522628474263e-05, 'epoch': 2.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f322fe0c3b843df8725c6ad043cfffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23446893787575152, 'eval_f1': 0.08906774588137314, 'eval_loss': 1.38671875, 'eval_runtime': 16.8904, 'eval_samples_per_second': 29.543, 'eval_steps_per_second': 29.543, 'epoch': 3.0}\n",
      "{'loss': 1.3706, 'grad_norm': 2.1004326343536377, 'learning_rate': 2.203829784838659e-05, 'epoch': 3.38}\n",
      "{'loss': 1.3742, 'grad_norm': 2.7644174098968506, 'learning_rate': 1.5599055529219255e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c27789b70b413badc92990801a520f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23647294589178355, 'eval_f1': 0.10052816315688022, 'eval_loss': 1.3867089748382568, 'eval_runtime': 17.1559, 'eval_samples_per_second': 29.086, 'eval_steps_per_second': 29.086, 'epoch': 4.0}\n",
      "{'loss': 1.375, 'grad_norm': 1.9737707376480103, 'learning_rate': 9.813291914525339e-06, 'epoch': 4.35}\n",
      "{'loss': 1.3733, 'grad_norm': 2.138916015625, 'learning_rate': 5.101059958035776e-06, 'epoch': 4.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02070af7a1d04c65b93cb5016cbec989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23647294589178355, 'eval_f1': 0.18601877501047798, 'eval_loss': 1.3865934610366821, 'eval_runtime': 18.016, 'eval_samples_per_second': 27.698, 'eval_steps_per_second': 27.698, 'epoch': 5.0}\n",
      "{'loss': 1.3697, 'grad_norm': 2.079101800918579, 'learning_rate': 1.8044730062000698e-06, 'epoch': 5.31}\n",
      "{'loss': 1.3723, 'grad_norm': 2.223784923553467, 'learning_rate': 1.6286698398285957e-07, 'epoch': 5.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7277ba69c642419af1aa062ab9bd0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23246492985971945, 'eval_f1': 0.08845007087345423, 'eval_loss': 1.3867168426513672, 'eval_runtime': 17.7611, 'eval_samples_per_second': 28.095, 'eval_steps_per_second': 28.095, 'epoch': 6.0}\n",
      "{'train_runtime': 5386.1301, 'train_samples_per_second': 4.61, 'train_steps_per_second': 1.153, 'train_loss': 1.3788943673101601, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0656c3738f884fa695fdf1fc3f4ccc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.3340\n",
      "F1 Score: 0.3315\n"
     ]
    }
   ],
   "source": [
    "path = \"./MCQA-Combined/Squad-Run/microsoft-deberta-v3-base/checkpoint-18291\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"MCQA-Combined-Run\", batch_size=4,  num_train_epochs=6)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb9f12-d022-4590-af99-611fc317aeaf",
   "metadata": {},
   "source": [
    "# End of NoteBook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a913c2e",
   "metadata": {},
   "source": [
    "## First Tuninig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d63bb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x0000027DA2B772B0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 27e37a07850, raw_cell=\"# Create the Trainer\n",
      "#trainer = create_trainer(mod..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/OEM/Notebooks/COMPSCI764/Project/4.%20ReClor%20Baseline%20Training%20and%20Evaluation.ipynb#X43sZmlsZQ%3D%3D>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:441\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py:732\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[1;32m--> 732\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:363\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[1;34m(self, resume)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    362\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    219\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[0;32m    220\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    150\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf32745ab8d1445ea1291c0555defbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m create_trainer(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ReClor/Optuna/trial_0/checkpoint-5800\u001b[39m\u001b[38;5;124m\"\u001b[39m,  run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptuna\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_val_encoded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3666\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3663\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3665\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3666\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3667\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3669\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3670\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3676\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3877\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3875\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mpad_across_processes(labels, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m   3876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3877\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3879\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:2550\u001b[0m, in \u001b[0;36mAccelerator.pad_across_processes\u001b[1;34m(self, tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m   2517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_across_processes\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, pad_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2519\u001b[0m \u001b[38;5;124;03m    Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so\u001b[39;00m\n\u001b[0;32m   2520\u001b[0m \u001b[38;5;124;03m    they can safely be gathered.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2548\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpad_across_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:412\u001b[0m, in \u001b[0;36mchained_operation.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DistributedOperationException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    414\u001b[0m         operation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:679\u001b[0m, in \u001b[0;36mpad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    676\u001b[0m     new_tensor[indices] \u001b[38;5;241m=\u001b[39m tensor\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_tensor\n\u001b[1;32m--> 679\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrecursively_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pad_across_processes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_on_other_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_first\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:127\u001b[0m, in \u001b[0;36mrecursively_apply\u001b[1;34m(func, data, test_type, error_on_other_type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data)(\n\u001b[0;32m    119\u001b[0m         {\n\u001b[0;32m    120\u001b[0m             k: recursively_apply(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m         }\n\u001b[0;32m    125\u001b[0m     )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m test_type(data):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(data, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_on_other_type:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported types (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) passed to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. Only nested list/tuple/dicts of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects that are valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` should be passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:659\u001b[0m, in \u001b[0;36mpad_across_processes.<locals>._pad_across_processes\u001b[1;34m(tensor, dim, pad_index, pad_first)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# Gather all sizes\u001b[39;00m\n\u001b[1;32m--> 659\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m    660\u001b[0m sizes \u001b[38;5;241m=\u001b[39m gather(size)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# Then pad to the maximum size\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x0000027DA2B772B0>> (for post_run_cell), with arguments args (<ExecutionResult object at 27e37a07610, execution_count=56 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 27e37a07850, raw_cell=\"# Create the Trainer\n",
      "#trainer = create_trainer(mod..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/c%3A/Users/OEM/Notebooks/COMPSCI764/Project/4.%20ReClor%20Baseline%20Training%20and%20Evaluation.ipynb#X43sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "[WinError 10054] An existing connection was forcibly closed by the remote host",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:436\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    435\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpausing backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py:724\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    723\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[1;32m--> 724\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:359\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[1;34m(self, pause)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    358\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:51\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[1;34m(self, record, local)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign(record)\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:221\u001b[0m, in \u001b[0;36mSockClient.send_record_publish\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    219\u001b[0m server_req \u001b[38;5;241m=\u001b[39m spb\u001b[38;5;241m.\u001b[39mServerRequest()\n\u001b[0;32m    220\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_publish\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    150\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m), raw_size)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    128\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sent \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] An existing connection was forcibly closed by the remote host"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "#trainer = create_trainer(model_name=\"./MCQA-Combined/Optuna/trial_5/checkpoint-22865\", run_name=\"Optuna\")  - Official Score 51.22%\n",
    "trainer = create_trainer(model_name=\"./ReClor/Optuna/trial_0/checkpoint-5800\",  run_name=\"Optuna\")\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_val_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9abf68ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 15:11:55,784] A new study created in memory with name: no-name-ea4bfe48-1cd2-49e9-baaa-1e5c2e900ae2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 0 parameters: {'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d833baa7eab54951b46490511b33906f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2007, 'grad_norm': 3.974223363911733e-05, 'learning_rate': 3.7509664515743714e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1361, 'grad_norm': 143.74293518066406, 'learning_rate': 7.539821453164645e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: 500 encountered ({\"errors\":[{\"message\":\"context deadline exceeded\",\"path\":[\"project\"]}],\"data\":{\"project\":null}}), retrying request\n",
      "wandb: Network error resolved after 0:00:27.526240, resuming normal operation.\n",
      "wandb: 500 encountered ({\"errors\":[{\"message\":\"context deadline exceeded\",\"path\":[\"project\"]}],\"data\":{\"project\":null}}), retrying request\n",
      "wandb: Network error resolved after 0:01:17.070744, resuming normal operation.\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 281, in check_network_status\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 915, in deliver_network_status\n",
      "    return self._deliver_network_status(status)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 503, in _deliver_network_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 305, in check_stop_status\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 907, in deliver_stop_status\n",
      "    return self._deliver_stop_status(status)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 487, in _deliver_stop_status\n",
      "    return self._deliver_record(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Exception in thread IntMsgThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 325, in check_internal_messages\n",
      "    self._loop_check_status(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\wandb_run.py\", line 235, in _loop_check_status\n",
      "    local_handle = request()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\", line 923, in deliver_internal_messages\n",
      "    return self._deliver_internal_messages(internal_message)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 509, in _deliver_internal_messages\n",
      "    return self._deliver_record(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\", line 452, in _deliver_record\n",
      "    handle = mailbox._deliver_record(record, interface=self)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py\", line 455, in _deliver_record\n",
      "    interface._publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py\", line 51, in _publish\n",
      "    self._sock_client.send_record_publish(record)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 221, in send_record_publish\n",
      "    self.send_server_request(server_req)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 155, in send_server_request\n",
      "    self._send_message(msg)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 152, in _send_message\n",
      "    self._sendall_with_error_handle(header + data)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py\", line 130, in _sendall_with_error_handle\n",
      "    sent = self._sock.send(data)\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a42779314384c1993979aa853fcf188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9399240439612142, 'eval_loss': 0.5564093589782715, 'eval_runtime': 23.9439, 'eval_samples_per_second': 20.84, 'eval_steps_per_second': 5.221, 'epoch': 1.0}\n",
      "{'loss': 0.1219, 'grad_norm': 0.00015371701738331467, 'learning_rate': 1.132867645475492e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1184, 'grad_norm': 238.6856231689453, 'learning_rate': 1.5117531456345194e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525819274b054d83a5471dc518a37e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9378831817562621, 'eval_loss': 0.5600924491882324, 'eval_runtime': 21.802, 'eval_samples_per_second': 22.888, 'eval_steps_per_second': 5.733, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2678.9295, 'train_samples_per_second': 51.939, 'train_steps_per_second': 12.99, 'train_loss': 0.15260590849251582, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d59922346b0417d94af90078da6cb37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 15:56:58,228] Trial 0 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 1 parameters: {'learning_rate': 2.081866957632205e-07, 'batch_size': 3, 'warmup_ratio': 0.6006070336374876, 'weight_decay': 0.0586348889480966, 'adam_beta1': 0.9050607005490999, 'adam_beta2': 0.9965461630006571, 'adam_epsilon': 1.4185782768203173e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a366837a114e9fa77482b38e0ef42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2161, 'grad_norm': 138.05088806152344, 'learning_rate': 3.699336411056257e-09, 'epoch': 0.32}\n",
      "{'loss': 0.186, 'grad_norm': 24.56732177734375, 'learning_rate': 7.428566449676605e-09, 'epoch': 0.65}\n",
      "{'loss': 0.1055, 'grad_norm': 0.0011058965465053916, 'learning_rate': 1.1165269895187975e-08, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f894220b184116858014cdd4ae6845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5544729828834534, 'eval_runtime': 17.9234, 'eval_samples_per_second': 27.841, 'eval_steps_per_second': 9.317, 'epoch': 1.0}\n",
      "{'loss': 0.1216, 'grad_norm': 0.11031674593687057, 'learning_rate': 1.4901973340699347e-08, 'epoch': 1.29}\n",
      "{'loss': 0.1579, 'grad_norm': 0.01069814432412386, 'learning_rate': 1.8638676786210716e-08, 'epoch': 1.62}\n",
      "{'loss': 0.191, 'grad_norm': 0.003246413776651025, 'learning_rate': 2.2375380231722088e-08, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3961e860cc3a426284f089054eca9a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.552856981754303, 'eval_runtime': 17.9306, 'eval_samples_per_second': 27.83, 'eval_steps_per_second': 9.314, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 1178.3674, 'train_samples_per_second': 118.079, 'train_steps_per_second': 39.36, 'train_loss': 0.1623492521579639, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc6bfe12f974d06b60e395330f10173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 16:16:55,579] Trial 1 finished with value: 0.935871743486974 and parameters: {'learning_rate': 2.081866957632205e-07, 'batch_size': 3, 'warmup_ratio': 0.6006070336374876, 'weight_decay': 0.0586348889480966, 'adam_beta1': 0.9050607005490999, 'adam_beta2': 0.9965461630006571, 'adam_epsilon': 1.4185782768203173e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 2 parameters: {'learning_rate': 4.209283208471863e-07, 'batch_size': 3, 'warmup_ratio': 0.37357937635019467, 'weight_decay': 0.09502589043491191, 'adam_beta1': 0.8596182665473875, 'adam_beta2': 0.9962880203824281, 'adam_epsilon': 6.970115945418126e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df7af16791c4f23bfd7f4b748879908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2162, 'grad_norm': 136.7920379638672, 'learning_rate': 1.2025135269773026e-08, 'epoch': 0.32}\n",
      "{'loss': 0.1858, 'grad_norm': 23.006372451782227, 'learning_rate': 2.4147443349806846e-08, 'epoch': 0.65}\n",
      "{'loss': 0.105, 'grad_norm': 0.0012465206673368812, 'learning_rate': 3.6294044632405864e-08, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8360945446849c59dab62e7fbd0be9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5590081214904785, 'eval_runtime': 17.9605, 'eval_samples_per_second': 27.783, 'eval_steps_per_second': 9.298, 'epoch': 1.0}\n",
      "{'loss': 0.1199, 'grad_norm': 0.0857333168387413, 'learning_rate': 4.844064591500488e-08, 'epoch': 1.29}\n",
      "{'loss': 0.1567, 'grad_norm': 0.010123919695615768, 'learning_rate': 6.058724719760389e-08, 'epoch': 1.62}\n",
      "{'loss': 0.1892, 'grad_norm': 0.004119736608117819, 'learning_rate': 7.273384848020291e-08, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188a8e806cd649dbbe24bf4e2d58f7f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5503807067871094, 'eval_runtime': 17.9472, 'eval_samples_per_second': 27.804, 'eval_steps_per_second': 9.305, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 1169.6738, 'train_samples_per_second': 118.956, 'train_steps_per_second': 39.652, 'train_loss': 0.16142211663615041, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b7d35b8de549609daaea7bfe0b3609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 16:36:44,213] Trial 2 finished with value: 0.935871743486974 and parameters: {'learning_rate': 4.209283208471863e-07, 'batch_size': 3, 'warmup_ratio': 0.37357937635019467, 'weight_decay': 0.09502589043491191, 'adam_beta1': 0.8596182665473875, 'adam_beta2': 0.9962880203824281, 'adam_epsilon': 6.970115945418126e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 3 parameters: {'learning_rate': 2.4320759012786312e-05, 'batch_size': 4, 'warmup_ratio': 0.43428494165126297, 'weight_decay': 0.17792509942077755, 'adam_beta1': 0.9072495493860744, 'adam_beta2': 0.9955090465942885, 'adam_epsilon': 8.072511769911557e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deec5a3aeb2e4f49a5229756fdd50ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1993, 'grad_norm': 9.122314804699272e-05, 'learning_rate': 7.965314087157089e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1364, 'grad_norm': 143.51292419433594, 'learning_rate': 1.6011085892366268e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbee1242e52493bab0d1ba5544fda12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9398895060983918, 'eval_loss': 0.5555239915847778, 'eval_runtime': 21.9241, 'eval_samples_per_second': 22.76, 'eval_steps_per_second': 5.701, 'epoch': 1.0}\n",
      "{'loss': 0.1119, 'grad_norm': 2.6215633624815382e-05, 'learning_rate': 2.4056857697575452e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1145, 'grad_norm': 245.3953857421875, 'learning_rate': 3.2102629502784634e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31d2ef63c0343a882c1f698717fc527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379038745902142, 'eval_loss': 0.5454299449920654, 'eval_runtime': 21.6573, 'eval_samples_per_second': 23.041, 'eval_steps_per_second': 5.772, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2297.0015, 'train_samples_per_second': 60.575, 'train_steps_per_second': 15.15, 'train_loss': 0.14942096348466544, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cbf96bd2224939ad749207dee13795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 17:15:23,910] Trial 3 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 2.4320759012786312e-05, 'batch_size': 4, 'warmup_ratio': 0.43428494165126297, 'weight_decay': 0.17792509942077755, 'adam_beta1': 0.9072495493860744, 'adam_beta2': 0.9955090465942885, 'adam_epsilon': 8.072511769911557e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 4 parameters: {'learning_rate': 1.8780148645548947e-06, 'batch_size': 3, 'warmup_ratio': 0.11853606348503187, 'weight_decay': 0.12637484667500865, 'adam_beta1': 0.9354524312732975, 'adam_beta2': 0.9958991004371357, 'adam_epsilon': 3.051130836550833e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d67775f4f634b6194f6ebf426cd0ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2156, 'grad_norm': 122.10905456542969, 'learning_rate': 1.6908282247265783e-07, 'epoch': 0.32}\n",
      "{'loss': 0.1832, 'grad_norm': 10.739239692687988, 'learning_rate': 3.398735522430193e-07, 'epoch': 0.65}\n",
      "{'loss': 0.1, 'grad_norm': 0.000938432349357754, 'learning_rate': 5.106642820133808e-07, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea658d664a8942b6a75972e4743aa505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5556657910346985, 'eval_runtime': 17.8792, 'eval_samples_per_second': 27.91, 'eval_steps_per_second': 9.34, 'epoch': 1.0}\n",
      "{'loss': 0.1, 'grad_norm': 0.006034303456544876, 'learning_rate': 6.814550117837422e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1449, 'grad_norm': 0.01698395237326622, 'learning_rate': 8.51904160094563e-07, 'epoch': 1.62}\n",
      "{'loss': 0.18, 'grad_norm': 0.04308560490608215, 'learning_rate': 1.0226948898649244e-06, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9538621027d5497ea08d137e1dc8f478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9398730517318628, 'eval_loss': 0.5076422691345215, 'eval_runtime': 17.8638, 'eval_samples_per_second': 27.934, 'eval_steps_per_second': 9.349, 'epoch': 2.0}\n",
      "{'loss': 0.0979, 'grad_norm': 0.0036783951800316572, 'learning_rate': 1.1934856196352858e-06, 'epoch': 2.26}\n",
      "{'loss': 0.1148, 'grad_norm': 16.524320602416992, 'learning_rate': 1.3642763494056473e-06, 'epoch': 2.59}\n",
      "{'loss': 0.155, 'grad_norm': 0.03502001240849495, 'learning_rate': 1.5343839162569273e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d90f31931b64db08c95c2104a2b9964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379318191574723, 'eval_loss': 0.5781373381614685, 'eval_runtime': 17.8984, 'eval_samples_per_second': 27.88, 'eval_steps_per_second': 9.33, 'epoch': 3.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 1757.4692, 'train_samples_per_second': 79.171, 'train_steps_per_second': 26.39, 'train_loss': 0.14347673171858893, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8d633e723144978665066ced74590c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 17:45:00,363] Trial 4 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 1.8780148645548947e-06, 'batch_size': 3, 'warmup_ratio': 0.11853606348503187, 'weight_decay': 0.12637484667500865, 'adam_beta1': 0.9354524312732975, 'adam_beta2': 0.9958991004371357, 'adam_epsilon': 3.051130836550833e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 5 parameters: {'learning_rate': 5.461764668132139e-05, 'batch_size': 4, 'warmup_ratio': 0.29748784861167366, 'weight_decay': 0.06446170389576267, 'adam_beta1': 0.9047555603231419, 'adam_beta2': 0.9929639240746806, 'adam_epsilon': 4.0218893028568194e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56d9c3ba57648e39fda1b4ccd0b722e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1958, 'grad_norm': 0.00024149479577317834, 'learning_rate': 2.6113913944995737e-06, 'epoch': 0.43}\n",
      "{'loss': 0.1548, 'grad_norm': 142.24588012695312, 'learning_rate': 5.249160479852679e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f305727f674c7ab2649e674b1b91cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9338677354709419, 'eval_f1': 0.9338928618680636, 'eval_loss': 0.5474833846092224, 'eval_runtime': 21.8632, 'eval_samples_per_second': 22.824, 'eval_steps_per_second': 5.717, 'epoch': 1.0}\n",
      "{'loss': 0.1198, 'grad_norm': 0.043761178851127625, 'learning_rate': 7.886929565205784e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1582, 'grad_norm': 185.4772491455078, 'learning_rate': 1.0524698650558889e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3e25d5fb3a417eb5da66122d5b4f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9078156312625251, 'eval_f1': 0.9078627685847167, 'eval_loss': 0.6732323169708252, 'eval_runtime': 21.4745, 'eval_samples_per_second': 23.237, 'eval_steps_per_second': 5.821, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2293.4304, 'train_samples_per_second': 60.669, 'train_steps_per_second': 15.174, 'train_loss': 0.17046518161379057, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f212b99c20341e6b92258a17af8e865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 18:23:36,457] Trial 5 finished with value: 0.9338677354709419 and parameters: {'learning_rate': 5.461764668132139e-05, 'batch_size': 4, 'warmup_ratio': 0.29748784861167366, 'weight_decay': 0.06446170389576267, 'adam_beta1': 0.9047555603231419, 'adam_beta2': 0.9929639240746806, 'adam_epsilon': 4.0218893028568194e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 6 parameters: {'learning_rate': 1.9517139030724175e-07, 'batch_size': 4, 'warmup_ratio': 0.22736486112025234, 'weight_decay': 0.08517600500154998, 'adam_beta1': 0.838934795685261, 'adam_beta2': 0.9913452942598424, 'adam_epsilon': 6.029933770180071e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4653cfb78b824d6c907db1717653863e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2021, 'grad_norm': 2.802017297653947e-05, 'learning_rate': 1.2209002679399048e-08, 'epoch': 0.43}\n",
      "{'loss': 0.1405, 'grad_norm': 146.70745849609375, 'learning_rate': 2.4541328618185966e-08, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea13bade90844b3913e2f0ee6b6d8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5523994565010071, 'eval_runtime': 73.9722, 'eval_samples_per_second': 6.746, 'eval_steps_per_second': 1.69, 'epoch': 1.0}\n",
      "{'loss': 0.1509, 'grad_norm': 0.00556301511824131, 'learning_rate': 3.687365455697288e-08, 'epoch': 1.29}\n",
      "{'loss': 0.1296, 'grad_norm': 233.63095092773438, 'learning_rate': 4.92059804957598e-08, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5d55422ba84409ba6e587c5f539bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5581141710281372, 'eval_runtime': 101.4891, 'eval_samples_per_second': 4.917, 'eval_steps_per_second': 1.232, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 19477.8021, 'train_samples_per_second': 7.144, 'train_steps_per_second': 1.787, 'train_loss': 0.16396001947337183, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d8d0d0d4024a7d9ca235efc04faf3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 23:49:57,732] Trial 6 finished with value: 0.935871743486974 and parameters: {'learning_rate': 1.9517139030724175e-07, 'batch_size': 4, 'warmup_ratio': 0.22736486112025234, 'weight_decay': 0.08517600500154998, 'adam_beta1': 0.838934795685261, 'adam_beta2': 0.9913452942598424, 'adam_epsilon': 6.029933770180071e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 7 parameters: {'learning_rate': 5.3331637660761855e-05, 'batch_size': 4, 'warmup_ratio': 0.5430317046572521, 'weight_decay': 0.19083919012158201, 'adam_beta1': 0.8101433445091133, 'adam_beta2': 0.9967761152201805, 'adam_epsilon': 8.263355819356603e-08, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7799e6216bd84935bc9b3ea418d78257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'grad_norm': 0.00017245460185222328, 'learning_rate': 1.396928809507732e-06, 'epoch': 0.43}\n",
      "{'loss': 0.1424, 'grad_norm': 139.92591857910156, 'learning_rate': 2.8079680110306937e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c7b41160a64d838994be7d745050d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359036696491496, 'eval_loss': 0.5496513247489929, 'eval_runtime': 21.7858, 'eval_samples_per_second': 22.905, 'eval_steps_per_second': 5.738, 'epoch': 1.0}\n",
      "{'loss': 0.111, 'grad_norm': 2.511823367967736e-05, 'learning_rate': 4.219007212553655e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1223, 'grad_norm': 228.8831024169922, 'learning_rate': 5.630046414076617e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e350d470b84ea1b843fcf0742799a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9318637274549099, 'eval_f1': 0.9319122892663356, 'eval_loss': 0.5565008521080017, 'eval_runtime': 21.7858, 'eval_samples_per_second': 22.905, 'eval_steps_per_second': 5.738, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2268.8974, 'train_samples_per_second': 61.325, 'train_steps_per_second': 15.338, 'train_loss': 0.15282705076809588, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87dc15cdd5a442d487c183171cfebbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 00:28:10,424] Trial 7 finished with value: 0.935871743486974 and parameters: {'learning_rate': 5.3331637660761855e-05, 'batch_size': 4, 'warmup_ratio': 0.5430317046572521, 'weight_decay': 0.19083919012158201, 'adam_beta1': 0.8101433445091133, 'adam_beta2': 0.9967761152201805, 'adam_epsilon': 8.263355819356603e-08, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 8 parameters: {'learning_rate': 6.290870893003492e-06, 'batch_size': 3, 'warmup_ratio': 0.8640569450830029, 'weight_decay': 0.0358800403430124, 'adam_beta1': 0.8889996603067263, 'adam_beta2': 0.992656822089469, 'adam_epsilon': 2.748398030680172e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce14dade2e3c4749816b55e12f83bb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2159, 'grad_norm': 130.80955505371094, 'learning_rate': 7.770383261476553e-08, 'epoch': 0.32}\n",
      "{'loss': 0.1847, 'grad_norm': 15.719806671142578, 'learning_rate': 1.5619255242766002e-07, 'epoch': 0.65}\n",
      "{'loss': 0.1022, 'grad_norm': 0.001390735269524157, 'learning_rate': 2.346812722405545e-07, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfb5861e507493f9f454e3d8faf7752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5640942454338074, 'eval_runtime': 18.1324, 'eval_samples_per_second': 27.52, 'eval_steps_per_second': 9.21, 'epoch': 1.0}\n",
      "{'loss': 0.1096, 'grad_norm': 0.026386797428131104, 'learning_rate': 3.1316999205344896e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1506, 'grad_norm': 0.010197930037975311, 'learning_rate': 3.915017344267176e-07, 'epoch': 1.62}\n",
      "{'loss': 0.1834, 'grad_norm': 0.01581771858036518, 'learning_rate': 4.6999045423961206e-07, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11d8a74d83a4c449574575854d0f734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9378944109059071, 'eval_loss': 0.5267962217330933, 'eval_runtime': 17.8775, 'eval_samples_per_second': 27.912, 'eval_steps_per_second': 9.341, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 1189.7615, 'train_samples_per_second': 116.948, 'train_steps_per_second': 38.983, 'train_loss': 0.15692259299955577, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c0d0e68b61458ba2919527b7ef565c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 00:48:19,257] Trial 8 finished with value: 0.9378757515030061 and parameters: {'learning_rate': 6.290870893003492e-06, 'batch_size': 3, 'warmup_ratio': 0.8640569450830029, 'weight_decay': 0.0358800403430124, 'adam_beta1': 0.8889996603067263, 'adam_beta2': 0.992656822089469, 'adam_epsilon': 2.748398030680172e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.9398797595190381.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 9 parameters: {'learning_rate': 1.332652302207946e-05, 'batch_size': 4, 'warmup_ratio': 0.3216606611220145, 'weight_decay': 0.0027023555623991724, 'adam_beta1': 0.864991780060698, 'adam_beta2': 0.9971022350324645, 'adam_epsilon': 4.665844108797715e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12af8f1d4244992ace674527fc792fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2, 'grad_norm': 5.4490072216140106e-05, 'learning_rate': 5.89300419504139e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1359, 'grad_norm': 143.32614135742188, 'learning_rate': 1.184553368498219e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a3d9aa21ac4b09a5b98f85d9b907c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9418837675350702, 'eval_f1': 0.9419072450828159, 'eval_loss': 0.5560993552207947, 'eval_runtime': 20.3371, 'eval_samples_per_second': 24.536, 'eval_steps_per_second': 6.146, 'epoch': 1.0}\n",
      "{'loss': 0.1156, 'grad_norm': 5.551341746468097e-05, 'learning_rate': 1.779806317492299e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1151, 'grad_norm': 238.62818908691406, 'learning_rate': 2.3750592664863788e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d94d871c9c4452beb94a164fe2d5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379038745902142, 'eval_loss': 0.5519194006919861, 'eval_runtime': 20.0127, 'eval_samples_per_second': 24.934, 'eval_steps_per_second': 6.246, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2097.3278, 'train_samples_per_second': 66.342, 'train_steps_per_second': 16.593, 'train_loss': 0.15026391950146906, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70245852af143fca726ebd60a24ff01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 01:23:38,081] Trial 9 finished with value: 0.9418837675350702 and parameters: {'learning_rate': 1.332652302207946e-05, 'batch_size': 4, 'warmup_ratio': 0.3216606611220145, 'weight_decay': 0.0027023555623991724, 'adam_beta1': 0.864991780060698, 'adam_beta2': 0.9971022350324645, 'adam_epsilon': 4.665844108797715e-07, 'lr_scheduler_type': 'linear'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 10 parameters: {'learning_rate': 1.740142214931175e-06, 'batch_size': 4, 'warmup_ratio': 0.02424349734159209, 'weight_decay': 0.24476976357878066, 'adam_beta1': 0.8619381387791497, 'adam_beta2': 0.9989940250760806, 'adam_epsilon': 4.978966294246238e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd425876d7a4f0e998e64c0c40602ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1985, 'grad_norm': 0.00012113740376662463, 'learning_rate': 1.0205810383778811e-06, 'epoch': 0.43}\n",
      "{'loss': 0.1383, 'grad_norm': 142.3710479736328, 'learning_rate': 1.7324039220093171e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dad1f3dcad43c0bf701a9f2e624271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9398895060983918, 'eval_loss': 0.5556939244270325, 'eval_runtime': 19.5016, 'eval_samples_per_second': 25.588, 'eval_steps_per_second': 6.41, 'epoch': 1.0}\n",
      "{'loss': 0.1081, 'grad_norm': 5.780799256172031e-05, 'learning_rate': 1.7067804355130989e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1028, 'grad_norm': 238.42990112304688, 'learning_rate': 1.6811569490168804e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c67a08ed9944d65a3381232afc6b7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9358642859453613, 'eval_loss': 0.5408743023872375, 'eval_runtime': 20.4165, 'eval_samples_per_second': 24.441, 'eval_steps_per_second': 6.122, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2280.1806, 'train_samples_per_second': 61.021, 'train_steps_per_second': 15.262, 'train_loss': 0.1441633290257947, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dfdf79a09e40189c54f70ba9beddb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 02:01:59,587] Trial 10 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 1.740142214931175e-06, 'batch_size': 4, 'warmup_ratio': 0.02424349734159209, 'weight_decay': 0.24476976357878066, 'adam_beta1': 0.8619381387791497, 'adam_beta2': 0.9989940250760806, 'adam_epsilon': 4.978966294246238e-07, 'lr_scheduler_type': 'linear'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 11 parameters: {'learning_rate': 1.106211044117024e-05, 'batch_size': 4, 'warmup_ratio': 0.7432294920061084, 'weight_decay': 0.002607992266424136, 'adam_beta1': 0.8078751716319704, 'adam_beta2': 0.9901704676507107, 'adam_epsilon': 9.809578420447187e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f2d8b7f68d4c9ea5584fffca386480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2014, 'grad_norm': 4.127285865251906e-05, 'learning_rate': 2.1170480063325998e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1372, 'grad_norm': 144.4433135986328, 'learning_rate': 4.2554803359614884e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d603813b672649e898073f0484d8dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5573342442512512, 'eval_runtime': 20.3195, 'eval_samples_per_second': 24.558, 'eval_steps_per_second': 6.152, 'epoch': 1.0}\n",
      "{'loss': 0.13, 'grad_norm': 0.0004075548204127699, 'learning_rate': 6.393912665590377e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1199, 'grad_norm': 233.52256774902344, 'learning_rate': 8.532344995219265e-07, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2eafdc7059b468abdd8413e0422a519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.935875288174291, 'eval_loss': 0.5552846193313599, 'eval_runtime': 20.4634, 'eval_samples_per_second': 24.385, 'eval_steps_per_second': 6.108, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2095.8091, 'train_samples_per_second': 66.39, 'train_steps_per_second': 16.605, 'train_loss': 0.15540576638846562, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb0c7e0bcdb42f6b9c303f08ae800b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 02:37:16,839] Trial 11 finished with value: 0.9378757515030061 and parameters: {'learning_rate': 1.106211044117024e-05, 'batch_size': 4, 'warmup_ratio': 0.7432294920061084, 'weight_decay': 0.002607992266424136, 'adam_beta1': 0.8078751716319704, 'adam_beta2': 0.9901704676507107, 'adam_epsilon': 9.809578420447187e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 12 parameters: {'learning_rate': 1.3045906121744648e-05, 'batch_size': 4, 'warmup_ratio': 0.5949894141215617, 'weight_decay': 0.14050806151632145, 'adam_beta1': 0.8346666433206839, 'adam_beta2': 0.9939513160406481, 'adam_epsilon': 2.653443903719701e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b35b07594c546e285ad5797c4ebe09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2011, 'grad_norm': 4.0176513721235096e-05, 'learning_rate': 3.1187692119499663e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1363, 'grad_norm': 143.80104064941406, 'learning_rate': 6.26904114321256e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733a2db6c46d415a82afa4d2014a42ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9399240439612142, 'eval_loss': 0.5572511553764343, 'eval_runtime': 19.6393, 'eval_samples_per_second': 25.408, 'eval_steps_per_second': 6.365, 'epoch': 1.0}\n",
      "{'loss': 0.1244, 'grad_norm': 0.00022325461031869054, 'learning_rate': 9.419313074475151e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1186, 'grad_norm': 237.1048126220703, 'learning_rate': 1.2569585005737744e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f549da1ab3c4dc38887140e57d4ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.935875288174291, 'eval_loss': 0.5554161071777344, 'eval_runtime': 19.5872, 'eval_samples_per_second': 25.476, 'eval_steps_per_second': 6.382, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2268.8914, 'train_samples_per_second': 61.325, 'train_steps_per_second': 15.338, 'train_loss': 0.15333370340281519, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fb94a09191459d81af5f44ff8d5056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 03:15:26,320] Trial 12 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 1.3045906121744648e-05, 'batch_size': 4, 'warmup_ratio': 0.5949894141215617, 'weight_decay': 0.14050806151632145, 'adam_beta1': 0.8346666433206839, 'adam_beta2': 0.9939513160406481, 'adam_epsilon': 2.653443903719701e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 13 parameters: {'learning_rate': 5.1300745373433385e-06, 'batch_size': 4, 'warmup_ratio': 0.985166175210749, 'weight_decay': 0.0016266145488112876, 'adam_beta1': 0.8338783120227198, 'adam_beta2': 0.9985120969837219, 'adam_epsilon': 4.7414189394788616e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd95366e2f1243fab56cb3657e691a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2019, 'grad_norm': 2.7512667656992562e-05, 'learning_rate': 7.406915459062398e-08, 'epoch': 0.43}\n",
      "{'loss': 0.1389, 'grad_norm': 145.76512145996094, 'learning_rate': 1.4888648245994114e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a6b0238eb643a5a27dc407df682376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5545303225517273, 'eval_runtime': 21.2693, 'eval_samples_per_second': 23.461, 'eval_steps_per_second': 5.877, 'epoch': 1.0}\n",
      "{'loss': 0.1416, 'grad_norm': 0.0016357159474864602, 'learning_rate': 2.2370381032925828e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1239, 'grad_norm': 227.5735626220703, 'learning_rate': 2.9852113819857544e-07, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e93e61e084518b93cee29d3a309fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.554935872554779, 'eval_runtime': 21.3722, 'eval_samples_per_second': 23.348, 'eval_steps_per_second': 5.849, 'epoch': 2.0}\n",
      "{'loss': 0.1614, 'grad_norm': 1.0587824306185212e-07, 'learning_rate': 3.7333846606789263e-07, 'epoch': 2.16}\n",
      "{'loss': 0.1203, 'grad_norm': 0.9098794460296631, 'learning_rate': 4.480061592814711e-07, 'epoch': 2.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970789f8c4b24f7aada69784c1a31961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9399240439612142, 'eval_loss': 0.5736071467399597, 'eval_runtime': 21.4387, 'eval_samples_per_second': 23.276, 'eval_steps_per_second': 5.831, 'epoch': 3.0}\n",
      "{'loss': 0.1378, 'grad_norm': 2.27242112159729, 'learning_rate': 5.228234871507882e-07, 'epoch': 3.02}\n",
      "{'loss': 0.0998, 'grad_norm': 0.011053909547626972, 'learning_rate': 5.976408150201054e-07, 'epoch': 3.45}\n",
      "{'loss': 0.1407, 'grad_norm': 0.00028350844513624907, 'learning_rate': 6.724581428894226e-07, 'epoch': 3.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63866b3fd4f54671a94723892ee27c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.573811948299408, 'eval_runtime': 21.3239, 'eval_samples_per_second': 23.401, 'eval_steps_per_second': 5.862, 'epoch': 4.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 4065.8596, 'train_samples_per_second': 34.222, 'train_steps_per_second': 8.559, 'train_loss': 0.13852832831185438, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00bde2e937043b4987ddeac6df8ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 04:23:34,427] Trial 13 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 5.1300745373433385e-06, 'batch_size': 4, 'warmup_ratio': 0.985166175210749, 'weight_decay': 0.0016266145488112876, 'adam_beta1': 0.8338783120227198, 'adam_beta2': 0.9985120969837219, 'adam_epsilon': 4.7414189394788616e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 14 parameters: {'learning_rate': 2.2721897374500115e-05, 'batch_size': 4, 'warmup_ratio': 0.6768089280531311, 'weight_decay': 0.15917904097684543, 'adam_beta1': 0.9451949608014663, 'adam_beta2': 0.9941893702334875, 'adam_epsilon': 5.1208282088829624e-08, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5abdf31320a46188b2c05a7d5fd3e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2002, 'grad_norm': 3.9290651329793036e-05, 'learning_rate': 4.775331889940797e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1348, 'grad_norm': 144.19581604003906, 'learning_rate': 9.59889945553756e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba9e729c91b43e8aa3bcf4a2a383e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9399240439612142, 'eval_loss': 0.560529351234436, 'eval_runtime': 20.8727, 'eval_samples_per_second': 23.907, 'eval_steps_per_second': 5.989, 'epoch': 1.0}\n",
      "{'loss': 0.1181, 'grad_norm': 5.4279986215988174e-05, 'learning_rate': 1.4422467021134325e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1166, 'grad_norm': 241.52877807617188, 'learning_rate': 1.9246034586731085e-06, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd3f79114b14ee7a7074e6546995147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.935875288174291, 'eval_loss': 0.5211436152458191, 'eval_runtime': 20.8974, 'eval_samples_per_second': 23.879, 'eval_steps_per_second': 5.982, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2133.5193, 'train_samples_per_second': 65.216, 'train_steps_per_second': 16.311, 'train_loss': 0.15080753030448124, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16a4d034a4b4377b9171aab6ca8c0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 04:59:29,714] Trial 14 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 2.2721897374500115e-05, 'batch_size': 4, 'warmup_ratio': 0.6768089280531311, 'weight_decay': 0.15917904097684543, 'adam_beta1': 0.9451949608014663, 'adam_beta2': 0.9941893702334875, 'adam_epsilon': 5.1208282088829624e-08, 'lr_scheduler_type': 'cosine'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 15 parameters: {'learning_rate': 9.53255349064821e-05, 'batch_size': 4, 'warmup_ratio': 0.46745217436286496, 'weight_decay': 0.22363355062361143, 'adam_beta1': 0.8715256999111988, 'adam_beta2': 0.9979493417368412, 'adam_epsilon': 1.7114394380042275e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a66246fda4d47d981eb4afff9226d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1956, 'grad_norm': 0.0002564020687714219, 'learning_rate': 2.9005495315163905e-06, 'epoch': 0.43}\n",
      "{'loss': 0.1597, 'grad_norm': 142.0081787109375, 'learning_rate': 5.8303975431491085e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20512f4c52d3479395702024de461e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9298597194388778, 'eval_f1': 0.9299122950475985, 'eval_loss': 0.5481395125389099, 'eval_runtime': 20.8713, 'eval_samples_per_second': 23.908, 'eval_steps_per_second': 5.989, 'epoch': 1.0}\n",
      "{'loss': 0.1217, 'grad_norm': 0.07999307662248611, 'learning_rate': 8.760245554781826e-06, 'epoch': 1.29}\n",
      "{'loss': 0.1681, 'grad_norm': 217.4483642578125, 'learning_rate': 1.1690093566414543e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f639dd60e3e4472b958fcd13e21118f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9118236472945892, 'eval_f1': 0.911957823690246, 'eval_loss': 0.8085912466049194, 'eval_runtime': 20.8011, 'eval_samples_per_second': 23.989, 'eval_steps_per_second': 6.009, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2134.1065, 'train_samples_per_second': 65.198, 'train_steps_per_second': 16.307, 'train_loss': 0.17650205513526654, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690027b21ccd4e4bba764090fb6f1167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 05:35:25,384] Trial 15 finished with value: 0.9298597194388778 and parameters: {'learning_rate': 9.53255349064821e-05, 'batch_size': 4, 'warmup_ratio': 0.46745217436286496, 'weight_decay': 0.22363355062361143, 'adam_beta1': 0.8715256999111988, 'adam_beta2': 0.9979493417368412, 'adam_epsilon': 1.7114394380042275e-07, 'lr_scheduler_type': 'linear'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 16 parameters: {'learning_rate': 1.1390437132148595e-06, 'batch_size': 4, 'warmup_ratio': 0.22472734071311423, 'weight_decay': 0.10563337518542765, 'adam_beta1': 0.8002699227072925, 'adam_beta2': 0.9920616635528605, 'adam_epsilon': 3.5758212131124344e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bae7b282d4a646dc8c1bbb7881c8659c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2019, 'grad_norm': 2.765080535027664e-05, 'learning_rate': 7.209137425410504e-08, 'epoch': 0.43}\n",
      "{'loss': 0.1391, 'grad_norm': 145.7869873046875, 'learning_rate': 1.4491094420774648e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7ebd4290f24ac3b9fd63cd09437c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5550214052200317, 'eval_runtime': 20.6904, 'eval_samples_per_second': 24.117, 'eval_steps_per_second': 6.041, 'epoch': 1.0}\n",
      "{'loss': 0.1421, 'grad_norm': 0.001591749140061438, 'learning_rate': 2.1773051416138796e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1243, 'grad_norm': 228.28860473632812, 'learning_rate': 2.905500841150294e-07, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008a8e00e4904d93a6a202b27ab89b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5564877390861511, 'eval_runtime': 20.8246, 'eval_samples_per_second': 23.962, 'eval_steps_per_second': 6.003, 'epoch': 2.0}\n",
      "{'loss': 0.1623, 'grad_norm': 7.613397912109576e-08, 'learning_rate': 3.6336965406867083e-07, 'epoch': 2.16}\n",
      "{'loss': 0.1217, 'grad_norm': 0.963443398475647, 'learning_rate': 4.3604358488240495e-07, 'epoch': 2.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db902c89abad46fdb14b4e36fb2db25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9398797595190381, 'eval_f1': 0.9399240439612142, 'eval_loss': 0.5736603736877441, 'eval_runtime': 20.7609, 'eval_samples_per_second': 24.036, 'eval_steps_per_second': 6.021, 'epoch': 3.0}\n",
      "{'loss': 0.1393, 'grad_norm': 2.202064275741577, 'learning_rate': 5.088631548360464e-07, 'epoch': 3.02}\n",
      "{'loss': 0.1015, 'grad_norm': 0.01070069894194603, 'learning_rate': 5.81682724789688e-07, 'epoch': 3.45}\n",
      "{'loss': 0.1431, 'grad_norm': 0.0002620970772113651, 'learning_rate': 6.545022947433293e-07, 'epoch': 3.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8d5d002b2e4fc983e9b986563b097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5775915384292603, 'eval_runtime': 21.1547, 'eval_samples_per_second': 23.588, 'eval_steps_per_second': 5.909, 'epoch': 4.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 4507.4996, 'train_samples_per_second': 30.869, 'train_steps_per_second': 7.72, 'train_loss': 0.13954102808031543, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2877ef3b9c7b4407a3d3df0c75ee48a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 06:50:56,015] Trial 16 finished with value: 0.9398797595190381 and parameters: {'learning_rate': 1.1390437132148595e-06, 'batch_size': 4, 'warmup_ratio': 0.22472734071311423, 'weight_decay': 0.10563337518542765, 'adam_beta1': 0.8002699227072925, 'adam_beta2': 0.9920616635528605, 'adam_epsilon': 3.5758212131124344e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 17 parameters: {'learning_rate': 5.4345030270771235e-06, 'batch_size': 4, 'warmup_ratio': 0.7665504200303693, 'weight_decay': 0.03516175614603054, 'adam_beta1': 0.8501054470277679, 'adam_beta2': 0.9950987378578932, 'adam_epsilon': 6.187976653469034e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96a17c065594b589bebbf73337f8b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2018, 'grad_norm': 4.270712815923616e-05, 'learning_rate': 1.0084266750649184e-07, 'epoch': 0.43}\n",
      "{'loss': 0.1385, 'grad_norm': 145.46963500976562, 'learning_rate': 2.0270394781607956e-07, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1931f86eeaee4df983d8726e0865a9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5553570985794067, 'eval_runtime': 20.4219, 'eval_samples_per_second': 24.435, 'eval_steps_per_second': 6.121, 'epoch': 1.0}\n",
      "{'loss': 0.1387, 'grad_norm': 0.0011071932967752218, 'learning_rate': 3.045652281256672e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1227, 'grad_norm': 227.7855224609375, 'learning_rate': 4.0642650843525493e-07, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaec6b2ea0845aab9f02d2c788e73d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9379182832541295, 'eval_loss': 0.5530847907066345, 'eval_runtime': 20.4407, 'eval_samples_per_second': 24.412, 'eval_steps_per_second': 6.115, 'epoch': 2.0}\n",
      "{'loss': 0.1597, 'grad_norm': 7.443355087843884e-08, 'learning_rate': 5.082877887448427e-07, 'epoch': 2.16}\n",
      "{'loss': 0.116, 'grad_norm': 0.6248034834861755, 'learning_rate': 6.099453464938112e-07, 'epoch': 2.59}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c77da948814fda861cfccf506e892f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9418837675350702, 'eval_f1': 0.9419072450828159, 'eval_loss': 0.5867544412612915, 'eval_runtime': 20.3915, 'eval_samples_per_second': 24.471, 'eval_steps_per_second': 6.13, 'epoch': 3.0}\n",
      "{'loss': 0.1335, 'grad_norm': 3.347468614578247, 'learning_rate': 7.118066268033989e-07, 'epoch': 3.02}\n",
      "{'loss': 0.0927, 'grad_norm': 0.009553579613566399, 'learning_rate': 8.136679071129867e-07, 'epoch': 3.45}\n",
      "{'loss': 0.1315, 'grad_norm': 0.0001974794577108696, 'learning_rate': 9.155291874225744e-07, 'epoch': 3.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ab8d9932b74510addbcf1f5ee81fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.9378757515030061, 'eval_f1': 0.9378980210766958, 'eval_loss': 0.5795443654060364, 'eval_runtime': 20.5329, 'eval_samples_per_second': 24.302, 'eval_steps_per_second': 6.088, 'epoch': 4.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 4200.5548, 'train_samples_per_second': 33.124, 'train_steps_per_second': 8.285, 'train_loss': 0.13497266666642552, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f331eb0508704e739131195bcd058ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 08:01:18,052] Trial 17 finished with value: 0.9418837675350702 and parameters: {'learning_rate': 5.4345030270771235e-06, 'batch_size': 4, 'warmup_ratio': 0.7665504200303693, 'weight_decay': 0.03516175614603054, 'adam_beta1': 0.8501054470277679, 'adam_beta2': 0.9950987378578932, 'adam_epsilon': 6.187976653469034e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 18 parameters: {'learning_rate': 4.154648473414455e-06, 'batch_size': 3, 'warmup_ratio': 0.8254611076004349, 'weight_decay': 0.021100516165727612, 'adam_beta1': 0.8486817950435868, 'adam_beta2': 0.9975388465691606, 'adam_epsilon': 6.359404716640919e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dda9e19d664bc39923bb7592b25abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2159, 'grad_norm': 132.9669952392578, 'learning_rate': 5.3716886361242135e-08, 'epoch': 0.32}\n",
      "{'loss': 0.185, 'grad_norm': 17.82255744934082, 'learning_rate': 1.0786784857186805e-07, 'epoch': 0.65}\n",
      "{'loss': 0.103, 'grad_norm': 0.0015064617618918419, 'learning_rate': 1.621273297448399e-07, 'epoch': 0.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5cbbfbe31114388b69d531675fb6b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5643511414527893, 'eval_runtime': 17.8447, 'eval_samples_per_second': 27.963, 'eval_steps_per_second': 9.359, 'epoch': 1.0}\n",
      "{'loss': 0.1128, 'grad_norm': 0.0397338792681694, 'learning_rate': 2.1638681091781175e-07, 'epoch': 1.29}\n",
      "{'loss': 0.1521, 'grad_norm': 0.009844542481005192, 'learning_rate': 2.706462920907836e-07, 'epoch': 1.62}\n",
      "{'loss': 0.1843, 'grad_norm': 0.011681566014885902, 'learning_rate': 3.2490577326375545e-07, 'epoch': 1.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f45a759d78947e0811a6713625f92aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.533847451210022, 'eval_runtime': 17.9076, 'eval_samples_per_second': 27.865, 'eval_steps_per_second': 9.326, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 1166.0003, 'train_samples_per_second': 119.331, 'train_steps_per_second': 39.777, 'train_loss': 0.1580897867294055, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc687c77f7c242af81d6ac32e5cc1f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 08:21:03,138] Trial 18 finished with value: 0.935871743486974 and parameters: {'learning_rate': 4.154648473414455e-06, 'batch_size': 3, 'warmup_ratio': 0.8254611076004349, 'weight_decay': 0.021100516165727612, 'adam_beta1': 0.8486817950435868, 'adam_beta2': 0.9975388465691606, 'adam_epsilon': 6.359404716640919e-07, 'lr_scheduler_type': 'linear'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 19 parameters: {'learning_rate': 6.169994646982436e-07, 'batch_size': 4, 'warmup_ratio': 0.9886008700272123, 'weight_decay': 0.045635541062629625, 'adam_beta1': 0.8867276898827927, 'adam_beta2': 0.9950703113403668, 'adam_epsilon': 7.987487123856168e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e10096ac44c1cb2e39bb5f0a59ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2021, 'grad_norm': 2.8235794161446393e-05, 'learning_rate': 8.8773030759688e-09, 'epoch': 0.43}\n",
      "{'loss': 0.1406, 'grad_norm': 146.78402709960938, 'learning_rate': 1.7844275879977687e-08, 'epoch': 0.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cf06f5236049a0a94f4f9df9f3dc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.5521913766860962, 'eval_runtime': 20.5431, 'eval_samples_per_second': 24.29, 'eval_steps_per_second': 6.085, 'epoch': 1.0}\n",
      "{'loss': 0.1515, 'grad_norm': 0.006157710216939449, 'learning_rate': 2.6811248683986577e-08, 'epoch': 1.29}\n",
      "{'loss': 0.1301, 'grad_norm': 233.99411010742188, 'learning_rate': 3.5778221487995466e-08, 'epoch': 1.72}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3bd116c7354cf4b43746939f8e6768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.935871743486974, 'eval_f1': 0.9359157700799514, 'eval_loss': 0.557083785533905, 'eval_runtime': 20.4528, 'eval_samples_per_second': 24.398, 'eval_steps_per_second': 6.112, 'epoch': 2.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 2016.148, 'train_samples_per_second': 69.013, 'train_steps_per_second': 17.261, 'train_loss': 0.16425138341969459, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9924c4fae9f04a9789d27478e7ff924d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-19 08:55:00,648] Trial 19 finished with value: 0.935871743486974 and parameters: {'learning_rate': 6.169994646982436e-07, 'batch_size': 4, 'warmup_ratio': 0.9886008700272123, 'weight_decay': 0.045635541062629625, 'adam_beta1': 0.8867276898827927, 'adam_beta2': 0.9950703113403668, 'adam_epsilon': 7.987487123856168e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 9 with value: 0.9418837675350702.\n"
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "#study.enqueue_trial({'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.enqueue_trial({'model_name': './MCQA-Combined/Optuna/trial_5/checkpoint-22865', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.optimize(objective, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a93bac0",
   "metadata": {},
   "source": [
    "## Second Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9474c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0dab528c994c8ab1dd40ba259c5cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m create_trainer(model_name\u001b[38;5;241m=\u001b[39mcheck_point_path,  run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptuna3\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m      9\u001b[0m test_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mget_test_encoded())\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3324\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\amp\\autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1599\u001b[0m, in \u001b[0;36mDebertaV2ForMultipleChoice.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1592\u001b[0m flat_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attention_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m flat_inputs_embeds \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1594\u001b[0m     inputs_embeds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m )\n\u001b[1;32m-> 1599\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1610\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1611\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1061\u001b[0m )\n\u001b[1;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    499\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m         output_attentions,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    348\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m ):\n\u001b[1;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    285\u001b[0m ):\n\u001b[1;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:726\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    721\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m attention_scores\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, attention_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), attention_scores\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    723\u001b[0m )\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# bsz x height x length x dimension\u001b[39;00m\n\u001b[1;32m--> 726\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mXSoftmax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    727\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n\u001b[0;32m    728\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(\n\u001b[0;32m    729\u001b[0m     attention_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), attention_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), value_layer\n\u001b[0;32m    730\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:105\u001b[0m, in \u001b[0;36mXSoftmax.forward\u001b[1;34m(ctx, input, mask, dim)\u001b[0m\n\u001b[0;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n\u001b[0;32m    103\u001b[0m rmask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39m(mask\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbool))\n\u001b[1;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmasked_fill(rmask, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmin))\n\u001b[0;32m    106\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(output, ctx\u001b[38;5;241m.\u001b[39mdim)\n\u001b[0;32m    107\u001b[0m output\u001b[38;5;241m.\u001b[39mmasked_fill_(rmask, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "check_point_path=\"./MCQA-Combined/Optuna2/trial_0/checkpoint-75540\" # 58.6% ReClore Leaderboard\n",
    "trainer = create_trainer(model_name=check_point_path,  run_name=\"Optuna3\", num_train_epochs=10)\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab977d6d",
   "metadata": {},
   "source": [
    "## LeaderBoard File generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e560ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|| 1000/1000 [17:47<00:00,  1.07s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model from the same checkpoint used during training\n",
    "#check_point_path=\"./ReClor/Optuna3/trial_17/checkpoint-3480\" # 59.5% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined/Optuna2/trial_0/checkpoint-62950\" # 57.10% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined/Optuna2/trial_0/checkpoint-75540\" # 58.6% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined/Optuna2/trial_0/checkpoint-125900\" # 55.0% ReClore Leaderboard\n",
    "\n",
    "#check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_0/checkpoint-36630\" # 71.04% Validation - 58% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_0/checkpoint-48840\" # 72.60% Validation - 57.40% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_0/checkpoint-61050\" # % Validation - 57.90% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_0/checkpoint-97680\" # 75.20% Validation - 59.6% ReClore Leaderboard\n",
    "#check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_4/checkpoint-12210\" # 60.78% Validation - 54.2% ReClore Leaderboard\n",
    "check_point_path=\"./MCQA-Combined-9/Optuna-1/trial_4/checkpoint-24420\" # 63.46% Validation - 54.9% ReClore Leaderboard\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(check_point_path)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(check_point_path)\n",
    "\n",
    "# Load test data\n",
    "with open('./ReClor/reclor_data/test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "class MultipleChoiceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, max_num_choices=5):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.max_num_choices = max_num_choices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        options = item['answers']\n",
    "        num_choices = len(options)\n",
    "        # Pad options to have max_num_choices\n",
    "        if num_choices < self.max_num_choices:\n",
    "            options += [''] * (self.max_num_choices - num_choices)\n",
    "        num_choices = self.max_num_choices\n",
    "\n",
    "        # Combine question with each option\n",
    "        second_sentences = [f\"{question} {option}\" for option in options]\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            [context] * num_choices,  # Context repeated for each option\n",
    "            second_sentences,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],  # Do not squeeze\n",
    "            'attention_mask': encoded['attention_mask'],  # Do not squeeze\n",
    "            'labels': torch.tensor(0)  # Dummy label\n",
    "        }\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "test_dataset = MultipleChoiceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prediction loop with tqdm for progress bar\n",
    "predictions = []\n",
    "model.eval()\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\", unit=\"batch\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],  # Do not squeeze\n",
    "            attention_mask=batch['attention_mask']  # Do not squeeze\n",
    "        )\n",
    "    pred = torch.argmax(outputs.logits, dim=1)\n",
    "    predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "# Create ID string mapping to predictions\n",
    "predicted_labels = {f\"test_{i}\": int(pred) for i, pred in enumerate(predictions)}\n",
    "\n",
    "# Save predictions to .npy file\n",
    "np.save('test_predictions.npy', np.array(predictions))\n",
    "\n",
    "print(\"Predictions saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b16ad82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|| 1000/1000 [16:47<00:00,  1.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMultipleChoice\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and model from the same checkpoint used during training\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./ReClor/Optuna/trial_0/checkpoint-5800\")\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"./ReClor/Optuna/trial_0/checkpoint-5800\")\n",
    "\n",
    "# Load test data\n",
    "with open('./ReClor/reclor_data/test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "class MultipleChoiceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512, max_num_choices=5):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.max_num_choices = max_num_choices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        options = item['answers']\n",
    "        num_choices = len(options)\n",
    "        # Pad options to have max_num_choices\n",
    "        if num_choices < self.max_num_choices:\n",
    "            options += [''] * (self.max_num_choices - num_choices)\n",
    "        num_choices = self.max_num_choices\n",
    "\n",
    "        # Combine question with each option\n",
    "        second_sentences = [f\"{question} {option}\" for option in options]\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            [context] * num_choices,  # Context repeated for each option\n",
    "            second_sentences,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],  # Do not squeeze\n",
    "            'attention_mask': encoded['attention_mask'],  # Do not squeeze\n",
    "            'labels': torch.tensor(0)  # Dummy label\n",
    "        }\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "test_dataset = MultipleChoiceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prediction loop with tqdm for progress bar\n",
    "predictions = []\n",
    "model.eval()\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\", unit=\"batch\"):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],  # Do not squeeze\n",
    "            attention_mask=batch['attention_mask']  # Do not squeeze\n",
    "        )\n",
    "    pred = torch.argmax(outputs.logits, dim=1)\n",
    "    predictions.extend(pred.cpu().numpy())\n",
    "\n",
    "# Create ID string mapping to predictions\n",
    "predicted_labels = {f\"test_{i}\": int(pred) for i, pred in enumerate(predictions)}\n",
    "\n",
    "# Save predictions to .npy file\n",
    "np.save('test_predictions.npy', np.array(predictions))\n",
    "\n",
    "print(\"Predictions saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52848b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))  # This will print the number of entries in the test.json\n",
    "print(len(test_dataset))  # This will also show the number of entries loaded into your dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f7913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:  66%|   | 662/1000 [09:53<05:02,  1.12batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 86\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[43mmc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mextend(pred\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 31\u001b[0m, in \u001b[0;36mDeBERTaForMultipleChoice.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m flat_input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_choices, seq_length)\n\u001b[0;32m     29\u001b[0m flat_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_choices, seq_length)\n\u001b[1;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# [batch_size*num_choices, hidden_size]\u001b[39;00m\n\u001b[0;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)  \u001b[38;5;66;03m# [batch_size*num_choices, 1]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:964\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    954\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    956\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    957\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    958\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    961\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    962\u001b[0m )\n\u001b[1;32m--> 964\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:460\u001b[0m, in \u001b[0;36mDebertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    450\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    451\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    452\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         output_attentions,\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    470\u001b[0m     hidden_states, att_m \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:373\u001b[0m, in \u001b[0;36mDebertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    366\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    372\u001b[0m ):\n\u001b[1;32m--> 373\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    382\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:306\u001b[0m, in \u001b[0;36mDebertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    299\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m ):\n\u001b[1;32m--> 306\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    315\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:651\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[0;32m    650\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[1;32m--> 651\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_att_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    654\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:713\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_att_bias\u001b[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[0;32m    711\u001b[0m     r_pos \u001b[38;5;241m=\u001b[39m relative_pos\n\u001b[0;32m    712\u001b[0m p2c_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39mr_pos \u001b[38;5;241m+\u001b[39m att_span, \u001b[38;5;241m0\u001b[39m, att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 713\u001b[0m p2c_att \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_query_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m p2c_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    715\u001b[0m     p2c_att, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mp2c_dynamic_expand(p2c_pos, query_layer, key_layer)\n\u001b[0;32m    716\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m!=\u001b[39m key_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMultipleChoice\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Load the base DeBERTa model\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-base-mnli\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base-mnli\")\n",
    "\n",
    "\n",
    "\n",
    "class DeBERTaForMultipleChoice(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.deberta = base_model\n",
    "        self.classifier = nn.Linear(self.deberta.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_choices = input_ids.size(1)\n",
    "        seq_length = input_ids.size(2)\n",
    "\n",
    "        # Flatten the batch and choice dimensions\n",
    "        flat_input_ids = input_ids.view(batch_size * num_choices, seq_length)\n",
    "        flat_attention_mask = attention_mask.view(batch_size * num_choices, seq_length)\n",
    "\n",
    "        outputs = self.deberta(input_ids=flat_input_ids, attention_mask=flat_attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size*num_choices, hidden_size]\n",
    "\n",
    "        logits = self.classifier(pooled_output)  # [batch_size*num_choices, 1]\n",
    "\n",
    "        # Reshape logits back to [batch_size, num_choices]\n",
    "        logits = logits.view(batch_size, num_choices)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize the custom multiple choice model\n",
    "mc_model = DeBERTaForMultipleChoice(model)\n",
    "\n",
    "# Load test data\n",
    "with open('./ReClor/reclor_data/test.json', 'r') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "class MultipleChoiceDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        options = item['answers']\n",
    "        encoded = self.tokenizer(\n",
    "            [context] * len(options),  # Context repeated for each option\n",
    "            options,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'],  # Keep the dimensions as is\n",
    "            'attention_mask': encoded['attention_mask'],\n",
    "            'labels': torch.tensor(item.get('question_type', 0))  # Dummy labels\n",
    "        }\n",
    "\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "test_dataset = MultipleChoiceDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Prediction loop with tqdm for progress bar\n",
    "mc_model.eval()\n",
    "predictions = []\n",
    "for batch in tqdm(test_loader, desc=\"Predicting\", unit=\"batch\"):\n",
    "    with torch.no_grad():\n",
    "        logits = mc_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    predictions.extend(pred.numpy())\n",
    "\n",
    "\n",
    "# Create ID string mapping to predictions\n",
    "predicted_labels = {f\"test_{i}\": pred for i, pred in enumerate(predictions)}\n",
    "\n",
    "# Save predictions to .npy file\n",
    "np.save('test_predictions.npy', np.array(predictions))\n",
    "\n",
    "print(\"Predictions saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc0df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
