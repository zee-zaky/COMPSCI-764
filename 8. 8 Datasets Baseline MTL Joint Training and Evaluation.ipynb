{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71614c8c-099c-4c44-91f3-5cd0ea933243",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebea513d-21f8-4f2a-b64e-e52e6acbed93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project libraries imported!\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "Device:cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from project_imports import *\n",
    "import use_gpu\n",
    "# Clear any cached memory to start fresh for each trial\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95e38-f23f-4972-a8cb-b4c1849cb726",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb17c692-f274-4f42-8985-1b42f4f1cfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Arguments and global vriables\n",
    "dataset_name=\"MCQA-Combined-8\"\n",
    "global_run_name=\"Optuna-1\"\n",
    "pretrained_model_name = \"microsoft/deberta-v3-base\"\n",
    "#pretrained_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "data_collator = DefaultDataCollator()\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = right_padding = tokenizer.padding_side == 'right'\n",
    "global_counter = 0\n",
    "traing_answer_mismatches = []\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5d0e7-f055-4058-986f-f82edc6fce8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Prepare the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38eef2e-4a62-434a-a3df-31a558fc1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1072514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 118521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 200566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('cleaned_dataset')\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b14e4b-8c9a-4cf4-817e-e8b7099bf1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize lists to hold datasets\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# List of datasets to combine without 'ReClor'\n",
    "datasets_to_combine = ['AR-LSAT', 'LogiQA 2.0', 'RTE', 'FOLIO', 'PrOntoQA', 'MRPC', 'Adversarial NLI', 'ConTRoL' ]\n",
    "\n",
    "# Loop through each dataset and filter\n",
    "for ds_name in datasets_to_combine:\n",
    "    train_datasets.append(combined_dataset['train'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "    val_datasets.append(combined_dataset['validation'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "    test_datasets.append(combined_dataset['test'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_train = concatenate_datasets(train_datasets)\n",
    "combined_val = concatenate_datasets(val_datasets)\n",
    "combined_test = concatenate_datasets(test_datasets)\n",
    "\n",
    "# Concatenate validation data into the training data and use the Test dataset for validation \n",
    "combined_train = concatenate_datasets([combined_train, combined_val])\n",
    "combined_val = combined_test\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "# To ensure that each training batch has a chance to contain a mix of examples from all sources. \n",
    "# This helps in reducing variance and improving the generalization of the model.\n",
    "combined_train = combined_train.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944988bb-97a9-4daf-80a6-743daea9b62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mcqa_preprocess_function(examples):\n",
    "    # Determine the maximum number of choices\n",
    "    max_num_choices = 5  # Since AR-LSAT has 5 options, we'll pad others to 5\n",
    "    contexts = examples['Context']\n",
    "    questions = examples['Question']\n",
    "    options_list = examples['Options']\n",
    "    labels = examples['Label']\n",
    "    \n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    labels_adjusted = []\n",
    "    \n",
    "    for context, question, options, label in zip(contexts, questions, options_list, labels):\n",
    "        num_choices = len(options)\n",
    "        # Pad options to have max_num_choices\n",
    "        if num_choices < max_num_choices:\n",
    "            options += [''] * (max_num_choices - num_choices)\n",
    "        first_sentences.append([context] * max_num_choices)\n",
    "        second_sentences.append([f\"{question} {option}\" for option in options])\n",
    "        labels_adjusted.append(label)\n",
    "    \n",
    "    # Flatten the lists\n",
    "    first_sentences = [item for sublist in first_sentences for item in sublist]\n",
    "    second_sentences = [item for sublist in second_sentences for item in sublist]\n",
    "    \n",
    "    # Tokenize the inputs\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    # Un-flatten to shape (num_examples, max_num_choices, seq_length)\n",
    "    tokenized_inputs = {\n",
    "        k: [v[i:i + max_num_choices] for i in range(0, len(v), max_num_choices)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Labels\n",
    "    tokenized_inputs[\"labels\"] = labels_adjusted\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29057a47-41c8-4073-ad1c-6e08a04ed14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd781ede319432e8667794e137c407a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply the preprocessing function to the combined datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoded_train \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcqa_preprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m encoded_val \u001b[38;5;241m=\u001b[39m combined_val\u001b[38;5;241m.\u001b[39mmap(mcqa_preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m encoded_test \u001b[38;5;241m=\u001b[39m combined_test\u001b[38;5;241m.\u001b[39mmap(mcqa_preprocess_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_dataset.py:3167\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3163\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3164\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3165\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3166\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3167\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3168\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3169\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_dataset.py:3581\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3579\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3581\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3582\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_writer.py:568\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    566\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m try_features[col] \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    567\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 568\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    569\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    570\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\pyarrow\\array.pxi:247\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\pyarrow\\array.pxi:112\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\datasets\\arrow_writer.py:193\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_int_optimization:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply the preprocessing function to the combined datasets\n",
    "encoded_train = combined_train.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_val = combined_val.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_test = combined_test.map(mcqa_preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27035bf4-f8bd-4e10-8b8e-80e6c33cb9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 31993\n",
      "Number of validation examples: 5069\n",
      "Number of test examples: 5069\n"
     ]
    }
   ],
   "source": [
    "# Set the format of the datasets to PyTorch tensors\n",
    "encoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "def get_train_encoded():\n",
    "    return encoded_train\n",
    "\n",
    "def get_val_encoded():\n",
    "    return encoded_val\n",
    "\n",
    "def get_test_encoded():\n",
    "    return encoded_test\n",
    "\n",
    "\n",
    "print(\"Number of training examples:\", len(encoded_train))\n",
    "print(\"Number of validation examples:\", len(encoded_val))\n",
    "print(\"Number of test examples:\", len(encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1af4-7b57-4c74-bcd9-48edefba6747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8384ce-d55e-4aa1-911b-7d8ac3ab07df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'eval_accuracy': acc, 'eval_f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62ae25b-b560-4182-a68d-e60ab612bc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_training_args(run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=3):\n",
    "    \"\"\"\n",
    "    Generates training arguments for training a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): The name of the dataset.\n",
    "    - run_name (str): The name of the run, useful for logging and saving models.\n",
    "    - model_name (str): The name of the model, typically including its configuration.\n",
    "    - num_train_epochs (int): The number of epochs to train for.\n",
    "    - learning_rate (float): The learning rate for training.\n",
    "    - batch_size (int): The batch size used for training.\n",
    "\n",
    "    Returns:\n",
    "    - TrainingArguments: A configured TrainingArguments instance.\n",
    "    \"\"\"    \n",
    "    output_dir = f\"./{dataset_name}/{run_name}/{normalized_model_name}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=398,\n",
    "        weight_decay=0.194,\n",
    "        adam_beta1=0.837,\n",
    "        adam_beta2=0.997,\n",
    "        adam_epsilon=5.87e-07,\n",
    "        lr_scheduler_type='cosine',\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    )\n",
    "    \n",
    "    return training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0f0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name=pretrained_model_name, dropout_rate=0.1):\n",
    "    model = AutoModelForMultipleChoice.from_pretrained(model_name)\n",
    "    model.config.hidden_dropout_prob = dropout_rate\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fbc498-f422-4980-9a8c-bbcf0852c992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(model_name=pretrained_model_name,run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=4):\n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name),\n",
    "        args=create_training_args(run_name=run_name, num_train_epochs=num_train_epochs, learning_rate=learning_rate, batch_size=batch_size),\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af7e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to stop training when either the performance falls below a certain threshold\n",
    "    or if there is no improvement over a set number of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_name, patience):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience        \n",
    "        self.best_score = None\n",
    "        self.no_improve_epochs = 0\n",
    "        self.config_file = \"early_stopping_config.json\"  # Config file for early stopping values\n",
    "\n",
    "    def read_early_stopping_config(self):\n",
    "        \"\"\"\n",
    "        Reads the early stopping configuration from the file system.\n",
    "        Returns the configuration as a dictionary.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.config_file):\n",
    "            with open(self.config_file, 'r') as file:\n",
    "                config = json.load(file)\n",
    "            return config\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Config file not found: {self.config_file}\")\n",
    "    def reset_manual_stop_flag(self):\n",
    "        \"\"\"\n",
    "        Resets the manual stop flag to False in the early stopping config file.\n",
    "        \"\"\"\n",
    "        config = self.read_early_stopping_config()\n",
    "        config['manual_stop'] = False\n",
    "        with open(self.config_file, 'w') as file:\n",
    "            json.dump(config, file, indent=4)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric_value = kwargs['metrics'].get(self.metric_name)\n",
    "\n",
    "        if self.best_score is None or metric_value > self.best_score:\n",
    "            self.best_score = metric_value\n",
    "            self.no_improve_epochs = 0\n",
    "        else:\n",
    "            self.no_improve_epochs += 1\n",
    "\n",
    "        # Check if no improvement has been seen over the allowed patience\n",
    "        if self.no_improve_epochs >= self.patience:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: No improvement in {self.metric_name} for {self.patience} epochs\")\n",
    "\n",
    "\n",
    "        # Read the early stopping configuration\n",
    "        config = self.read_early_stopping_config()\n",
    "        min_accuracy = config.get(\"min_accuracy\", 0.35)                \n",
    "        num_epochs_min_acc = config.get(\"num_epochs_min_acc\", 2)  \n",
    "        max_variance = config.get(\"max_variance\", 0.2)  \n",
    "\n",
    "        # Check if performance is below the threshold\n",
    "        if metric_value < min_accuracy:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: {self.metric_name} below manual min_acc of {min_accuracy}\")\n",
    "\n",
    "         # Manual stop from config\n",
    "        if config.get(\"manual_stop\", False):\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Manual early stopping triggered!!\")\n",
    "            self.reset_manual_stop_flag()  # Reset the flag for future runs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a456ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Clear any cached memory to start fresh for each trial\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    model_name = trial.suggest_categorical('model_name', [pretrained_model_name])     \n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [3, 3])\n",
    "    #warmup_steps = trial.suggest_int('warmup_steps', 0, 1000)\n",
    "    warmup_ratio= trial.suggest_float('warmup_ratio', 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.25)\n",
    "    adam_beta1 = trial.suggest_float('adam_beta1', 0.8, 0.95)\n",
    "    adam_beta2 = trial.suggest_float('adam_beta2', 0.990, 0.999)\n",
    "    adam_epsilon = trial.suggest_float('adam_epsilon', 1e-8, 1e-6)\n",
    "    lr_scheduler_type = trial.suggest_categorical('lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts']) #,'constant_with_warmup'   \n",
    "    \n",
    "\n",
    "    output_dir = f\"./{dataset_name}/{global_run_name}/trial_{trial.number}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=30,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    ) \n",
    "    \n",
    "    # Print trial parameters\n",
    "    print(f\"Current Trial {trial.number} parameters: {trial.params}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name, dropout_rate),\n",
    "        args=training_args,\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[AdvancedEarlyStoppingCallback(metric_name='eval_accuracy', patience=1)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "        \n",
    "    torch.cuda.empty_cache()  # Clear cache after evaluation\n",
    "    gc.collect()  # Collect garbage\n",
    "\n",
    "    return eval_results['eval_accuracy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9b86e-63c3-4ee8-bb91-bbb43e5a6481",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. DeBERTa Joint MTL Training on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362fbe0",
   "metadata": {},
   "source": [
    "## 4.1 Optuna Hyperparameters Tuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03dcd721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 13:46:11,719] A new study created in memory with name: no-name-29336d1f-a95e-497e-b1de-30ae37010f92\n",
      "[W 2024-10-22 13:46:11,858] Trial 0 failed with parameters: {} because of the following error: ValueError(\"'microsoft/deberta-v3-base' not in ('sentence-transformers/all-mpnet-base-v2',).\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\OEM\\AppData\\Local\\Temp\\ipykernel_34512\\2979044354.py\", line 8, in objective\n",
      "    model_name = trial.suggest_categorical('model_name', [pretrained_model_name])\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 402, in suggest_categorical\n",
      "    return self._suggest(name, CategoricalDistribution(choices=choices))\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 623, in _suggest\n",
      "    if self._is_fixed_param(name, distribution):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py\", line 648, in _is_fixed_param\n",
      "    param_value_in_internal_repr = distribution.to_internal_repr(param_value)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\distributions.py\", line 540, in to_internal_repr\n",
      "    raise ValueError(f\"'{param_value_in_external_repr}' not in {self.choices}.\")\n",
      "ValueError: 'microsoft/deberta-v3-base' not in ('sentence-transformers/all-mpnet-base-v2',).\n",
      "[W 2024-10-22 13:46:11,860] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'microsoft/deberta-v3-base' not in ('sentence-transformers/all-mpnet-base-v2',).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m study\u001b[38;5;241m.\u001b[39menqueue_trial({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v3-base\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0052258285035737e-05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.04149176551014211\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.006685281279171756\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9429922176765829\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9918592948813898\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8.867767549079712e-08\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_scheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_with_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#study.enqueue_trial({'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      5\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m----> 8\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m     \n\u001b[0;32m      9\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     10\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1e-7\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py:402\u001b[0m, in \u001b[0;36mTrial.suggest_categorical\u001b[1;34m(self, name, choices)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Suggest a value for the categorical parameter.\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mThe value is sampled from ``choices``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    :ref:`configurations` tutorial describes more details and flexible usages.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# There is no need to call self._check_distribution because\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# CategoricalDistribution does not support dynamic value space.\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_suggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCategoricalDistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py:623\u001b[0m, in \u001b[0;36mTrial._suggest\u001b[1;34m(self, name, distribution)\u001b[0m\n\u001b[0;32m    621\u001b[0m     param_value \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mparams[name]\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_fixed_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    624\u001b[0m         param_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixed_params[name]\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m distribution\u001b[38;5;241m.\u001b[39msingle():\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\trial\\_trial.py:648\u001b[0m, in \u001b[0;36mTrial._is_fixed_param\u001b[1;34m(self, name, distribution)\u001b[0m\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    647\u001b[0m param_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixed_params[name]\n\u001b[1;32m--> 648\u001b[0m param_value_in_internal_repr \u001b[38;5;241m=\u001b[39m \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_internal_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    650\u001b[0m contained \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39m_contains(param_value_in_internal_repr)\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contained:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\distributions.py:540\u001b[0m, in \u001b[0;36mCategoricalDistribution.to_internal_repr\u001b[1;34m(self, param_value_in_external_repr)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _categorical_choice_equal(param_value_in_external_repr, choice):\n\u001b[0;32m    538\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m index\n\u001b[1;32m--> 540\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_value_in_external_repr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: 'microsoft/deberta-v3-base' not in ('sentence-transformers/all-mpnet-base-v2',)."
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective\n",
    "global_run_name=\"Optuna-1\"\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.enqueue_trial({'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "#study.enqueue_trial({'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.optimize(objective, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4db3c9",
   "metadata": {},
   "source": [
    "## 4.3 Optuna Hyperparameters Tuning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9169a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 01:57:26,347] A new study created in memory with name: no-name-198a630a-9e5c-4ede-ae18-36aabe3fac1a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 0 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.15718330934471478, 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135ca6e0a7d442a8bbd5136dac6452ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6088, 'grad_norm': 0.9182241559028625, 'learning_rate': 4.780025580617451e-08, 'epoch': 0.05}\n",
      "{'loss': 1.6088, 'grad_norm': 0.7863313555717468, 'learning_rate': 9.569649606175901e-08, 'epoch': 0.11}\n",
      "{'loss': 1.6093, 'grad_norm': 0.7911553978919983, 'learning_rate': 1.4368872076675347e-07, 'epoch': 0.16}\n",
      "{'loss': 1.6085, 'grad_norm': 0.8689716458320618, 'learning_rate': 1.91488976572928e-07, 'epoch': 0.22}\n",
      "{'loss': 1.6076, 'grad_norm': 0.8607678413391113, 'learning_rate': 2.3948120127792246e-07, 'epoch': 0.27}\n",
      "{'loss': 1.6065, 'grad_norm': 1.0950431823730469, 'learning_rate': 2.8737744153350694e-07, 'epoch': 0.33}\n",
      "{'loss': 1.6024, 'grad_norm': 1.1426678895950317, 'learning_rate': 3.3536966623850147e-07, 'epoch': 0.38}\n",
      "{'loss': 1.5868, 'grad_norm': 1.5567299127578735, 'learning_rate': 3.8336189094349596e-07, 'epoch': 0.44}\n",
      "{'loss': 1.5336, 'grad_norm': 2.8619110584259033, 'learning_rate': 4.3135411564849044e-07, 'epoch': 0.49}\n",
      "{'loss': 1.4498, 'grad_norm': 3.4570772647857666, 'learning_rate': 4.792503559040749e-07, 'epoch': 0.55}\n",
      "{'loss': 1.3822, 'grad_norm': 4.822655200958252, 'learning_rate': 5.270506117102494e-07, 'epoch': 0.6}\n",
      "{'loss': 1.3307, 'grad_norm': 9.937758445739746, 'learning_rate': 5.750428364152439e-07, 'epoch': 0.66}\n",
      "{'loss': 1.3258, 'grad_norm': 14.819893836975098, 'learning_rate': 6.230350611202384e-07, 'epoch': 0.71}\n",
      "{'loss': 1.3088, 'grad_norm': 37.391571044921875, 'learning_rate': 6.710272858252329e-07, 'epoch': 0.76}\n",
      "{'loss': 1.2735, 'grad_norm': 16.22966194152832, 'learning_rate': 7.190195105302274e-07, 'epoch': 0.82}\n",
      "{'loss': 1.2904, 'grad_norm': 15.184688568115234, 'learning_rate': 7.670117352352218e-07, 'epoch': 0.87}\n",
      "{'loss': 1.267, 'grad_norm': 12.10547924041748, 'learning_rate': 8.149079754908063e-07, 'epoch': 0.93}\n",
      "{'loss': 1.2554, 'grad_norm': 7.877290725708008, 'learning_rate': 8.628042157463908e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b979213031034d029f5fdb1431c24689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.45789190159813253, 'eval_f1': 0.4441435889737021, 'eval_loss': 1.154205560684204, 'eval_runtime': 130.2404, 'eval_samples_per_second': 42.759, 'eval_steps_per_second': 10.696, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "{'train_runtime': 2960.9734, 'train_samples_per_second': 371.128, 'train_steps_per_second': 92.787, 'train_loss': 1.4549762686675072, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f32010e9d941788cab35b01cf2953a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 02:48:58,638] Trial 0 finished with value: 0.45789190159813253 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.15718330934471478, 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.45789190159813253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "Current Trial 1 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.3046487955829154, 'learning_rate': 4.1206992440781206e-07, 'batch_size': 3, 'warmup_ratio': 0.07116169962743613, 'weight_decay': 0.2388626303853753, 'adam_beta1': 0.8422162512825583, 'adam_beta2': 0.9904831834704457, 'adam_epsilon': 5.201468169219309e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f601bfb1ec43e38bd0e05d760dbc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6105, 'grad_norm': 1.0038748979568481, 'learning_rate': 7.888245378428597e-09, 'epoch': 0.04}\n",
      "{'loss': 1.6097, 'grad_norm': 0.9802391529083252, 'learning_rate': 1.5776490756857193e-08, 'epoch': 0.08}\n",
      "{'loss': 1.6096, 'grad_norm': 0.9342759847640991, 'learning_rate': 2.364892802831499e-08, 'epoch': 0.12}\n",
      "{'loss': 1.6092, 'grad_norm': 0.9232449531555176, 'learning_rate': 3.155298151371439e-08, 'epoch': 0.16}\n",
      "{'loss': 1.6099, 'grad_norm': 0.9416474103927612, 'learning_rate': 3.945703499911378e-08, 'epoch': 0.2}\n",
      "{'loss': 1.6095, 'grad_norm': 0.9696550369262695, 'learning_rate': 4.734528037754238e-08, 'epoch': 0.25}\n",
      "{'loss': 1.6097, 'grad_norm': 1.3210350275039673, 'learning_rate': 5.524933386294177e-08, 'epoch': 0.29}\n",
      "{'loss': 1.6095, 'grad_norm': 0.9429779648780823, 'learning_rate': 6.315338734834116e-08, 'epoch': 0.33}\n",
      "{'loss': 1.6089, 'grad_norm': 1.0847434997558594, 'learning_rate': 7.105744083374056e-08, 'epoch': 0.37}\n",
      "{'loss': 1.6081, 'grad_norm': 1.0135345458984375, 'learning_rate': 7.896149431913996e-08, 'epoch': 0.41}\n",
      "{'loss': 1.609, 'grad_norm': 0.9849110841751099, 'learning_rate': 8.686554780453936e-08, 'epoch': 0.45}\n",
      "{'loss': 1.6082, 'grad_norm': 1.1165719032287598, 'learning_rate': 9.476960128993875e-08, 'epoch': 0.49}\n",
      "{'loss': 1.6083, 'grad_norm': 1.0244745016098022, 'learning_rate': 1.0267365477533815e-07, 'epoch': 0.53}\n",
      "{'loss': 1.608, 'grad_norm': 1.0121873617172241, 'learning_rate': 1.1057770826073753e-07, 'epoch': 0.57}\n",
      "{'loss': 1.6074, 'grad_norm': 1.053575873374939, 'learning_rate': 1.1846595363916614e-07, 'epoch': 0.61}\n",
      "{'loss': 1.6058, 'grad_norm': 5.690029144287109, 'learning_rate': 1.2635419901759473e-07, 'epoch': 0.66}\n",
      "{'loss': 1.6059, 'grad_norm': 1.0543270111083984, 'learning_rate': 1.3425825250299413e-07, 'epoch': 0.7}\n",
      "{'loss': 1.605, 'grad_norm': 1.243895411491394, 'learning_rate': 1.4216230598839353e-07, 'epoch': 0.74}\n",
      "{'loss': 1.6024, 'grad_norm': 1.2252775430679321, 'learning_rate': 1.5006635947379293e-07, 'epoch': 0.78}\n",
      "{'loss': 1.5998, 'grad_norm': 1.137698769569397, 'learning_rate': 1.579704129591923e-07, 'epoch': 0.82}\n",
      "{'loss': 1.5939, 'grad_norm': 1.5133697986602783, 'learning_rate': 1.658744664445917e-07, 'epoch': 0.86}\n",
      "{'loss': 1.5872, 'grad_norm': 1.272009015083313, 'learning_rate': 1.737785199299911e-07, 'epoch': 0.9}\n",
      "{'loss': 1.5707, 'grad_norm': 1.8961106538772583, 'learning_rate': 1.816825734153905e-07, 'epoch': 0.94}\n",
      "{'loss': 1.5509, 'grad_norm': 2.1738815307617188, 'learning_rate': 1.895708187938191e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989396a4198e42d2aa7bdf1bef4846d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4410127491470641, 'eval_f1': 0.43217578765164366, 'eval_loss': 1.4569653272628784, 'eval_runtime': 129.4868, 'eval_samples_per_second': 43.008, 'eval_steps_per_second': 14.341, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "{'train_runtime': 2948.5397, 'train_samples_per_second': 372.693, 'train_steps_per_second': 124.231, 'train_loss': 1.6010893125791807, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94105c42ddd7453b8ffbe8577dd43199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 03:40:18,641] Trial 1 finished with value: 0.4410127491470641 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.3046487955829154, 'learning_rate': 4.1206992440781206e-07, 'batch_size': 3, 'warmup_ratio': 0.07116169962743613, 'weight_decay': 0.2388626303853753, 'adam_beta1': 0.8422162512825583, 'adam_beta2': 0.9904831834704457, 'adam_epsilon': 5.201468169219309e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.45789190159813253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "Current Trial 2 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.10926740728558144, 'learning_rate': 1.335608479306097e-05, 'batch_size': 3, 'warmup_ratio': 0.41012687080315513, 'weight_decay': 0.09479782297028039, 'adam_beta1': 0.912797523704336, 'adam_beta2': 0.997103226208837, 'adam_epsilon': 7.229757891985637e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ad89151cea450b88a3e22acdf99d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6105, 'grad_norm': 1.0041431188583374, 'learning_rate': 4.418540998569728e-08, 'epoch': 0.04}\n",
      "{'loss': 1.6095, 'grad_norm': 0.9794089198112488, 'learning_rate': 8.863753270772673e-08, 'epoch': 0.08}\n",
      "{'loss': 1.6093, 'grad_norm': 0.9342834949493408, 'learning_rate': 1.330896554297562e-07, 'epoch': 0.12}\n",
      "{'loss': 1.6086, 'grad_norm': 0.9255183935165405, 'learning_rate': 1.774528739063416e-07, 'epoch': 0.16}\n",
      "{'loss': 1.609, 'grad_norm': 0.9468817710876465, 'learning_rate': 2.2190499662837103e-07, 'epoch': 0.2}\n",
      "{'loss': 1.6078, 'grad_norm': 0.9842406511306763, 'learning_rate': 2.6626821510495643e-07, 'epoch': 0.25}\n",
      "{'loss': 1.6068, 'grad_norm': 1.0521067380905151, 'learning_rate': 3.1072033782698587e-07, 'epoch': 0.29}\n",
      "{'loss': 1.6038, 'grad_norm': 1.0336287021636963, 'learning_rate': 3.5517246054901536e-07, 'epoch': 0.33}\n",
      "{'loss': 1.5938, 'grad_norm': 1.386725902557373, 'learning_rate': 3.996245832710448e-07, 'epoch': 0.37}\n",
      "{'loss': 1.5667, 'grad_norm': 1.9762277603149414, 'learning_rate': 4.440767059930743e-07, 'epoch': 0.41}\n",
      "{'loss': 1.5031, 'grad_norm': 4.502912521362305, 'learning_rate': 4.882621159787716e-07, 'epoch': 0.45}\n",
      "{'loss': 1.4316, 'grad_norm': 3.5446276664733887, 'learning_rate': 5.327142387008009e-07, 'epoch': 0.49}\n",
      "{'loss': 1.3792, 'grad_norm': 8.139463424682617, 'learning_rate': 5.771663614228305e-07, 'epoch': 0.53}\n",
      "{'loss': 1.3415, 'grad_norm': 1.9827455282211304, 'learning_rate': 6.215295798994158e-07, 'epoch': 0.57}\n",
      "{'loss': 1.3302, 'grad_norm': 19.97446060180664, 'learning_rate': 6.658927983760013e-07, 'epoch': 0.61}\n",
      "{'loss': 1.3296, 'grad_norm': 30.350744247436523, 'learning_rate': 7.103449210980307e-07, 'epoch': 0.66}\n",
      "{'loss': 1.3047, 'grad_norm': 35.56308364868164, 'learning_rate': 7.547970438200602e-07, 'epoch': 0.7}\n",
      "{'loss': 1.3144, 'grad_norm': 16.744962692260742, 'learning_rate': 7.992491665420896e-07, 'epoch': 0.74}\n",
      "{'loss': 1.2778, 'grad_norm': 16.832265853881836, 'learning_rate': 8.436123850186751e-07, 'epoch': 0.78}\n",
      "{'loss': 1.262, 'grad_norm': 17.182348251342773, 'learning_rate': 8.880645077407045e-07, 'epoch': 0.82}\n",
      "{'loss': 1.2796, 'grad_norm': 5.064531326293945, 'learning_rate': 9.325166304627339e-07, 'epoch': 0.86}\n",
      "{'loss': 1.268, 'grad_norm': 21.449968338012695, 'learning_rate': 9.769687531847633e-07, 'epoch': 0.9}\n",
      "{'loss': 1.2336, 'grad_norm': 10.136995315551758, 'learning_rate': 1.021420875906793e-06, 'epoch': 0.94}\n",
      "{'loss': 1.234, 'grad_norm': 12.402498245239258, 'learning_rate': 1.0658729986288224e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d91ab4559de41638b41882819f97d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.46004668701741785, 'eval_f1': 0.4511414731237447, 'eval_loss': 1.158411979675293, 'eval_runtime': 129.244, 'eval_samples_per_second': 43.089, 'eval_steps_per_second': 14.368, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "{'train_runtime': 2948.6868, 'train_samples_per_second': 372.674, 'train_steps_per_second': 124.225, 'train_loss': 1.434618428298238, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334eb3908e5b402fa327d414fc6e617c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 04:31:38,277] Trial 2 finished with value: 0.46004668701741785 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.10926740728558144, 'learning_rate': 1.335608479306097e-05, 'batch_size': 3, 'warmup_ratio': 0.41012687080315513, 'weight_decay': 0.09479782297028039, 'adam_beta1': 0.912797523704336, 'adam_beta2': 0.997103226208837, 'adam_epsilon': 7.229757891985637e-07, 'lr_scheduler_type': 'linear'}. Best is trial 2 with value: 0.46004668701741785.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "Current Trial 3 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.1816376875451311, 'learning_rate': 5.4982191466617046e-05, 'batch_size': 4, 'warmup_ratio': 0.6229699420810516, 'weight_decay': 0.07934363728473695, 'adam_beta1': 0.9386008693968517, 'adam_beta2': 0.9935052156642877, 'adam_epsilon': 6.932517927990338e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6282742a40745f7a046d670480aa762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6095, 'grad_norm': 0.8916115760803223, 'learning_rate': 1.5997856533770726e-07, 'epoch': 0.05}\n",
      "{'loss': 1.6094, 'grad_norm': 0.935311496257782, 'learning_rate': 3.2027837277448625e-07, 'epoch': 0.11}\n",
      "{'loss': 1.6084, 'grad_norm': 0.7695035338401794, 'learning_rate': 4.808994223103369e-07, 'epoch': 0.16}\n",
      "{'loss': 1.6056, 'grad_norm': 1.04118013381958, 'learning_rate': 6.415204718461876e-07, 'epoch': 0.22}\n",
      "{'loss': 1.5815, 'grad_norm': 3.112593173980713, 'learning_rate': 8.021415213820383e-07, 'epoch': 0.27}\n",
      "{'loss': 1.4552, 'grad_norm': 16.392295837402344, 'learning_rate': 9.62441328818817e-07, 'epoch': 0.33}\n",
      "{'loss': 1.3405, 'grad_norm': 22.638620376586914, 'learning_rate': 1.1224198941565245e-06, 'epoch': 0.38}\n",
      "{'loss': 1.2959, 'grad_norm': 16.598974227905273, 'learning_rate': 1.2823984594942319e-06, 'epoch': 0.44}\n",
      "{'loss': 1.2856, 'grad_norm': 16.50101089477539, 'learning_rate': 1.4430195090300825e-06, 'epoch': 0.49}\n",
      "{'loss': 1.288, 'grad_norm': 3.342400312423706, 'learning_rate': 1.6033193164668614e-06, 'epoch': 0.55}\n",
      "{'loss': 1.2777, 'grad_norm': 55.011451721191406, 'learning_rate': 1.7636191239036405e-06, 'epoch': 0.6}\n",
      "{'loss': 1.2367, 'grad_norm': 11.459199905395508, 'learning_rate': 1.924240173439491e-06, 'epoch': 0.66}\n",
      "{'loss': 1.2335, 'grad_norm': 7.066983222961426, 'learning_rate': 2.084861222975342e-06, 'epoch': 0.71}\n",
      "{'loss': 1.2325, 'grad_norm': 29.7535343170166, 'learning_rate': 2.2454822725111924e-06, 'epoch': 0.76}\n",
      "{'loss': 1.2092, 'grad_norm': 23.53926658630371, 'learning_rate': 2.4061033220470435e-06, 'epoch': 0.82}\n",
      "{'loss': 1.2223, 'grad_norm': 11.238018989562988, 'learning_rate': 2.566724371582894e-06, 'epoch': 0.87}\n",
      "{'loss': 1.1995, 'grad_norm': 14.918429374694824, 'learning_rate': 2.7273454211187447e-06, 'epoch': 0.93}\n",
      "{'loss': 1.2015, 'grad_norm': 10.35622501373291, 'learning_rate': 2.88700274435738e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df45993fe7040a999cb74836513b29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4720775722750943, 'eval_f1': 0.46856230684954503, 'eval_loss': 1.133382797241211, 'eval_runtime': 129.7892, 'eval_samples_per_second': 42.908, 'eval_steps_per_second': 10.733, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "{'train_runtime': 2899.806, 'train_samples_per_second': 378.956, 'train_steps_per_second': 94.744, 'train_loss': 1.357816904021757, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6470e54e4548b890c5732f4ea9b528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 05:22:09,212] Trial 3 finished with value: 0.4720775722750943 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.1816376875451311, 'learning_rate': 5.4982191466617046e-05, 'batch_size': 4, 'warmup_ratio': 0.6229699420810516, 'weight_decay': 0.07934363728473695, 'adam_beta1': 0.9386008693968517, 'adam_beta2': 0.9935052156642877, 'adam_epsilon': 6.932517927990338e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 3 with value: 0.4720775722750943.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "Current Trial 4 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.2500228054734358, 'learning_rate': 6.331007353356651e-05, 'batch_size': 3, 'warmup_ratio': 0.31837920111611373, 'weight_decay': 0.09971031186766507, 'adam_beta1': 0.8682929292594384, 'adam_beta2': 0.9909672212836867, 'adam_epsilon': 5.424397228138408e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2df9610c9a459f8c165bf5513b07f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6105, 'grad_norm': 1.003128170967102, 'learning_rate': 2.703447572066927e-07, 'epoch': 0.04}\n",
      "{'loss': 1.6088, 'grad_norm': 0.9811621904373169, 'learning_rate': 5.417752363298781e-07, 'epoch': 0.08}\n",
      "{'loss': 1.6068, 'grad_norm': 1.012223720550537, 'learning_rate': 8.132057154530636e-07, 'epoch': 0.12}\n",
      "{'loss': 1.5915, 'grad_norm': 1.8195114135742188, 'learning_rate': 1.0840933336180026e-06, 'epoch': 0.16}\n",
      "{'loss': 1.472, 'grad_norm': 4.724691390991211, 'learning_rate': 1.3544380908246952e-06, 'epoch': 0.2}\n",
      "{'loss': 1.3579, 'grad_norm': 15.955487251281738, 'learning_rate': 1.6258685699478809e-06, 'epoch': 0.25}\n",
      "{'loss': 1.3038, 'grad_norm': 37.59272384643555, 'learning_rate': 1.8972990490710663e-06, 'epoch': 0.29}\n",
      "{'loss': 1.2731, 'grad_norm': 18.960540771484375, 'learning_rate': 2.167643806277759e-06, 'epoch': 0.33}\n",
      "{'loss': 1.2412, 'grad_norm': 6.070656776428223, 'learning_rate': 2.439074285400944e-06, 'epoch': 0.37}\n",
      "{'loss': 1.2635, 'grad_norm': 17.87575912475586, 'learning_rate': 2.71050476452413e-06, 'epoch': 0.41}\n",
      "{'loss': 1.2377, 'grad_norm': 10.940373420715332, 'learning_rate': 2.981392382689069e-06, 'epoch': 0.45}\n",
      "{'loss': 1.2378, 'grad_norm': 18.635204315185547, 'learning_rate': 3.2528228618122544e-06, 'epoch': 0.49}\n",
      "{'loss': 1.2384, 'grad_norm': 5.704532146453857, 'learning_rate': 3.52425334093544e-06, 'epoch': 0.53}\n",
      "{'loss': 1.2208, 'grad_norm': 20.00847625732422, 'learning_rate': 3.7956838200586254e-06, 'epoch': 0.57}\n",
      "{'loss': 1.2082, 'grad_norm': 35.61050033569336, 'learning_rate': 4.06711429918181e-06, 'epoch': 0.61}\n",
      "{'loss': 1.1778, 'grad_norm': 8.443663597106934, 'learning_rate': 4.338544778304996e-06, 'epoch': 0.66}\n",
      "{'loss': 1.1779, 'grad_norm': 22.02908706665039, 'learning_rate': 4.609975257428182e-06, 'epoch': 0.7}\n",
      "{'loss': 1.2114, 'grad_norm': 9.237483024597168, 'learning_rate': 4.881405736551367e-06, 'epoch': 0.74}\n",
      "{'loss': 1.1791, 'grad_norm': 12.181902885437012, 'learning_rate': 5.152836215674552e-06, 'epoch': 0.78}\n",
      "{'loss': 1.1844, 'grad_norm': 14.46401309967041, 'learning_rate': 5.423723833839492e-06, 'epoch': 0.82}\n",
      "{'loss': 1.1798, 'grad_norm': 193.4146270751953, 'learning_rate': 5.69461145200443e-06, 'epoch': 0.86}\n",
      "{'loss': 1.1902, 'grad_norm': 21.213825225830078, 'learning_rate': 5.9660419311276156e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1694, 'grad_norm': 10.657881736755371, 'learning_rate': 6.237472410250802e-06, 'epoch': 0.94}\n",
      "{'loss': 1.1625, 'grad_norm': 14.470998764038086, 'learning_rate': 6.508902889373987e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46a1f1875bd4dfd970f7a58bdb10f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.51212066798348, 'eval_f1': 0.5008762224153099, 'eval_loss': 1.0327595472335815, 'eval_runtime': 129.2988, 'eval_samples_per_second': 43.071, 'eval_steps_per_second': 14.362, 'epoch': 1.0}\n",
      "{'loss': 1.1207, 'grad_norm': 25.40252113342285, 'learning_rate': 6.780333368497172e-06, 'epoch': 1.02}\n",
      "{'loss': 1.1143, 'grad_norm': 19.99736213684082, 'learning_rate': 7.051220986662111e-06, 'epoch': 1.06}\n",
      "{'loss': 1.1162, 'grad_norm': 15.271824836730957, 'learning_rate': 7.322108604827051e-06, 'epoch': 1.11}\n",
      "{'loss': 1.1192, 'grad_norm': 30.091594696044922, 'learning_rate': 7.593539083950236e-06, 'epoch': 1.15}\n",
      "{'loss': 1.1263, 'grad_norm': 20.144153594970703, 'learning_rate': 7.864969563073422e-06, 'epoch': 1.19}\n",
      "{'loss': 1.0935, 'grad_norm': 9.870036125183105, 'learning_rate': 8.136400042196607e-06, 'epoch': 1.23}\n",
      "{'loss': 1.1304, 'grad_norm': 19.627023696899414, 'learning_rate': 8.407830521319792e-06, 'epoch': 1.27}\n",
      "{'loss': 1.1352, 'grad_norm': 30.71637535095215, 'learning_rate': 8.679261000442978e-06, 'epoch': 1.31}\n",
      "{'loss': 1.115, 'grad_norm': 5.224144458770752, 'learning_rate': 8.950691479566163e-06, 'epoch': 1.35}\n",
      "{'loss': 1.1172, 'grad_norm': 22.929309844970703, 'learning_rate': 9.222121958689349e-06, 'epoch': 1.39}\n",
      "{'loss': 1.0987, 'grad_norm': 84.35053253173828, 'learning_rate': 9.493009576854289e-06, 'epoch': 1.43}\n",
      "{'loss': 1.1049, 'grad_norm': 35.65603256225586, 'learning_rate': 9.764440055977474e-06, 'epoch': 1.47}\n",
      "{'loss': 1.1362, 'grad_norm': 17.36473274230957, 'learning_rate': 1.0035870535100658e-05, 'epoch': 1.52}\n",
      "{'loss': 1.1173, 'grad_norm': 7.902273178100586, 'learning_rate': 1.0307301014223845e-05, 'epoch': 1.56}\n",
      "{'loss': 1.0962, 'grad_norm': 13.83945369720459, 'learning_rate': 1.057873149334703e-05, 'epoch': 1.6}\n",
      "{'loss': 1.068, 'grad_norm': 13.668889999389648, 'learning_rate': 1.0850161972470216e-05, 'epoch': 1.64}\n",
      "{'loss': 1.1079, 'grad_norm': 23.647907257080078, 'learning_rate': 1.11215924515934e-05, 'epoch': 1.68}\n",
      "{'loss': 1.0831, 'grad_norm': 12.186593055725098, 'learning_rate': 1.1393022930716586e-05, 'epoch': 1.72}\n",
      "{'loss': 1.0707, 'grad_norm': 23.760482788085938, 'learning_rate': 1.1664453409839773e-05, 'epoch': 1.76}\n",
      "{'loss': 1.0658, 'grad_norm': 11.171010971069336, 'learning_rate': 1.1935883888962956e-05, 'epoch': 1.8}\n",
      "{'loss': 1.0854, 'grad_norm': 12.513233184814453, 'learning_rate': 1.2207314368086142e-05, 'epoch': 1.84}\n",
      "{'loss': 1.0491, 'grad_norm': 4.132542610168457, 'learning_rate': 1.2478744847209328e-05, 'epoch': 1.88}\n",
      "{'loss': 1.0701, 'grad_norm': 11.339996337890625, 'learning_rate': 1.2750175326332513e-05, 'epoch': 1.92}\n",
      "{'loss': 1.0564, 'grad_norm': 66.87300872802734, 'learning_rate': 1.30216058054557e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12843f695ca84e73866533b8d520ff03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.5421080984018675, 'eval_f1': 0.5427438398167861, 'eval_loss': 0.9257205128669739, 'eval_runtime': 129.1864, 'eval_samples_per_second': 43.108, 'eval_steps_per_second': 14.375, 'epoch': 2.0}\n",
      "{'loss': 1.03, 'grad_norm': 6.124541759490967, 'learning_rate': 1.3293036284578884e-05, 'epoch': 2.01}\n",
      "{'loss': 0.9802, 'grad_norm': 60.2114372253418, 'learning_rate': 1.3563923902743822e-05, 'epoch': 2.05}\n",
      "{'loss': 0.9531, 'grad_norm': 64.00323486328125, 'learning_rate': 1.3835354381867009e-05, 'epoch': 2.09}\n",
      "{'loss': 0.9351, 'grad_norm': 37.82403564453125, 'learning_rate': 1.4106242000031947e-05, 'epoch': 2.13}\n",
      "{'loss': 0.9746, 'grad_norm': 16.772634506225586, 'learning_rate': 1.4377672479155133e-05, 'epoch': 2.17}\n",
      "{'loss': 0.9465, 'grad_norm': 24.6521053314209, 'learning_rate': 1.4649102958278318e-05, 'epoch': 2.21}\n",
      "{'loss': 0.9418, 'grad_norm': 69.53672790527344, 'learning_rate': 1.4920533437401504e-05, 'epoch': 2.25}\n",
      "{'loss': 0.9725, 'grad_norm': 16.6578426361084, 'learning_rate': 1.519196391652469e-05, 'epoch': 2.29}\n",
      "{'loss': 0.9621, 'grad_norm': 29.099245071411133, 'learning_rate': 1.5463394395647875e-05, 'epoch': 2.33}\n",
      "{'loss': 0.9347, 'grad_norm': 7.633969306945801, 'learning_rate': 1.5734282013812814e-05, 'epoch': 2.38}\n",
      "{'loss': 0.9557, 'grad_norm': 40.904579162597656, 'learning_rate': 1.6005712492936e-05, 'epoch': 2.42}\n",
      "{'loss': 0.9614, 'grad_norm': 21.16887664794922, 'learning_rate': 1.6277142972059183e-05, 'epoch': 2.46}\n",
      "{'loss': 0.9548, 'grad_norm': 52.81398391723633, 'learning_rate': 1.654857345118237e-05, 'epoch': 2.5}\n",
      "{'loss': 0.965, 'grad_norm': 26.441783905029297, 'learning_rate': 1.6820003930305556e-05, 'epoch': 2.54}\n",
      "{'loss': 0.9643, 'grad_norm': 9.248602867126465, 'learning_rate': 1.7091434409428744e-05, 'epoch': 2.58}\n",
      "{'loss': 0.9454, 'grad_norm': 64.44178771972656, 'learning_rate': 1.7362864888551925e-05, 'epoch': 2.62}\n",
      "{'loss': 0.9728, 'grad_norm': 4.7457499504089355, 'learning_rate': 1.7634295367675113e-05, 'epoch': 2.66}\n",
      "{'loss': 0.9763, 'grad_norm': 13.496034622192383, 'learning_rate': 1.7905725846798298e-05, 'epoch': 2.7}\n",
      "{'loss': 0.973, 'grad_norm': 21.526384353637695, 'learning_rate': 1.8176613464963236e-05, 'epoch': 2.74}\n",
      "{'loss': 0.9523, 'grad_norm': 20.811094284057617, 'learning_rate': 1.8448043944086424e-05, 'epoch': 2.78}\n",
      "{'loss': 0.917, 'grad_norm': 15.826096534729004, 'learning_rate': 1.8719474423209605e-05, 'epoch': 2.83}\n",
      "{'loss': 0.9315, 'grad_norm': 83.42687225341797, 'learning_rate': 1.8990904902332793e-05, 'epoch': 2.87}\n",
      "{'loss': 0.9657, 'grad_norm': 20.86747169494629, 'learning_rate': 1.9262335381455978e-05, 'epoch': 2.91}\n",
      "{'loss': 0.9136, 'grad_norm': 52.625831604003906, 'learning_rate': 1.953322299962092e-05, 'epoch': 2.95}\n",
      "{'loss': 0.9279, 'grad_norm': 36.625892639160156, 'learning_rate': 1.9804653478744104e-05, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4601e3caa0b04dae8e7775270bd0eb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6186029807864967, 'eval_f1': 0.6182180568613557, 'eval_loss': 0.9976822137832642, 'eval_runtime': 129.1709, 'eval_samples_per_second': 43.113, 'eval_steps_per_second': 14.376, 'epoch': 3.0}\n",
      "{'loss': 0.8005, 'grad_norm': 18.805315017700195, 'learning_rate': 2.007554109690904e-05, 'epoch': 3.03}\n",
      "{'loss': 0.7618, 'grad_norm': 1.7467933893203735, 'learning_rate': 2.0346971576032227e-05, 'epoch': 3.07}\n",
      "{'loss': 0.8173, 'grad_norm': 10.063373565673828, 'learning_rate': 2.0618402055155412e-05, 'epoch': 3.11}\n",
      "{'loss': 0.8088, 'grad_norm': 4.948492050170898, 'learning_rate': 2.08898325342786e-05, 'epoch': 3.15}\n",
      "{'loss': 0.7888, 'grad_norm': 45.62972640991211, 'learning_rate': 2.1161263013401785e-05, 'epoch': 3.19}\n",
      "{'loss': 0.8674, 'grad_norm': 31.724933624267578, 'learning_rate': 2.143269349252497e-05, 'epoch': 3.24}\n",
      "{'loss': 0.7657, 'grad_norm': 33.06605911254883, 'learning_rate': 2.1704123971648154e-05, 'epoch': 3.28}\n",
      "{'loss': 0.8493, 'grad_norm': 16.989675521850586, 'learning_rate': 2.197555445077134e-05, 'epoch': 3.32}\n",
      "{'loss': 0.8402, 'grad_norm': 35.69833755493164, 'learning_rate': 2.2246984929894527e-05, 'epoch': 3.36}\n",
      "{'loss': 0.8749, 'grad_norm': 28.90913963317871, 'learning_rate': 2.251841540901771e-05, 'epoch': 3.4}\n",
      "{'loss': 0.8406, 'grad_norm': 15.977958679199219, 'learning_rate': 2.2789845888140896e-05, 'epoch': 3.44}\n",
      "{'loss': 0.8446, 'grad_norm': 1.3704063892364502, 'learning_rate': 2.3060733506305834e-05, 'epoch': 3.48}\n",
      "{'loss': 0.894, 'grad_norm': 41.822669982910156, 'learning_rate': 2.3332163985429022e-05, 'epoch': 3.52}\n",
      "{'loss': 0.8895, 'grad_norm': 11.897102355957031, 'learning_rate': 2.3603594464552207e-05, 'epoch': 3.56}\n",
      "{'loss': 0.8345, 'grad_norm': 2.144068479537964, 'learning_rate': 2.387502494367539e-05, 'epoch': 3.6}\n",
      "{'loss': 0.843, 'grad_norm': 13.907278060913086, 'learning_rate': 2.414645542279858e-05, 'epoch': 3.64}\n",
      "{'loss': 0.9042, 'grad_norm': 20.226835250854492, 'learning_rate': 2.441788590192176e-05, 'epoch': 3.69}\n",
      "{'loss': 0.8458, 'grad_norm': 11.757200241088867, 'learning_rate': 2.4688773520086703e-05, 'epoch': 3.73}\n",
      "{'loss': 0.8463, 'grad_norm': 45.452789306640625, 'learning_rate': 2.4960203999209887e-05, 'epoch': 3.77}\n",
      "{'loss': 0.8126, 'grad_norm': 14.714232444763184, 'learning_rate': 2.5231634478333075e-05, 'epoch': 3.81}\n",
      "{'loss': 0.8503, 'grad_norm': 22.2526798248291, 'learning_rate': 2.550306495745626e-05, 'epoch': 3.85}\n",
      "{'loss': 0.8785, 'grad_norm': 14.840360641479492, 'learning_rate': 2.5773952575621195e-05, 'epoch': 3.89}\n",
      "{'loss': 0.8253, 'grad_norm': 46.12831497192383, 'learning_rate': 2.6045383054744383e-05, 'epoch': 3.93}\n",
      "{'loss': 0.8493, 'grad_norm': 12.899779319763184, 'learning_rate': 2.6316813533867568e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6497e21cbe9d41a89e7fda2b40d68110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6415873585922068, 'eval_f1': 0.6415504437113901, 'eval_loss': 1.0187714099884033, 'eval_runtime': 130.4745, 'eval_samples_per_second': 42.683, 'eval_steps_per_second': 14.233, 'epoch': 4.0}\n",
      "{'loss': 0.7835, 'grad_norm': 21.102596282958984, 'learning_rate': 2.6588244012990756e-05, 'epoch': 4.01}\n",
      "{'loss': 0.6664, 'grad_norm': 5.044328212738037, 'learning_rate': 2.685967449211394e-05, 'epoch': 4.05}\n",
      "{'loss': 0.6521, 'grad_norm': 16.94122314453125, 'learning_rate': 2.7131104971237125e-05, 'epoch': 4.1}\n",
      "{'loss': 0.7038, 'grad_norm': 37.15906524658203, 'learning_rate': 2.740253545036031e-05, 'epoch': 4.14}\n",
      "{'loss': 0.75, 'grad_norm': 18.979867935180664, 'learning_rate': 2.7673423068525248e-05, 'epoch': 4.18}\n",
      "{'loss': 0.7511, 'grad_norm': 28.13322639465332, 'learning_rate': 2.794431068669019e-05, 'epoch': 4.22}\n",
      "{'loss': 0.7576, 'grad_norm': 11.955854415893555, 'learning_rate': 2.8215741165813374e-05, 'epoch': 4.26}\n",
      "{'loss': 0.7405, 'grad_norm': 9.219260215759277, 'learning_rate': 2.848717164493656e-05, 'epoch': 4.3}\n",
      "{'loss': 0.763, 'grad_norm': 27.077842712402344, 'learning_rate': 2.8758602124059744e-05, 'epoch': 4.34}\n",
      "{'loss': 0.803, 'grad_norm': 30.15535545349121, 'learning_rate': 2.903003260318293e-05, 'epoch': 4.38}\n",
      "{'loss': 0.7422, 'grad_norm': 67.7715835571289, 'learning_rate': 2.9301463082306116e-05, 'epoch': 4.42}\n",
      "{'loss': 0.7878, 'grad_norm': 12.841975212097168, 'learning_rate': 2.95728935614293e-05, 'epoch': 4.46}\n",
      "{'loss': 0.7561, 'grad_norm': 55.045257568359375, 'learning_rate': 2.9844324040552486e-05, 'epoch': 4.5}\n",
      "{'loss': 0.7522, 'grad_norm': 19.242769241333008, 'learning_rate': 3.011575451967567e-05, 'epoch': 4.55}\n",
      "{'loss': 0.7845, 'grad_norm': 18.728805541992188, 'learning_rate': 3.0387184998798858e-05, 'epoch': 4.59}\n",
      "{'loss': 0.8138, 'grad_norm': 23.57891273498535, 'learning_rate': 3.065861547792204e-05, 'epoch': 4.63}\n",
      "{'loss': 0.791, 'grad_norm': 37.53937911987305, 'learning_rate': 3.093004595704523e-05, 'epoch': 4.67}\n",
      "{'loss': 0.8087, 'grad_norm': 10.262370109558105, 'learning_rate': 3.120147643616841e-05, 'epoch': 4.71}\n",
      "{'loss': 0.8247, 'grad_norm': 20.24808120727539, 'learning_rate': 3.14729069152916e-05, 'epoch': 4.75}\n",
      "{'loss': 0.7917, 'grad_norm': 7.71514892578125, 'learning_rate': 3.174433739441478e-05, 'epoch': 4.79}\n",
      "{'loss': 0.8018, 'grad_norm': 9.705709457397461, 'learning_rate': 3.2015767873537966e-05, 'epoch': 4.83}\n",
      "{'loss': 0.7974, 'grad_norm': 3.0646936893463135, 'learning_rate': 3.228665549170291e-05, 'epoch': 4.87}\n",
      "{'loss': 0.8562, 'grad_norm': 1.8996127843856812, 'learning_rate': 3.2558085970826096e-05, 'epoch': 4.91}\n",
      "{'loss': 0.8045, 'grad_norm': 29.36471939086914, 'learning_rate': 3.282951644994928e-05, 'epoch': 4.95}\n",
      "{'loss': 0.8255, 'grad_norm': 37.36577606201172, 'learning_rate': 3.3100946929072465e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aae44deb45840f98d693ae70bb3c3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6437421440114922, 'eval_f1': 0.643986597412205, 'eval_loss': 1.344173789024353, 'eval_runtime': 130.3053, 'eval_samples_per_second': 42.738, 'eval_steps_per_second': 14.251, 'epoch': 5.0}\n",
      "{'loss': 0.6788, 'grad_norm': 26.029659271240234, 'learning_rate': 3.337237740819565e-05, 'epoch': 5.04}\n",
      "{'loss': 0.6854, 'grad_norm': 0.8334842324256897, 'learning_rate': 3.364326502636059e-05, 'epoch': 5.08}\n",
      "{'loss': 0.6623, 'grad_norm': 29.628421783447266, 'learning_rate': 3.391469550548377e-05, 'epoch': 5.12}\n",
      "{'loss': 0.6937, 'grad_norm': 42.893489837646484, 'learning_rate': 3.4186125984606964e-05, 'epoch': 5.16}\n",
      "{'loss': 0.6594, 'grad_norm': 14.304396629333496, 'learning_rate': 3.445755646373015e-05, 'epoch': 5.2}\n",
      "{'loss': 0.6919, 'grad_norm': 1.6768128871917725, 'learning_rate': 3.472898694285333e-05, 'epoch': 5.24}\n",
      "{'loss': 0.759, 'grad_norm': 55.0181884765625, 'learning_rate': 3.499987456101827e-05, 'epoch': 5.28}\n",
      "{'loss': 0.6734, 'grad_norm': 0.09906376153230667, 'learning_rate': 3.5271305040141456e-05, 'epoch': 5.32}\n",
      "{'loss': 0.7168, 'grad_norm': 3.3665852546691895, 'learning_rate': 3.554273551926464e-05, 'epoch': 5.36}\n",
      "{'loss': 0.7606, 'grad_norm': 17.88541030883789, 'learning_rate': 3.5814165998387826e-05, 'epoch': 5.41}\n",
      "{'loss': 0.7222, 'grad_norm': 34.41759490966797, 'learning_rate': 3.608559647751102e-05, 'epoch': 5.45}\n",
      "{'loss': 0.7257, 'grad_norm': 0.46199601888656616, 'learning_rate': 3.63570269566342e-05, 'epoch': 5.49}\n",
      "{'loss': 0.7517, 'grad_norm': 60.869110107421875, 'learning_rate': 3.662845743575738e-05, 'epoch': 5.53}\n",
      "{'loss': 0.6933, 'grad_norm': 26.180139541625977, 'learning_rate': 3.689988791488057e-05, 'epoch': 5.57}\n",
      "{'loss': 0.7941, 'grad_norm': 124.86273956298828, 'learning_rate': 3.717023267208726e-05, 'epoch': 5.61}\n",
      "{'loss': 0.7337, 'grad_norm': 20.95762825012207, 'learning_rate': 3.7441120290252195e-05, 'epoch': 5.65}\n",
      "{'loss': 0.7363, 'grad_norm': 5.351559638977051, 'learning_rate': 3.7712550769375386e-05, 'epoch': 5.69}\n",
      "{'loss': 0.7832, 'grad_norm': 12.044846534729004, 'learning_rate': 3.798398124849857e-05, 'epoch': 5.73}\n",
      "{'loss': 0.7552, 'grad_norm': 9.128318786621094, 'learning_rate': 3.825541172762176e-05, 'epoch': 5.77}\n",
      "{'loss': 0.7966, 'grad_norm': 12.191381454467773, 'learning_rate': 3.852684220674494e-05, 'epoch': 5.81}\n",
      "{'loss': 0.7651, 'grad_norm': 14.79427433013916, 'learning_rate': 3.879772982490988e-05, 'epoch': 5.86}\n",
      "{'loss': 0.8195, 'grad_norm': 25.712448120117188, 'learning_rate': 3.906916030403307e-05, 'epoch': 5.9}\n",
      "{'loss': 0.8213, 'grad_norm': 5.661149024963379, 'learning_rate': 3.934059078315625e-05, 'epoch': 5.94}\n",
      "{'loss': 0.7788, 'grad_norm': 14.934768676757812, 'learning_rate': 3.961202126227944e-05, 'epoch': 5.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7359ffa9522f4104ac55fd97fba01d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6502065002693482, 'eval_f1': 0.6480668146829563, 'eval_loss': 1.1717666387557983, 'eval_runtime': 130.254, 'eval_samples_per_second': 42.755, 'eval_steps_per_second': 14.257, 'epoch': 6.0}\n",
      "{'loss': 0.7332, 'grad_norm': 38.68946838378906, 'learning_rate': 3.9883451741402624e-05, 'epoch': 6.02}\n",
      "{'loss': 0.6181, 'grad_norm': 1.2194770574569702, 'learning_rate': 4.0154882220525815e-05, 'epoch': 6.06}\n",
      "{'loss': 0.6082, 'grad_norm': 34.59031295776367, 'learning_rate': 4.042631269964899e-05, 'epoch': 6.1}\n",
      "{'loss': 0.7004, 'grad_norm': 6.616634845733643, 'learning_rate': 4.069720031781393e-05, 'epoch': 6.14}\n",
      "{'loss': 0.6777, 'grad_norm': 0.04622616246342659, 'learning_rate': 4.096863079693712e-05, 'epoch': 6.18}\n",
      "{'loss': 0.6969, 'grad_norm': 103.87966918945312, 'learning_rate': 4.12400612760603e-05, 'epoch': 6.22}\n",
      "{'loss': 0.7291, 'grad_norm': 16.712339401245117, 'learning_rate': 4.151149175518349e-05, 'epoch': 6.27}\n",
      "{'loss': 0.7262, 'grad_norm': 35.68686294555664, 'learning_rate': 4.178292223430668e-05, 'epoch': 6.31}\n",
      "{'loss': 0.7642, 'grad_norm': 24.409412384033203, 'learning_rate': 4.205380985247161e-05, 'epoch': 6.35}\n",
      "{'loss': 0.7333, 'grad_norm': 7.341552257537842, 'learning_rate': 4.23252403315948e-05, 'epoch': 6.39}\n",
      "{'loss': 0.8026, 'grad_norm': 13.403573036193848, 'learning_rate': 4.259612794975974e-05, 'epoch': 6.43}\n",
      "{'loss': 0.697, 'grad_norm': 6.32455587387085, 'learning_rate': 4.286755842888293e-05, 'epoch': 6.47}\n",
      "{'loss': 0.7866, 'grad_norm': 0.24279490113258362, 'learning_rate': 4.313898890800611e-05, 'epoch': 6.51}\n",
      "{'loss': 0.8201, 'grad_norm': 29.289897918701172, 'learning_rate': 4.341041938712929e-05, 'epoch': 6.55}\n",
      "{'loss': 0.7664, 'grad_norm': 7.577607154846191, 'learning_rate': 4.3681849866252484e-05, 'epoch': 6.59}\n",
      "{'loss': 0.7804, 'grad_norm': 62.514766693115234, 'learning_rate': 4.3952737484417415e-05, 'epoch': 6.63}\n",
      "{'loss': 0.8265, 'grad_norm': 16.73362922668457, 'learning_rate': 4.422416796354061e-05, 'epoch': 6.67}\n",
      "{'loss': 0.7176, 'grad_norm': 0.13164299726486206, 'learning_rate': 4.449559844266379e-05, 'epoch': 6.72}\n",
      "{'loss': 0.8936, 'grad_norm': 0.9042079448699951, 'learning_rate': 4.4767028921786976e-05, 'epoch': 6.76}\n",
      "{'loss': 0.9879, 'grad_norm': 0.44001927971839905, 'learning_rate': 4.503845940091016e-05, 'epoch': 6.8}\n",
      "{'loss': 0.8422, 'grad_norm': 10.32852554321289, 'learning_rate': 4.5309889880033345e-05, 'epoch': 6.84}\n",
      "{'loss': 0.8249, 'grad_norm': 0.5242711305618286, 'learning_rate': 4.558132035915654e-05, 'epoch': 6.88}\n",
      "{'loss': 0.8155, 'grad_norm': 161.19961547851562, 'learning_rate': 4.5852750838279715e-05, 'epoch': 6.92}\n",
      "{'loss': 0.8116, 'grad_norm': 467.6871032714844, 'learning_rate': 4.612363845644465e-05, 'epoch': 6.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4035f78cf3814a0594484d1e3ee2a385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6414077931405997, 'eval_f1': 0.64276980382379, 'eval_loss': 1.5463054180145264, 'eval_runtime': 128.7807, 'eval_samples_per_second': 43.244, 'eval_steps_per_second': 14.42, 'epoch': 7.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 20758.428, 'train_samples_per_second': 52.938, 'train_steps_per_second': 17.646, 'train_loss': 0.9223125685833206, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9058ecfaf36346c380bb54f17a0a8cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 11:10:20,203] Trial 4 finished with value: 0.6502065002693482 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.2500228054734358, 'learning_rate': 6.331007353356651e-05, 'batch_size': 3, 'warmup_ratio': 0.31837920111611373, 'weight_decay': 0.09971031186766507, 'adam_beta1': 0.8682929292594384, 'adam_beta2': 0.9909672212836867, 'adam_epsilon': 5.424397228138408e-07, 'lr_scheduler_type': 'linear'}. Best is trial 4 with value: 0.6502065002693482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Current Trial 5 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.43619344239874647, 'learning_rate': 4.744804985319591e-07, 'batch_size': 3, 'warmup_ratio': 0.6610094216704753, 'weight_decay': 0.214015165889555, 'adam_beta1': 0.8312852854211105, 'adam_beta2': 0.996036397101193, 'adam_epsilon': 5.420035889344036e-08, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18c453a59304d2c89b56e406e2eb894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/366300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6085, 'grad_norm': 1.0757840871810913, 'learning_rate': 9.798133601482668e-10, 'epoch': 0.04}\n",
      "{'loss': 1.6087, 'grad_norm': 1.0348371267318726, 'learning_rate': 1.957667093576237e-09, 'epoch': 0.08}\n",
      "{'loss': 1.609, 'grad_norm': 0.9857816696166992, 'learning_rate': 2.9355208270042074e-09, 'epoch': 0.12}\n",
      "{'loss': 1.609, 'grad_norm': 0.9480171799659729, 'learning_rate': 3.915334187152474e-09, 'epoch': 0.16}\n",
      "{'loss': 1.6091, 'grad_norm': 0.9768244624137878, 'learning_rate': 4.895147547300741e-09, 'epoch': 0.2}\n",
      "{'loss': 1.6093, 'grad_norm': 0.982633650302887, 'learning_rate': 5.8749609074490075e-09, 'epoch': 0.25}\n",
      "{'loss': 1.6088, 'grad_norm': 1.8424819707870483, 'learning_rate': 6.8528146408769786e-09, 'epoch': 0.29}\n",
      "{'loss': 1.609, 'grad_norm': 0.988273024559021, 'learning_rate': 7.832628001025244e-09, 'epoch': 0.33}\n",
      "{'loss': 1.6085, 'grad_norm': 1.1059460639953613, 'learning_rate': 8.81244136117351e-09, 'epoch': 0.37}\n",
      "{'loss': 1.6093, 'grad_norm': 1.0883785486221313, 'learning_rate': 9.792254721321779e-09, 'epoch': 0.41}\n",
      "{'loss': 1.6091, 'grad_norm': 1.1354783773422241, 'learning_rate': 1.077010845474975e-08, 'epoch': 0.45}\n",
      "{'loss': 1.6095, 'grad_norm': 1.0810142755508423, 'learning_rate': 1.1749921814898015e-08, 'epoch': 0.49}\n",
      "{'loss': 1.6086, 'grad_norm': 1.059558629989624, 'learning_rate': 1.2729735175046282e-08, 'epoch': 0.53}\n",
      "{'loss': 1.6095, 'grad_norm': 1.0277293920516968, 'learning_rate': 1.370954853519455e-08, 'epoch': 0.57}\n",
      "{'loss': 1.6082, 'grad_norm': 1.0557674169540405, 'learning_rate': 1.4689361895342816e-08, 'epoch': 0.61}\n",
      "{'loss': 1.6086, 'grad_norm': 1.0446022748947144, 'learning_rate': 1.5667215628770787e-08, 'epoch': 0.66}\n",
      "{'loss': 1.6082, 'grad_norm': 1.0586448907852173, 'learning_rate': 1.664702898891905e-08, 'epoch': 0.7}\n",
      "{'loss': 1.6079, 'grad_norm': 1.148160457611084, 'learning_rate': 1.762684234906732e-08, 'epoch': 0.74}\n",
      "{'loss': 1.608, 'grad_norm': 1.1914886236190796, 'learning_rate': 1.8606655709215586e-08, 'epoch': 0.78}\n",
      "{'loss': 1.6092, 'grad_norm': 1.0065940618515015, 'learning_rate': 1.9584509442643558e-08, 'epoch': 0.82}\n",
      "{'loss': 1.6086, 'grad_norm': 1.2320585250854492, 'learning_rate': 2.0564322802791822e-08, 'epoch': 0.86}\n",
      "{'loss': 1.6081, 'grad_norm': 1.0341814756393433, 'learning_rate': 2.154413616294009e-08, 'epoch': 0.9}\n",
      "{'loss': 1.6085, 'grad_norm': 1.0837699174880981, 'learning_rate': 2.252198989636806e-08, 'epoch': 0.94}\n",
      "{'loss': 1.6084, 'grad_norm': 1.0983372926712036, 'learning_rate': 2.350180325651633e-08, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64d938ac753494fb2b66562a94a4673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.31836954569940745, 'eval_f1': 0.32442948394051324, 'eval_loss': 1.607865333557129, 'eval_runtime': 130.1834, 'eval_samples_per_second': 42.778, 'eval_steps_per_second': 14.264, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n",
      "{'train_runtime': 2976.1961, 'train_samples_per_second': 369.23, 'train_steps_per_second': 123.077, 'train_loss': 1.6087190195344492, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b080a2d16a4c40b8de1e86c19799ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1857 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 12:02:06,610] Trial 5 finished with value: 0.31836954569940745 and parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.43619344239874647, 'learning_rate': 4.744804985319591e-07, 'batch_size': 3, 'warmup_ratio': 0.6610094216704753, 'weight_decay': 0.214015165889555, 'adam_beta1': 0.8312852854211105, 'adam_beta2': 0.996036397101193, 'adam_epsilon': 5.420035889344036e-08, 'lr_scheduler_type': 'linear'}. Best is trial 4 with value: 0.6502065002693482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 6 parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.23732993865505386, 'learning_rate': 1.1685926112932391e-07, 'batch_size': 4, 'warmup_ratio': 0.8400211050841219, 'weight_decay': 0.05868908832126296, 'adam_beta1': 0.8519566435750197, 'adam_beta2': 0.9913554074386653, 'adam_epsilon': 7.201851832756769e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForMultipleChoice were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937d8e6ef254426490c63fc5260ce914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6096, 'grad_norm': 0.892684817314148, 'learning_rate': 2.5165542741075785e-10, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-22 12:04:46,078] Trial 6 failed with parameters: {'model_name': 'sentence-transformers/all-mpnet-base-v2', 'dropout_rate': 0.23732993865505386, 'learning_rate': 1.1685926112932391e-07, 'batch_size': 4, 'warmup_ratio': 0.8400211050841219, 'weight_decay': 0.05868908832126296, 'adam_beta1': 0.8519566435750197, 'adam_beta2': 0.9913554074386653, 'adam_epsilon': 7.201851832756769e-07, 'lr_scheduler_type': 'cosine_with_restarts'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\OEM\\AppData\\Local\\Temp\\ipykernel_29776\\2979044354.py\", line 60, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 2284, in _inner_training_loop\n",
      "    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-22 12:04:46,080] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m study\u001b[38;5;241m.\u001b[39menqueue_trial({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: pretrained_model_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.5807103066634623e-05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5994150649377659\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.12506835879573128\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8136227307274486\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9924116710027883\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.9858068243318367e-07\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_scheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_with_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 5\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 60\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     49\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     50\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init(model_name, dropout_rate),\n\u001b[0;32m     51\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[AdvancedEarlyStoppingCallback(metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     63\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective for pretrained_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "global_run_name=\"Optuna-2\"\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.enqueue_trial({'model_name': pretrained_model_name, 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.optimize(objective, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0629740",
   "metadata": {},
   "source": [
    "## 4.4 Optuna Hyperparameters Tuning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b567c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98fb9f12-d022-4590-af99-611fc317aeaf",
   "metadata": {},
   "source": [
    "# End of NoteBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a6a10-fee5-4be2-b061-29591435f60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
