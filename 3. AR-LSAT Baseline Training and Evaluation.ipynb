{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71614c8c-099c-4c44-91f3-5cd0ea933243",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebea513d-21f8-4f2a-b64e-e52e6acbed93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "from IPython.display import HTML, display\n",
    "import math\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "# Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "from datasets import ClassLabel, Sequence, Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets, load_from_disk\n",
    "\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scikitplot as skplt  # Uncomment if scikit-plot is installed and needed\n",
    "\n",
    "# Machine Learning: Model Preparation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, f1_score\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine Learning: Models and Frameworks\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import xgboost\n",
    "import wandb\n",
    "from xgboost import plot_importance  # Uncomment if xgboost importance plot is required\n",
    "\n",
    "\n",
    "# NLP and Transformers\n",
    "import spacy\n",
    "import transformers\n",
    "from transformers import (AdamW, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoModelForMultipleChoice,\n",
    "                          AutoTokenizer, CamembertForSequenceClassification, DistilBertConfig,\n",
    "                          DistilBertForSequenceClassification, DistilBertModel, EarlyStoppingCallback,\n",
    "                          get_linear_schedule_with_warmup, RobertaForSequenceClassification, EvalPrediction,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, XLMRobertaForSequenceClassification,\n",
    "                         DefaultDataCollator, BertForQuestionAnswering, DataCollatorWithPadding, PreTrainedTokenizerFast,\n",
    "                         default_data_collator, is_torch_xla_available, pipeline)\n",
    "from transformers.trainer_utils import PredictionOutput, speed_metrics\n",
    "\n",
    "# Experiment Tracking and Optimization Utilities\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import wandb  # Uncomment if using Weights & Biases for experiment tracking\n",
    "\n",
    "# Progress Bar Utilities\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f79ebc-867c-48d7-9f62-abaa0ba19f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95e38-f23f-4972-a8cb-b4c1849cb726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb17c692-f274-4f42-8985-1b42f4f1cfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Arguments and global vriables\n",
    "dataset_name=\"AR-LSAT\"\n",
    "pretrained_model_name = \"microsoft/deberta-v3-base\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "data_collator = DefaultDataCollator()\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = right_padding = tokenizer.padding_side == 'right'\n",
    "global_counter = 0\n",
    "traing_answer_mismatches = []\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bdab5d03-6a98-42af-8f2b-4f85639452dc",
   "metadata": {},
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{dataset_name}/{normalized_model_name}-best_model\",\n",
    "    overwrite_output_dir = True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "80455fb1-dbc9-412f-afae-25880509eaed",
   "metadata": {
    "tags": []
   },
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{dataset_name}/{normalized_model_name}-best_model\",\n",
    "    overwrite_output_dir = True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",    \n",
    "    num_train_epochs=3,\n",
    "    learning_rate=4.91978224351371e-05,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=398,\n",
    "    weight_decay=0.19404494917360213,\n",
    "    adam_beta1=0.8368183686077211,\n",
    "    adam_beta2=0.9969107098230887,\n",
    "    adam_epsilon=5.874878468800929e-07,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5d0e7-f055-4058-986f-f82edc6fce8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Prepare the AR-LSAT Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38eef2e-4a62-434a-a3df-31a558fc1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1072514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 118521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 200566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('cleaned_dataset')\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49b14e4b-8c9a-4cf4-817e-e8b7099bf1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the dataset to only include AR-LSAT data\n",
    "ar_lsat_train = combined_dataset['train'].filter(lambda x: x['Source Dataset'] == 'AR-LSAT')\n",
    "ar_lsat_val = combined_dataset['validation'].filter(lambda x: x['Source Dataset'] == 'AR-LSAT')\n",
    "ar_lsat_test = combined_dataset['test'].filter(lambda x: x['Source Dataset'] == 'AR-LSAT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944988bb-97a9-4daf-80a6-743daea9b62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing function for multiple-choice tasks\n",
    "def mcqa_preprocess_function(examples):\n",
    "    num_choices = num_choices = len(examples['Options'][0])    \n",
    "    first_sentences = [[context] * num_choices for context in examples['Context']]  # Repeat context for each option\n",
    "    question_headers = examples['Question']\n",
    "    options_list = examples['Options']\n",
    "    \n",
    "    second_sentences = []\n",
    "    for question, options in zip(question_headers, options_list):\n",
    "        # Combine question with each option\n",
    "        second_sentences.append([f\"{question} {option}\" for option in options])\n",
    "    \n",
    "    # Flatten the lists\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    \n",
    "    # Tokenize the inputs\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    # Un-flatten the tokenized inputs to have shape (num_examples, num_choices, seq_length)\n",
    "    tokenized_inputs = {k: [v[i:i + num_choices] for i in range(0, len(v), num_choices)] for k, v in tokenized_examples.items()}\n",
    "    \n",
    "    # Labels\n",
    "    tokenized_inputs[\"labels\"] = examples[\"Label\"]\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the preprocessing function to the datasets\n",
    "encoded_ar_lsat_train = ar_lsat_train.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_ar_lsat_val = ar_lsat_val.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_ar_lsat_test = ar_lsat_test.map(mcqa_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27035bf4-f8bd-4e10-8b8e-80e6c33cb9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the format of the datasets to PyTorch tensors\n",
    "encoded_ar_lsat_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_ar_lsat_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_ar_lsat_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "def get_train_encoded():\n",
    "    return encoded_ar_lsat_train\n",
    "\n",
    "def get_val_encoded():\n",
    "    return encoded_ar_lsat_val\n",
    "\n",
    "def get_test_encoded():\n",
    "    return encoded_ar_lsat_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1af4-7b57-4c74-bcd9-48edefba6747",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb8384ce-d55e-4aa1-911b-7d8ac3ab07df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'eval_accuracy': acc, 'eval_f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8368650d-b524-46e2-bb7b-92a241957506",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_training_args(run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=4):\n",
    "    \"\"\"\n",
    "    Generates training arguments for training a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): The name of the dataset.\n",
    "    - run_name (str): The name of the run, useful for logging and saving models.\n",
    "    - model_name (str): The name of the model, typically including its configuration.\n",
    "    - num_train_epochs (int): The number of epochs to train for.\n",
    "    - learning_rate (float): The learning rate for training.\n",
    "    - batch_size (int): The batch size used for training.\n",
    "\n",
    "    Returns:\n",
    "    - TrainingArguments: A configured TrainingArguments instance.\n",
    "    \"\"\"    \n",
    "    output_dir = f\"./{dataset_name}/{run_name}/{normalized_model_name}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=1,\n",
    "        warmup_steps=398,\n",
    "        weight_decay=0.194,\n",
    "        adam_beta1=0.837,\n",
    "        adam_beta2=0.997,\n",
    "        adam_epsilon=5.87e-07,\n",
    "        lr_scheduler_type='cosine',\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    )\n",
    "    \n",
    "    return training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81470afe-4e03-414c-9af0-8937c7643097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=4):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=create_training_args(run_name=run_name, num_train_epochs=num_train_epochs, learning_rate=learning_rate, batch_size=batch_size),\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9b86e-63c3-4ee8-bb91-bbb43e5a6481",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Fine-tuning DeBERTa on MCQA task (AR-LSAT Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5960c1-4b7d-4871-9029-bb724849439c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.1 Evaluate Vanilla DeBERTa (Acc = 15.22%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920052b1-e4d5-45c3-bb80-a471f35d5c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241014_225823-fap2g3f0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/fap2g3f0' target=\"_blank\">./AR-LSAT/microsoft-deberta-v3-base-best_model</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/fap2g3f0' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/fap2g3f0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.15217391304347827, 'eval_f1': 0.1483351657844587, 'eval_loss': 1.609476923942566, 'eval_model_preparation_time': 0.001, 'eval_runtime': 8.7947, 'eval_samples_per_second': 26.152, 'eval_steps_per_second': 26.152}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96f4d5-67f3-4d84-b110-42b283a66ce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2 Fine-Tune and Evaluate Vanilla DeBERTa (Acc=22.61%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcbb144a-c79b-4747-99c9-efd8f6131fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2382' max='2382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2382/2382 39:53, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.217696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.610500</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0.162603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.610900</td>\n",
       "      <td>1.609383</td>\n",
       "      <td>0.177489</td>\n",
       "      <td>0.126622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.614800</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>0.188633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.614800</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.145762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.610600</td>\n",
       "      <td>1.609375</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.142153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.22608695652173913, 'eval_f1': 0.20898684836140372, 'eval_loss': 1.609375, 'eval_runtime': 9.3401, 'eval_samples_per_second': 24.625, 'eval_steps_per_second': 24.625, 'epoch': 6.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForMultipleChoice.from_pretrained(pretrained_model_name)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02783c0c-6c08-42fc-a741-705ff1bac74b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.3 Evaluate SQUAD DeBERTa (Acc=22.61%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90877cd6-3ca5-4cc4-8a77-a755a0d6896a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.22608695652173913, 'eval_f1': 0.22492208750128037, 'eval_loss': 1.6092263460159302, 'eval_model_preparation_time': 0.002, 'eval_runtime': 9.2167, 'eval_samples_per_second': 24.955, 'eval_steps_per_second': 24.955}\n"
     ]
    }
   ],
   "source": [
    "path = \"./squad-trained-model\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"Squad-Run\")\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33aceb-a791-4e83-a2f4-cec16d086dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.4 Fine-Tune and Evaluate SQUAD DeBERTa (Acc=23.91%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e63fad85-dd7b-4143-aefe-ad0995cca802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241014_234556-fjkgb8ou</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/fjkgb8ou' target=\"_blank\">./AR-LSAT/microsoft-deberta-v3-base-best_model</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/fjkgb8ou' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/fjkgb8ou</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1191' max='1191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1191/1191 21:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.609358</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.245915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.615800</td>\n",
       "      <td>1.609392</td>\n",
       "      <td>0.220779</td>\n",
       "      <td>0.222109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.618300</td>\n",
       "      <td>1.609400</td>\n",
       "      <td>0.199134</td>\n",
       "      <td>0.199943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.2391304347826087, 'eval_f1': 0.23980029660214316, 'eval_loss': 1.6094005107879639, 'eval_runtime': 9.0823, 'eval_samples_per_second': 25.324, 'eval_steps_per_second': 25.324, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "path = \"./squad-trained-model\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"Squad-Run\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb07981-bc4f-4ce7-90c6-aa652077607b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4.5 Evaluate Trained DeBERTa on Other Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf70cf36-dc7d-467e-9f02-62f668606a07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.1956521739130435, 'eval_f1': 0.19823219158947752, 'eval_loss': 1.6125339269638062, 'eval_model_preparation_time': 0.0, 'eval_runtime': 10.2579, 'eval_samples_per_second': 22.422, 'eval_steps_per_second': 22.422}\n"
     ]
    }
   ],
   "source": [
    "path = \"./LogiQA/Squad-Run/microsoft-deberta-v3-base/checkpoint-12567\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"LogiQA-Run\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "598f316d-dd84-47a0-a572-93f68544df3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='461' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 1:43:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.20869565217391303, 'eval_f1': 0.2074393665896999, 'eval_loss': 2.0346715450286865, 'eval_model_preparation_time': 0.0, 'eval_runtime': 9.2238, 'eval_samples_per_second': 24.935, 'eval_steps_per_second': 24.935}\n"
     ]
    }
   ],
   "source": [
    "path = \"./ReClor/Squad-Run/microsoft-deberta-v3-base-best_model/checkpoint-3105\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"ReColr-Run\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48d2a89a-a458-4548-92f1-55c3f1618ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1191' max='1191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1191/1191 15:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.609472</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>0.228899</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.666500</td>\n",
       "      <td>1.607451</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.189353</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.662142</td>\n",
       "      <td>0.173160</td>\n",
       "      <td>0.172048</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_accuracy': 0.20434782608695654, 'eval_f1': 0.20337999588676806, 'eval_loss': 1.609222173690796, 'eval_model_preparation_time': 0.0, 'eval_runtime': 9.2973, 'eval_samples_per_second': 24.738, 'eval_steps_per_second': 24.738, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(f\"Test Results: {test_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb9f12-d022-4590-af99-611fc317aeaf",
   "metadata": {},
   "source": [
    "# End of NoteBook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714wingpu",
   "language": "python",
   "name": "compsci714wingpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
