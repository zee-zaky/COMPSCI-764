{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71614c8c-099c-4c44-91f3-5cd0ea933243",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebea513d-21f8-4f2a-b64e-e52e6acbed93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import ast\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "import random\n",
    "import collections\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "from IPython.display import HTML, display\n",
    "import math\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "# Data Handling Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from torch.utils.data import random_split\n",
    "import datasets\n",
    "from datasets import ClassLabel, Sequence, Dataset, DatasetDict, load_dataset, load_metric, concatenate_datasets, load_from_disk\n",
    "\n",
    "\n",
    "# Data Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import scikitplot as skplt  # Uncomment if scikit-plot is installed and needed\n",
    "\n",
    "# Machine Learning: Model Preparation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine Learning: Models and Frameworks\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import xgboost\n",
    "import wandb\n",
    "from xgboost import plot_importance  # Uncomment if xgboost importance plot is required\n",
    "\n",
    "\n",
    "# NLP and Transformers\n",
    "import spacy\n",
    "import transformers\n",
    "from transformers import (AdamW, AutoModelForSequenceClassification, AutoModelForQuestionAnswering, AutoModelForMultipleChoice,\n",
    "                          AutoTokenizer, CamembertForSequenceClassification, DistilBertConfig,\n",
    "                          DistilBertForSequenceClassification, DistilBertModel, EarlyStoppingCallback,\n",
    "                          get_linear_schedule_with_warmup, RobertaForSequenceClassification, EvalPrediction,\n",
    "                          Trainer, TrainerCallback, TrainingArguments, XLMRobertaForSequenceClassification,\n",
    "                         DefaultDataCollator, BertForQuestionAnswering, DataCollatorWithPadding, PreTrainedTokenizerFast,\n",
    "                         default_data_collator, is_torch_xla_available, pipeline)\n",
    "from transformers.trainer_utils import PredictionOutput, speed_metrics\n",
    "\n",
    "# Experiment Tracking and Optimization Utilities\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "# import wandb  # Uncomment if using Weights & Biases for experiment tracking\n",
    "\n",
    "# Progress Bar Utilities\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7f79ebc-867c-48d7-9f62-abaa0ba19f72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
    "else:\n",
    "    print(\"No GPU available. Training will run on CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95e38-f23f-4972-a8cb-b4c1849cb726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb17c692-f274-4f42-8985-1b42f4f1cfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Arguments and global vriables\n",
    "pretrained_model_name = \"microsoft/deberta-v3-base\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "data_collator = DefaultDataCollator()\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = right_padding = tokenizer.padding_side == 'right'\n",
    "global_counter = 0\n",
    "traing_answer_mismatches = []\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9fed9a5-6fe6-4b85-9292-2b40f65f8dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{normalized_model_name}-best_model\",\n",
    "    overwrite_output_dir = True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=4, \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",  # Enable logging to Weights & Biases\n",
    "    run_name=f\"{normalized_model_name}-best_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    lr_scheduler_type = 'linear',\n",
    "    fp16=True,  # Enable mixed-precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e85478-38c0-44c2-88c8-4c9cca4f5d29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.Datasets Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e4ce80-14a6-48d0-ac41-127223724290",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.1 Explore Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd0d88a-bb3f-4cee-941b-0b42a0bdde0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762d0c9ab9ab420cbc69be05b7f3a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/407 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b369f68075a54f7199edec75d981d007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ffbcb63fc34a96be568bdd9491ea0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/546k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6544aae853a1412f882771c62fd36b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/4638 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5154eb28b7484102abaa3ac520df5695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "--    Test Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 4638\n",
      "Size of validation set: 500\n",
      "Example 1: {'answers': ['Unlike aspirin and other medications that reduce pain and swelling and that are currently available, the new medication would repair existing cell damage that had been caused by rheumatoid arthritis.', 'A patient treated with the new medication for rheumatoid arthritis could sustain a joint injury without becoming aware of it.', 'Joint diseases other than rheumatoid arthritis would not be affected by the new medication.', \"The benefits to rheumatoid arthritis sufferers of the new medication would outweigh the medication's possible harmful side effects.\"], 'context': \"In rheumatoid arthritis, the body' s immune system misfunctions by attacking healthy cells in the joints causing the release of a hormone that in turn causes pain and swelling. This hormone is normally activated only in reaction to injury or infection. A new arthritis medication will contain a protein that inhibits the functioning of the hormone that causes pain and swelling in the joints.\", 'label': 1, 'id_string': 'train_0', 'question': 'The statements above, if true, most strongly support which one of the following conclusions?'}\n",
      "Example 2: {'answers': ['attempting to discredit a position by questioning the motives of the proponents of that position', 'rejecting a questionable position on the grounds that the general public does not support that position', \"pointing out an unstated assumption on which the pharmacists' argument relies and then refuting it\", 'asserting that pharmacists lack the appropriate knowledge to have informed opinions on the subject under discussion'], 'context': 'Patient: Pharmacists maintain that doctors should not be permitted to sell the medicine that they prescribe because doctors would then be tempted to prescribe unnecessary medicines in order to earn extra income. But pharmacists have a financial interest in having a monopoly on the sale of prescription medicines, so their objection to the sale of medicines by doctors cannot be taken seriously.', 'label': 0, 'id_string': 'train_1', 'question': \"The patient's argument proceeds by\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Names of the datasets as they might appear in Hugging Face's datasets library\n",
    "dataset_names = {        \n",
    "    \"Test\" : \"metaeval/reclor\",\n",
    "}\n",
    "\n",
    "# Attempt to load each dataset and print a few examples\n",
    "for name, dataset_info in dataset_names.items():\n",
    "    try:\n",
    "        if isinstance(dataset_info, tuple):\n",
    "            dataset = load_dataset(*dataset_info, trust_remote_code=True)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_info, trust_remote_code=True)\n",
    "        \n",
    "        print(\"--------------------------------------------\")\n",
    "        print(f\"--    {name} Dataset Examples:\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        for split in dataset.keys():\n",
    "            print(f\"Size of {split} set: {len(dataset[split])}\")\n",
    "        for i, example in enumerate(dataset['train'].take(2)):\n",
    "            print(f\"Example {i + 1}: {example}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name}: {str(e)}\")\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "452edd2c-10c4-4e25-a9be-aba40a0b681d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "--    AR-LSAT (Zhong et al., 2021) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 1585\n",
      "Size of validation set: 231\n",
      "Size of test set: 230\n",
      "Example 1: {'context': 'Exactly six trade representatives negotiate a treaty: Klosnik, Londi, Manley, Neri, Osata, Poirier. There are exactly six chairs evenly spaced around a circular table. The chairs are numbered 1 through 6, with successively numbered chairs next to each other and chair number 1 next to chair number 6. Each chair is occupied by exactly one of the representatives. The following conditions apply: Poirier sits immediately next to Neri. Londi sits immediately next to Manley, Neri, or both. Klosnik does not sit immediately next to Manley. If Osata sits immediately next to Poirier, Osata does not sit immediately next to Manley.', 'question': 'Which one of the following seating arrangements of the six representatives in chairs 1 through 6 would NOT violate the stated conditions?', 'answers': ['Klosnik, Poirier, Neri, Manley, Osata, Londi', 'Klosnik, Londi, Manley, Poirier, Neri, Osata', 'Klosnik, Londi, Manley, Osata, Poirier, Neri', 'Klosnik, Osata, Poirier, Neri, Londi, Manley', 'Klosnik, Neri, Londi, Osata, Manley, Poirier'], 'label': 1}\n",
      "Example 2: {'context': 'Exactly six trade representatives negotiate a treaty: Klosnik, Londi, Manley, Neri, Osata, Poirier. There are exactly six chairs evenly spaced around a circular table. The chairs are numbered 1 through 6, with successively numbered chairs next to each other and chair number 1 next to chair number 6. Each chair is occupied by exactly one of the representatives. The following conditions apply: Poirier sits immediately next to Neri. Londi sits immediately next to Manley, Neri, or both. Klosnik does not sit immediately next to Manley. If Osata sits immediately next to Poirier, Osata does not sit immediately next to Manley.', 'question': 'If Londi sits immediately next to Poirier, which one of the following is a pair of representatives who must sit immediately next to each other?', 'answers': ['Klosnik and Osata', 'Londi and Neri', 'Londi and Osata', 'Manley and Neri', 'Manley and Poirier'], 'label': 0}\n",
      "\n",
      "--------------------------------------------\n",
      "--    ReClor (Yu et al., 2020) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 4638\n",
      "Size of validation set: 500\n",
      "Example 1: {'answers': ['Unlike aspirin and other medications that reduce pain and swelling and that are currently available, the new medication would repair existing cell damage that had been caused by rheumatoid arthritis.', 'A patient treated with the new medication for rheumatoid arthritis could sustain a joint injury without becoming aware of it.', 'Joint diseases other than rheumatoid arthritis would not be affected by the new medication.', \"The benefits to rheumatoid arthritis sufferers of the new medication would outweigh the medication's possible harmful side effects.\"], 'context': \"In rheumatoid arthritis, the body' s immune system misfunctions by attacking healthy cells in the joints causing the release of a hormone that in turn causes pain and swelling. This hormone is normally activated only in reaction to injury or infection. A new arthritis medication will contain a protein that inhibits the functioning of the hormone that causes pain and swelling in the joints.\", 'label': 1, 'id_string': 'train_0', 'question': 'The statements above, if true, most strongly support which one of the following conclusions?'}\n",
      "Example 2: {'answers': ['attempting to discredit a position by questioning the motives of the proponents of that position', 'rejecting a questionable position on the grounds that the general public does not support that position', \"pointing out an unstated assumption on which the pharmacists' argument relies and then refuting it\", 'asserting that pharmacists lack the appropriate knowledge to have informed opinions on the subject under discussion'], 'context': 'Patient: Pharmacists maintain that doctors should not be permitted to sell the medicine that they prescribe because doctors would then be tempted to prescribe unnecessary medicines in order to earn extra income. But pharmacists have a financial interest in having a monopoly on the sale of prescription medicines, so their objection to the sale of medicines by doctors cannot be taken seriously.', 'label': 0, 'id_string': 'train_1', 'question': \"The patient's argument proceeds by\"}\n",
      "\n",
      "--------------------------------------------\n",
      "--    LogiQA 2.0 (Liu et al., 2023) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 12567\n",
      "Size of test set: 1572\n",
      "Size of validation set: 1569\n",
      "Example 1: {'id': 4554, 'answer': 0, 'text': \"A college will continue to implement the overseas funding plan this year. It plans to select several of the six teachers from Mr. Liu, Mr. Zhang, Mr. Wang, Mr. Ma, Mr. Niu and Mr. Zhou to visit abroad. Due to the limitations of funding, the needs of discipline development, curriculum arrangement, place and time of each student's visit, the selection shall meet the following conditions: (1) Mr. Liu is the reserve discipline leader of the college, This time we have to send out. (2) if we choose Mr. Liu, we should also choose Mr. Zhou, but we can't choose Mr. Zhang. (3) only if Mr. Niu can't choose, at least one of Mr. Wang and Mr. Ma can choose. (4) if we don't choose Mr. Wang, we don't choose Mr. Zhou either.\", 'type': \"{'Categorical Reasoning': True, 'Sufficient Conditional Reasoning': True, 'Necessry Condtional Reasoning': True, 'Conjunctive Reasoning': True}\", 'question': 'If the above statement is true, which of the followings must be true?', 'options': [\"Mr. Niu didn't choose, but Mr. Zhou did\", \"Mr. Liu was chose, but Mr. Ma didn't\", 'Mr. Wang and Mr. Ma were chosen', 'Neither Mr. Wang nor Mr. Niu was elected']}\n",
      "Example 2: {'id': 7446, 'answer': 0, 'text': 'The straw effect means that the juice around the tube can be inhaled by inhaling the fruit juice in the cup with a straw. Implanting some kind of straw in a certain area can gather resources and cause obvious regional effect.', 'type': \"{'Categorical Reasoning': True, 'Sufficient Conditional Reasoning': True, 'Conjunctive Reasoning': True}\", 'question': 'Which of the following options does not belong to the straw effect?', 'options': ['The common sense of petroleum prospecting is that the standard distance between the two mining areas is 100 meters. At present, China has explored eight oil fields in the East China Sea, including the new Bajiaoting oil and gas field', 'The function of the city has shifted to the south, and property prices in Zhongcun, Nanhai Pingzhou, Sanshan and Shunde near the new railway station have all been pushed up', 'Xijiao Town integrates the resources of the industrial park in the town, and with the strong foundation of its textile industry, it has established the textile characteristics as the main direction of attack, and guided the textile enterprises to transform from extensive to centralized and carry out cluster development', 'This year, let the prosperity of the modern service industry be matched with convenient transportation, and the combination of the two will become a prosperous area where capital flows yearn for one after another']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "--    RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007, 2008; Bentivogli et al., 2009) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 2490\n",
      "Size of validation set: 277\n",
      "Size of test set: 3000\n",
      "Example 1: {'text1': 'No Weapons of Mass Destruction Found in Iraq Yet.', 'text2': 'Weapons of Mass Destruction Found in Iraq.', 'label': 1, 'idx': 0, 'label_text': 'not entailment'}\n",
      "Example 2: {'text1': 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.', 'text2': 'Pope Benedict XVI is the new leader of the Roman Catholic Church.', 'label': 0, 'idx': 1, 'label_text': 'entailment'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    FOLIO (Han et al., 2022) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 1001\n",
      "Size of validation set: 203\n",
      "Example 1: {'story_id': 406, 'premises': \"All people who regularly drink coffee are dependent on caffeine.\\nPeople regularly drink coffee, or they don't want to be addicted to caffeine, or both.\\nNo one who doesn't want to be addicted to caffeine is unaware that caffeine is a drug.\\nRina is either a student who is unaware that caffeine is a drug, or she is not a student and is she aware that caffeine is a drug.\\nRina  is either a student who is dependent on caffeine, or she is not a student and not dependent on caffeine.\", 'premises-FOL': '∀x (DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine))\\n∀x (DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine)))\\n∀x (¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine))\\n¬(Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine))\\n¬(IsDependentOn(rina, caffeine) ⊕ Student(rina))', 'conclusion': \"Rina doesn't want to be addicted to caffeine or is unaware that caffeine is a drug.\", 'conclusion-FOL': '¬WantToBeAddictedTo(rina, caffeine) ∨ (¬AwareThatDrug(rina, caffeine))', 'label': 'True', 'example_id': 1126}\n",
      "Example 2: {'story_id': 406, 'premises': \"All people who regularly drink coffee are dependent on caffeine.\\nPeople regularly drink coffee, or they don't want to be addicted to caffeine, or both.\\nNo one who doesn't want to be addicted to caffeine is unaware that caffeine is a drug.\\nRina is either a student who is unaware that caffeine is a drug, or she is not a student and is she aware that caffeine is a drug.\\nRina  is either a student who is dependent on caffeine, or she is not a student and not dependent on caffeine.\", 'premises-FOL': '∀x (DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine))\\n∀x (DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine)))\\n∀x (¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine))\\n¬(Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine))\\n¬(IsDependentOn(rina, caffeine) ⊕ Student(rina))', 'conclusion': \"Rina eith doesn't want to be addicted to caffeine or is unaware that caffeine is a drug.\", 'conclusion-FOL': '¬WantToBeAddictedTo(rina, caffeine) ⊕ ¬AwareThatDrug(rina, caffeine)', 'label': 'True', 'example_id': 1127}\n",
      "\n",
      "--------------------------------------------\n",
      "--    PrOntoQA (Saparov and He, 2023) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 2880\n",
      "Example 1: {'prompt': \"###Context: Every yumpus is small. Every tumpus is hot. Each tumpus is a zumpus. Zumpuses are not small. Each zumpus is a rompus. Rompuses are brown. Each rompus is a wumpus. Fae is a zumpus. Is the following statement true or false? Fae is not small.\\n    ###Response: Let's think step by step. Fae is a zumpus. Zumpuses are not small. Fae is not small. Hence, the statement Fae is not small is true\\n    ###The answer is: True\"}\n",
      "Example 2: {'prompt': \"###Context: Every impus is not mean. Zumpuses are not transparent. Zumpuses are numpuses. Numpuses are not spicy. Each numpus is a wumpus. Every wumpus is not bright. Wumpuses are dumpuses. Every dumpus is floral. Each dumpus is a tumpus. Each tumpus is temperate. Tumpuses are vumpuses. Each vumpus is mean. Every vumpus is a rompus. Rompuses are feisty. Rompuses are jompuses. Each jompus is red. Each jompus is a yumpus. Wren is a vumpus. Is the following statement true or false? Wren is mean.\\n    ###Response: Let's think step by step. Wren is a vumpus. Each vumpus is mean. Wren is mean. Hence, the statement Wren is mean is true\\n    ###The answer is: True\"}\n",
      "\n",
      "--------------------------------------------\n",
      "--    TellMeWhy (Lal et al., 2021) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 71892\n",
      "Size of validation set: 8976\n",
      "Size of test set: 10689\n",
      "Example 1: {'narrative': 'Cam ordered a pizza and took it home. He opened the box to take out a slice. Cam discovered that the store did not cut the pizza for him. He looked for his pizza cutter but did not find it. He had to use his chef knife to cut a slice.', 'question': 'Why did Cam order a pizza?', 'original_sentence_for_question': 'Cam ordered a pizza and took it home.', 'narrative_lexical_overlap': 0.33333333330000003, 'is_ques_answerable': 'Not Answerable', 'answer': 'Cam was hungry.', 'is_ques_answerable_annotator': 'Not Answerable', 'original_narrative_form': ['Cam ordered a pizza and took it home.', 'He opened the box to take out a slice.', 'Cam discovered that the store did not cut the pizza for him.', 'He looked for his pizza cutter but did not find it.', 'He had to use his chef knife to cut a slice.'], 'question_meta': 'rocstories_narrative_41270_sentence_0_question_0', 'helpful_sentences': [], 'human_eval': False, 'val_ann': [], 'gram_ann': []}\n",
      "Example 2: {'narrative': 'Cam ordered a pizza and took it home. He opened the box to take out a slice. Cam discovered that the store did not cut the pizza for him. He looked for his pizza cutter but did not find it. He had to use his chef knife to cut a slice.', 'question': 'Why did He open the box?', 'original_sentence_for_question': 'He opened the box to take out a slice.', 'narrative_lexical_overlap': 0.33333333330000003, 'is_ques_answerable': 'Answerable', 'answer': 'The pizza was in the box.', 'is_ques_answerable_annotator': 'Not Answerable', 'original_narrative_form': ['Cam ordered a pizza and took it home.', 'He opened the box to take out a slice.', 'Cam discovered that the store did not cut the pizza for him.', 'He looked for his pizza cutter but did not find it.', 'He had to use his chef knife to cut a slice.'], 'question_meta': 'rocstories_narrative_41270_sentence_1_question_1', 'helpful_sentences': [], 'human_eval': False, 'val_ann': [], 'gram_ann': []}\n",
      "\n",
      "--------------------------------------------\n",
      "--    HotpotQA (Yang et al., 2018) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 90447\n",
      "Size of validation set: 7405\n",
      "Size of test set: 7405\n",
      "Example 1: {'id': '5a7a06935542990198eaf050', 'question': \"Which magazine was started first Arthur's Magazine or First for Women?\", 'answer': \"Arthur's Magazine\", 'type': 'comparison', 'level': 'medium', 'supporting_facts': {'title': [\"Arthur's Magazine\", 'First for Women'], 'sent_id': [0, 0]}, 'context': {'title': ['Radio City (Indian radio station)', 'History of Albanian football', 'Echosmith', \"Women's colleges in the Southern United States\", 'First Arthur County Courthouse and Jail', \"Arthur's Magazine\", '2014–15 Ukrainian Hockey Championship', 'First for Women', 'Freeway Complex Fire', 'William Rast'], 'sentences': [[\"Radio City is India's first private FM radio station and was started on 3 July 2001.\", ' It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).', ' It plays Hindi, English and regional songs.', ' It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.', ' Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.', ' The Radio station currently plays a mix of Hindi and Regional music.', ' Abraham Thomas is the CEO of the company.'], ['Football in Albania existed before the Albanian Football Federation (FSHF) was created.', \" This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .\", ' Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.', ' In 1932, Albania joined FIFA (during the 12–16 June convention ) And in 1954 she was one of the founding members of UEFA.'], ['Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.', ' Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.', ' Echosmith started first as \"Ready Set Go!\"', ' until they signed to Warner Bros.', ' Records in May 2012.', ' They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.', ' The song was Warner Bros.', \" Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.\", ' The band\\'s debut album, \"Talking Dreams\", was released on October 8, 2013.'], [\"Women's colleges in the Southern United States refers to undergraduate, bachelor's degree–granting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.\", \" Many started first as girls' seminaries or academies.\", ' Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.', ' Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.'], ['The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.'], [\"Arthur's Magazine (1844–1846) was an American literary periodical published in Philadelphia in the 19th century.\", ' Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.', ' In May 1846 it was merged into \"Godey\\'s Lady\\'s Book\".'], ['The 2014–15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.', ' Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.', ' Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.', ' The regular season included just 12 rounds, where all the teams went to the semifinals.', ' In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.'], [\"First for Women is a woman's magazine published by Bauer Media Group in the USA.\", ' The magazine was started in 1989.', ' It is based in Englewood Cliffs, New Jersey.', ' In 2011 the circulation of the magazine was 1,310,696 copies.'], ['The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.', ' The fire started as two separate fires on November 15, 2008.', ' The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.', ' These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.'], ['William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.', ' It is most known for their premium jeans.', ' On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.', ' The label also produces other clothing items such as jackets and tops.', ' The company started first as a denim line, later evolving into a men’s and women’s clothing line.']]}}\n",
      "Example 2: {'id': '5a879ab05542996e4f30887e', 'question': 'The Oberoi family is part of a hotel company that has a head office in what city?', 'answer': 'Delhi', 'type': 'bridge', 'level': 'medium', 'supporting_facts': {'title': ['Oberoi family', 'The Oberoi Group'], 'sent_id': [0, 0]}, 'context': {'title': ['Ritz-Carlton Jakarta', 'Oberoi family', 'Ishqbaaaz', 'Hotel Tallcorn', 'Mohan Singh Oberoi', 'Hotel Bond', 'The Oberoi Group', 'Future Fibre Technologies', '289th Military Police Company', 'Glennwanis Hotel'], 'sentences': [['The Ritz-Carlton Jakarta is a hotel and skyscraper in Jakarta, Indonesia and 14th Tallest building in Jakarta.', ' It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel.', ' It is operated by The Ritz-Carlton Hotel Company.', ' The complex has two towers that comprises a hotel and the Airlangga Apartment respectively.', ' The hotel was opened in 2005.'], ['The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group.'], ['Ishqbaaaz (English: \"Lovers\") is an Indian drama television series which is broadcast on Star Plus.', ' It premiered on 27 June 2016 and airs Mon-Fri 10-11pm IST.', 'Nakuul Mehta, Kunal Jaisingh and Leenesh Mattoo respectively portray Shivaay, Omkara and Rudra, the three heirs of the Oberoi family.', ' The show initially focused on the tale of three brothers, later become centered on the love story of Shivaay and Annika (Surbhi Chandna); with the story of Omkara and Rudra being shifted to the spinoff series \"Dil Boley Oberoi\".', ' In July 2017 \"Dil Boley Oberoi\" ended and the storylines were merged back into \"Ishqbaaaz\" which doubled its runtime.'], ['The Hotel Tallcorn is located in Marshalltown, Iowa.', ' Today it is called the Tallcorn Towers Apartments.', ' Built in 1928 by the Eppley Hotel Company, local citizens contributed $120,000 to ensure the successful completion of this seven-story hotel.', ' It was completed in connection to the seventy-fifth anniversary of Marshalltown.', \" The hotel's sale in 1956 from the Eppley chain to the Sheraton Corporation was part of the second largest hotel sale in United States history.\", ' The Tallcorn was listed as a contributing property in the Marshalltown Downtown Historic District on the National Register of Historic Places in 2002.'], [\"Rai Bahadur Mohan Singh Oberoi (15 August 1898\\xa0– 3 May 2002) was an Indian hotelier, the founder and chairman of Oberoi Hotels & Resorts, India's second-largest hotel company, with 35 hotels in India, Sri Lanka, Nepal, Egypt, Australia and Hungary.\"], ['Hotel Bond is a historic hotel, built in two stages in 1913 and 1921, in downtown Hartford, Connecticut by hotelier Harry S. Bond.', ' It is located near Bushnell Park, and was considered the grandest hotel in Hartford during its heyday.', ' The second section is a 12 story building attached to the 6 story first section.', ' A Statler Hotel opened in the area in 1954, creating competition, and the Bond Hotel company declared bankruptcy shortly after that.', ' It was bought by the California-based Masaglia Hotel chain, which began an incremental renovation program.', ' In 1964 it was sold to a Cincinnati, Ohio investment group which announced extensive renovation plans.', ' However, the financing plans fell through and the hotel was again in bankruptcy.', ' The building was sold at auction to the Roman Catholic Archdiocese of Hartford in 1965, and it became the home of the Saint Francis Hospital School of Nursing.', ' The Bond Ballroom reopened in 2001, with the rest of the building becoming a Homewood Suites by Hilton in 2006.'], ['The Oberoi Group is a hotel company with its head office in Delhi.', ' Founded in 1934, the company owns and/or operates 30+ luxury hotels and two river cruise ships in six countries, primarily under its Oberoi Hotels & Resorts and Trident Hotels brands.'], ['Future Fibre Technologies (FFT) is a fiber optic sensing technologies company based in Melbourne, Australia, with its US head office in Mountain View, California, Middle East head office in Dubai, Indian head office in New Delhi and European head office in London.', ' Founded in 1994, Future Fibre Technologies product line provides optical fiber intrusion detection systems for perimeters, buried oil and gas pipelines and data communication networks.'], ['The 289th Military Police Company was activated on 1 November 1994 and attached to Hotel Company, 3rd Infantry (The Old Guard), Fort Myer, Virginia.', \" Hotel Company is the regiment's specialty company.\"], ['The Glennwanis Hotel is a historic hotel in Glennville, Georgia, Tattnall County, Georgia, built on the site of the Hughes Hotel.', ' The hotel is located at 209-215 East Barnard Street.', ' The old Hughes Hotel was built out of Georgia pine circa 1905 and burned in 1920.', ' The Glennwanis was built in brick in 1926.', ' The local Kiwanis club led the effort to get the replacement hotel built, and organized a Glennville Hotel Company with directors being local business leaders.', ' The wife of a local doctor won a naming contest with the name \"Glennwanis Hotel\", a suggestion combining \"Glennville\" and \"Kiwanis\".']]}}\n",
      "\n",
      "--------------------------------------------\n",
      "--    GSM8K (Cobbe et al., 2021) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 7473\n",
      "Size of test set: 1319\n",
      "Example 1: {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n",
      "Example 2: {'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?', 'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    MRPC (Dolan & Brockett, 2015) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 3668\n",
      "Size of validation set: 408\n",
      "Size of test set: 1725\n",
      "Example 1: {'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .', 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .', 'label': 1, 'idx': 0}\n",
      "Example 2: {'sentence1': \"Yucaipa owned Dominick 's before selling the chain to Safeway in 1998 for $ 2.5 billion .\", 'sentence2': \"Yucaipa bought Dominick 's in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .\", 'label': 0, 'idx': 1}\n",
      "\n",
      "--------------------------------------------\n",
      "--    AbductionRules (Young et al., 2022) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 40012\n",
      "Size of test set: 11464\n",
      "Size of dev set: 5716\n",
      "Example 1: {'context': 'Harry is big. Anne is poor. Erin is little. If a person is clever, is rough, and is little, that person is also huge. Harry is high. If something is little, is clever, and is tiny, then it is thin. People that are clever, are wealthy, and are kind, are quiet. Harry is strong. All people that are strong, are short, and are kind, are also sad. Anne is dull. Anne is bad. Erin is small. If something is wealthy, is bad, and is heavy, it is thin. All people that are rough, are poor, and are high, are also sad. If a person is bad, is wealthy, and is kind, that person is also quiet. All things that are short, are strong, and are heavy, are also huge. Charlie is wealthy. Erin is short. Charlie is clever. If a person is rough, is bad, and is poor, that person is sad. If something is poor, is tiny, and is high, it is also quiet. People that are strong, are high, and are heavy, are huge. If something is tiny, is little, and is short, it is thin.', 'text': 'Harry is huge.', 'label': 'Harry is heavy.', 'QCat': '0'}\n",
      "Example 2: {'context': 'Harry is big. Anne is poor. Erin is little. If a person is clever, is rough, and is little, that person is also huge. Harry is high. If something is little, is clever, and is tiny, then it is thin. People that are clever, are wealthy, and are kind, are quiet. Harry is strong. All people that are strong, are short, and are kind, are also sad. Anne is dull. Anne is bad. Erin is small. If something is wealthy, is bad, and is heavy, it is thin. All people that are rough, are poor, and are high, are also sad. If a person is bad, is wealthy, and is kind, that person is also quiet. All things that are short, are strong, and are heavy, are also huge. Charlie is wealthy. Erin is short. Charlie is clever. If a person is rough, is bad, and is poor, that person is sad. If something is poor, is tiny, and is high, it is also quiet. People that are strong, are high, and are heavy, are huge. If something is tiny, is little, and is short, it is thin.', 'text': 'Harry is not huge.', 'label': 'Harry is heavy.', 'QCat': '0_0'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    ProofWriter (Tafjord et al., 2021) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 585552\n",
      "Size of test set: 174476\n",
      "Size of validation set: 85468\n",
      "Example 1: {'id': 'AttNeg-OWA-D0-4611', 'maxD': 0, 'NFact': 7, 'NRule': 8, 'theory': 'Gary is furry. Gary is nice. Gary is red. Gary is rough. Gary is not smart. Gary is white. Gary is young. If Gary is nice and Gary is not white then Gary is red. If someone is white then they are red. All young people are furry. If someone is white and not red then they are furry. Smart, red people are rough. If Gary is not red and Gary is not furry then Gary is not smart. If Gary is white then Gary is not smart. If someone is rough and not white then they are not smart.', 'question': 'Gary is white.', 'answer': 'True', 'QDep': 0, 'QLen': 1.0, 'allProofs': '@0: Gary is furry.[(triple1 OR ((triple7) -> rule3))] Gary is nice.[(triple2)] Gary is not smart.[(triple5 OR ((triple6) -> rule7))] Gary is red.[(triple3 OR ((triple6) -> rule2))] Gary is rough.[(triple4)] Gary is white.[(triple6)] Gary is young.[(triple7)]', 'config': 'depth-0'}\n",
      "Example 2: {'id': 'AttNeg-OWA-D0-4611', 'maxD': 0, 'NFact': 7, 'NRule': 8, 'theory': 'Gary is furry. Gary is nice. Gary is red. Gary is rough. Gary is not smart. Gary is white. Gary is young. If Gary is nice and Gary is not white then Gary is red. If someone is white then they are red. All young people are furry. If someone is white and not red then they are furry. Smart, red people are rough. If Gary is not red and Gary is not furry then Gary is not smart. If Gary is white then Gary is not smart. If someone is rough and not white then they are not smart.', 'question': 'Gary is not nice.', 'answer': 'False', 'QDep': 0, 'QLen': 1.0, 'allProofs': '@0: Gary is furry.[(triple1 OR ((triple7) -> rule3))] Gary is nice.[(triple2)] Gary is not smart.[(triple5 OR ((triple6) -> rule7))] Gary is red.[(triple3 OR ((triple6) -> rule2))] Gary is rough.[(triple4)] Gary is white.[(triple6)] Gary is young.[(triple7)]', 'config': 'depth-0'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    SQuAD 2.0 (Rajpurkar et al., 2018) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 130319\n",
      "Size of validation set: 11873\n",
      "Example 1: {'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n",
      "Example 2: {'id': '56be85543aeaaa14008c9065', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'What areas did Beyonce compete in when she was growing up?', 'answers': {'text': ['singing and dancing'], 'answer_start': [207]}}\n",
      "\n",
      "--------------------------------------------\n",
      "--    MultiNLI (Williams, Nangia, and Bowman 2018) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 392702\n",
      "Size of validation_matched set: 9815\n",
      "Size of validation_mismatched set: 9832\n",
      "Example 1: {'promptID': 31193, 'pairID': '31193n', 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.', 'premise_binary_parse': '( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )', 'premise_parse': '(ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))', 'hypothesis': 'Product and geography are what make cream skimming work. ', 'hypothesis_binary_parse': '( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))', 'genre': 'government', 'label': 1}\n",
      "Example 2: {'promptID': 101457, 'pairID': '101457e', 'premise': 'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him', 'premise_binary_parse': '( you ( ( know ( during ( ( ( the season ) and ) ( i guess ) ) ) ) ( at ( at ( ( your level ) ( uh ( you ( ( ( lose them ) ( to ( the ( next level ) ) ) ) ( if ( ( if ( they ( decide ( to ( recall ( the ( the ( parent team ) ) ) ) ) ) ) ) ( ( the Braves ) ( decide ( to ( call ( to ( ( recall ( a guy ) ) ( from ( ( triple A ) ( ( ( then ( ( a ( double ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) and ) ( ( a ( single ( A guy ) ) ) ( ( goes up ) ( to ( replace him ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) )', 'premise_parse': '(ROOT (S (NP (PRP you)) (VP (VBP know) (PP (IN during) (NP (NP (DT the) (NN season)) (CC and) (NP (FW i) (FW guess)))) (PP (IN at) (IN at) (NP (NP (PRP$ your) (NN level)) (SBAR (S (INTJ (UH uh)) (NP (PRP you)) (VP (VBP lose) (NP (PRP them)) (PP (TO to) (NP (DT the) (JJ next) (NN level))) (SBAR (IN if) (S (SBAR (IN if) (S (NP (PRP they)) (VP (VBP decide) (S (VP (TO to) (VP (VB recall) (NP (DT the) (DT the) (NN parent) (NN team)))))))) (NP (DT the) (NNPS Braves)) (VP (VBP decide) (S (VP (TO to) (VP (VB call) (S (VP (TO to) (VP (VB recall) (NP (DT a) (NN guy)) (PP (IN from) (NP (NP (RB triple) (DT A)) (SBAR (S (S (ADVP (RB then)) (NP (DT a) (JJ double) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))) (CC and) (S (NP (DT a) (JJ single) (NNP A) (NN guy)) (VP (VBZ goes) (PRT (RP up)) (S (VP (TO to) (VP (VB replace) (NP (PRP him))))))))))))))))))))))))))))', 'hypothesis': 'You lose the things to the following level if the people recall.', 'hypothesis_binary_parse': '( You ( ( ( ( lose ( the things ) ) ( to ( the ( following level ) ) ) ) ( if ( ( the people ) recall ) ) ) . ) )', 'hypothesis_parse': '(ROOT (S (NP (PRP You)) (VP (VBP lose) (NP (DT the) (NNS things)) (PP (TO to) (NP (DT the) (JJ following) (NN level))) (SBAR (IN if) (S (NP (DT the) (NNS people)) (VP (VBP recall))))) (. .)))', 'genre': 'telephone', 'label': 0}\n",
      "\n",
      "--------------------------------------------\n",
      "--    Adversarial NLI (Nie et al. 2020) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 3600\n",
      "Example 1: {'premise': \"Good Will Hunting is a 1997 American drama film , directed by Gus Van Sant , and starring Robin Williams , Matt Damon , Ben Affleck , Minnie Driver and Stellan Skarsgård . Amelia Fiona `` Minnie '' Driver ( born 31 January 1970 ) is an English actress and singer-songwriter . Patricia Davies Clarkson ( born December 29 , 1959 ) is an American actress .\", 'hypothesis': 'Clarkson was born exactly 72 years before 29 December 2031 .', 'label': 'ENTAILMENT'}\n",
      "Example 2: {'premise': 'José gallinacean . He was responsible for signing the Cedula of Population in 1783 ( which led to extensive French immigration to Trinidad ) , founded the city of San Fernando in 1784 and surrendered the island of Trinidad to a British fleet under the command of Sir Ralph Abercromby in 1797 .', 'hypothesis': 'José María Chacón founded the most populous city in the world.', 'label': 'CONTRADICTION'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    ConTRoL (Liu et al., 2021a) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of train set: 6719\n",
      "Size of validation set: 799\n",
      "Size of test set: 805\n",
      "Example 1: {'uid': 'id_0', 'premise': '100 Years of the Western Workplace Conditions in the working environment of Western countries changed significantly over the 20th century. Though not without some associated problems, these changes may be viewed generally as positive: child labour all but ceased, wages rose, the number of working hours in a week decreased, pension policies became standard, fringe benefits multiplied and concerns over health and safety issues were enforced. The collection of data relating to work conditions also became a far more exact science. In particular, there were important developments in methodology and data gathering. Additionally, there was a major expansion of the data collection effort more people became involved in learning about the workplace; and, for the first time, results started to be published. This being the case, at the end of the century, not only were most workers better off than their early 20th century predecessors had been, but they were also in a position to understand how and why this was the case. By carefully analyzing the statistical data made available, specific changes in the workplace not least regarding the concept of what work should involve became clearly discernible. The most obvious changes to the workplace involved the size and composition of the countries workforces. Registering only 24 million in 1900 (and including labourers of age ten and up) and 139 million (aged 16 and older), the size of Americas workforce, for instance, increased by almost six-fold in line with its overall population growth. At the same time, the composition of the workforce shifted from industries dominated by primary production occupations, such as farmers and foresters, to those dominated by professional, technical and, in particular, service workers. At the beginning of the 20th century, 38% of all American workers were employed on farms, by the end of the same century, that figure had fallen to less than 3 %. In Europe, much the same process occurred. In the 1930s, in every European country, bar Britain and Belgium, more than 20 per cent of the population worked in agriculture. By the 1980s, however, the farming populations of all developed countries, excluding Eastern Europe, had dropped to ten per cent and often even lower. At the same time, capital intensive farming using highly mechanized techniques dramatically reduced the numbers needed to farm there. And therein lay the problem. While the workplace became a safer and more productive environment, a world away from the harsh working conditions of our forefathers, the switch from an agricultural to a modern working environment also created massive unemployment in many countries. Fundamental to this problem was the widespread move from the countryside to the city. Having lost their livelihoods, the worlds peasant populations amassed in ever larger numbers in already crowded communities, where rates of job growth failed to keep up with internal migration. As a result, thousands were left squatting in shanty towns on the periphery of cities, waiting for jobs that might never arrive. While this was (and is) particularly true of Third World countries, the same phenomenon could also be witnessed in several American, French, English and German cities in the late 20th century. From a different and more positive perspective, in the 20th century, women became visible and active members of all sectors of the Western workplace. In 1900, only 19% of European women of working age participated in the labour force; by 1999, this figure had risen to 60%. In 1900, only 1% of the countrys lawyers and 6% of its physicians were female; by contrast, the figures were 29% and 24% in 1999. A recent survey of French teenagers, both male and female, revealed that over 50% of those polled thought that, in any job (bar those involving military service), women make better employees, as they are less likely to become riled under stress and less overtly competitive than men. The last and perhaps most significant change to the 20th-century workplace involved the introduction of technology. The list of technological improvements in the workplace is endless: communication and measuring devices, computers of all shapes and sizes, x-ray, lasers, neon lights, stainless steel, and so on and on. Such improvements led to a more productive, safer work environment. Moreover, the fact that medicine improved so dramatically led to an increase in the average lifespan among Western populations. In turn, workers of very different ages were able to work shoulder to shoulder, and continue in their jobs far longer. By the end of 20th century, the Western workplace had undergone remarkable changes. In general, both men and women worked fewer hours per day for more years under better conditions. Yet, the power of agriculture had waned as farmers and foresters moved to cities to earn greater salaries as annalists and accountants. For those who could not make this transition, however, life at the dawn of the new century seemed less appealing.', 'hypothesis': 'Improvements in medicine led to workers earning more over a longer period.', 'label': 'neutral'}\n",
      "Example 2: {'uid': 'id_1', 'premise': '100 Years of the Western Workplace Conditions in the working environment of Western countries changed significantly over the 20th century. Though not without some associated problems, these changes may be viewed generally as positive: child labour all but ceased, wages rose, the number of working hours in a week decreased, pension policies became standard, fringe benefits multiplied and concerns over health and safety issues were enforced. The collection of data relating to work conditions also became a far more exact science. In particular, there were important developments in methodology and data gathering. Additionally, there was a major expansion of the data collection effort more people became involved in learning about the workplace; and, for the first time, results started to be published. This being the case, at the end of the century, not only were most workers better off than their early 20th century predecessors had been, but they were also in a position to understand how and why this was the case. By carefully analyzing the statistical data made available, specific changes in the workplace not least regarding the concept of what work should involve became clearly discernible. The most obvious changes to the workplace involved the size and composition of the countries workforces. Registering only 24 million in 1900 (and including labourers of age ten and up) and 139 million (aged 16 and older), the size of Americas workforce, for instance, increased by almost six-fold in line with its overall population growth. At the same time, the composition of the workforce shifted from industries dominated by primary production occupations, such as farmers and foresters, to those dominated by professional, technical and, in particular, service workers. At the beginning of the 20th century, 38% of all American workers were employed on farms, by the end of the same century, that figure had fallen to less than 3 %. In Europe, much the same process occurred. In the 1930s, in every European country, bar Britain and Belgium, more than 20 per cent of the population worked in agriculture. By the 1980s, however, the farming populations of all developed countries, excluding Eastern Europe, had dropped to ten per cent and often even lower. At the same time, capital intensive farming using highly mechanized techniques dramatically reduced the numbers needed to farm there. And therein lay the problem. While the workplace became a safer and more productive environment, a world away from the harsh working conditions of our forefathers, the switch from an agricultural to a modern working environment also created massive unemployment in many countries. Fundamental to this problem was the widespread move from the countryside to the city. Having lost their livelihoods, the worlds peasant populations amassed in ever larger numbers in already crowded communities, where rates of job growth failed to keep up with internal migration. As a result, thousands were left squatting in shanty towns on the periphery of cities, waiting for jobs that might never arrive. While this was (and is) particularly true of Third World countries, the same phenomenon could also be witnessed in several American, French, English and German cities in the late 20th century. From a different and more positive perspective, in the 20th century, women became visible and active members of all sectors of the Western workplace. In 1900, only 19% of European women of working age participated in the labour force; by 1999, this figure had risen to 60%. In 1900, only 1% of the countrys lawyers and 6% of its physicians were female; by contrast, the figures were 29% and 24% in 1999. A recent survey of French teenagers, both male and female, revealed that over 50% of those polled thought that, in any job (bar those involving military service), women make better employees, as they are less likely to become riled under stress and less overtly competitive than men. The last and perhaps most significant change to the 20th-century workplace involved the introduction of technology. The list of technological improvements in the workplace is endless: communication and measuring devices, computers of all shapes and sizes, x-ray, lasers, neon lights, stainless steel, and so on and on. Such improvements led to a more productive, safer work environment. Moreover, the fact that medicine improved so dramatically led to an increase in the average lifespan among Western populations. In turn, workers of very different ages were able to work shoulder to shoulder, and continue in their jobs far longer. By the end of 20th century, the Western workplace had undergone remarkable changes. In general, both men and women worked fewer hours per day for more years under better conditions. Yet, the power of agriculture had waned as farmers and foresters moved to cities to earn greater salaries as annalists and accountants. For those who could not make this transition, however, life at the dawn of the new century seemed less appealing.', 'hypothesis': 'In 1900, 19% of North American women of working age participated in the workforce.', 'label': 'contradiction'}\n",
      "\n",
      "--------------------------------------------\n",
      "--    RACE (lai et al., 2017) Dataset Examples:\n",
      "--------------------------------------------\n",
      "Size of test set: 4934\n",
      "Size of train set: 87866\n",
      "Size of validation set: 4887\n",
      "Example 1: {'example_id': 'high19088.txt', 'article': 'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.', 'answer': 'C', 'question': 'We can know from the passage that the author works as a_.', 'options': ['doctor', 'model', 'teacher', 'reporter']}\n",
      "Example 2: {'example_id': 'high19088.txt', 'article': 'Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\\n\"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\\nOne girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\\nAt that point, I was shocked. I am short, I can\\'t deny that, but I don\\'t think I would put myself through months of agony just to be a few centimetres taller. I don\\'t even bother to wear shoes with thick soles, as I\\'m not trying to hide the fact that I am just not tall!\\nIt seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\\nNo one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.', 'answer': 'C', 'question': 'Many graduates today turn to cosmetic surgery to_.', 'options': ['marry a better man/woman', 'become a model', 'get an advantage over others in job-hunting', 'attract more admirers']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Names of the datasets as they might appear in Hugging Face's datasets library\n",
    "dataset_names = {    \n",
    "    \"AR-LSAT (Zhong et al., 2021)\": \"olegbask/AR-LSAT\",\n",
    "    \"ReClor (Yu et al., 2020)\": \"metaeval/reclor\",\n",
    "    \"LogiQA 2.0 (Liu et al., 2023)\": \"baber/logiqa2\",    \n",
    "    \"RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007, 2008; Bentivogli et al., 2009)\": \"SetFit/rte\",\n",
    "    \"FOLIO (Han et al., 2022)\" : \"tasksource/folio\",    \n",
    "    \"PrOntoQA (Saparov and He, 2023)\" : \"longface/prontoqa-train\",\n",
    "    #\"PrOntoQA (Saparov and He, 2023)\" : \"longface/pronto-qa-flanT5\",\n",
    "    \"TellMeWhy (Lal et al., 2021)\" : \"StonyBrookNLP/tellmewhy\",\n",
    "    \"HotpotQA (Yang et al., 2018)\" : (\"hotpotqa/hotpot_qa\", \"fullwiki\"),  # 'distractor' or 'fullwiki'\n",
    "    \"GSM8K (Cobbe et al., 2021)\" : (\"openai/gsm8k\", \"main\"), #\"socratic\" or \"main\"\n",
    "    \"MRPC (Dolan & Brockett, 2015)\" : (\"glue\", \"mrpc\"), #\"SetFit/mrpc\",    \n",
    "    \"AbductionRules (Young et al., 2022)\" : \"tasksource/AbductionRules\",\n",
    "    \"ProofWriter (Tafjord et al., 2021)\" : \"tasksource/proofwriter\",\n",
    "    \"SQuAD 2.0 (Rajpurkar et al., 2018)\" : \"squad_v2\",\n",
    "    \"MultiNLI (Williams, Nangia, and Bowman 2018)\" : \"nyu-mll/multi_nli\",\n",
    "    \"Adversarial NLI (Nie et al. 2020)\" : \"Aivalf/NLI_adversarial_dataset\",\n",
    "    \"ConTRoL (Liu et al., 2021a)\" : \"tasksource/ConTRoL-nli\",\n",
    "    \"RACE (lai et al., 2017)\" : ('ehovy/race', 'all'),\n",
    "    \n",
    "}\n",
    "\n",
    "# Attempt to load each dataset and print a few examples\n",
    "for name, dataset_info in dataset_names.items():\n",
    "    try:\n",
    "        if isinstance(dataset_info, tuple):\n",
    "            dataset = load_dataset(*dataset_info, trust_remote_code=True)\n",
    "        else:\n",
    "            dataset = load_dataset(dataset_info, trust_remote_code=True)\n",
    "        \n",
    "        print(\"--------------------------------------------\")\n",
    "        print(f\"--    {name} Dataset Examples:\")\n",
    "        print(\"--------------------------------------------\")\n",
    "        for split in dataset.keys():\n",
    "            print(f\"Size of {split} set: {len(dataset[split])}\")\n",
    "        for i, example in enumerate(dataset['train'].take(2)):\n",
    "            print(f\"Example {i + 1}: {example}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {name}: {str(e)}\")\n",
    "        \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ced75360-0a41-4aa8-85da-50ba0ef1c6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in train split: {'True', 'Unknown', 'False'}\n",
      "Size of train split: 585552\n",
      "\n",
      "Unique labels in test split: {'True', 'Unknown', 'False'}\n",
      "Size of test split: 174476\n",
      "\n",
      "Unique labels in validation split: {'True', 'Unknown', 'False'}\n",
      "Size of validation split: 85468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from Hugging Face's datasets library\n",
    "dataset_name = \"tasksource/proofwriter\"\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "\n",
    "# Function to fetch unique labels from a dataset split and count the size\n",
    "def get_labels_and_size(data_split):\n",
    "    unique_labels = set()\n",
    "    for example in data_split:\n",
    "        unique_labels.add(example['answer'])\n",
    "    return unique_labels, len(data_split)\n",
    "\n",
    "# Fetch unique labels and sizes for each split in the dataset\n",
    "split_info = {\n",
    "    split: get_labels_and_size(dataset[split])\n",
    "    for split in dataset.keys()\n",
    "}\n",
    "\n",
    "# Display the unique labels and size for each split\n",
    "for split, (labels, size) in split_info.items():\n",
    "    print(f\"Unique labels in {split} split: {labels}\")\n",
    "    print(f\"Size of {split} split: {size}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648ce85c-19e4-4d01-8981-d4ae03c9178d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Answers in PrOntoQA Dataset:\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Load the PrOntoQA dataset\n",
    "dataset_name = \"longface/prontoqa-train\"\n",
    "dataset = load_dataset(dataset_name, trust_remote_code=True)\n",
    "\n",
    "# Initialize a set to store unique answers\n",
    "unique_answers = set()\n",
    "\n",
    "# Iterate through the training dataset to extract answers\n",
    "for example in dataset['train']:\n",
    "    # Extract the text after \"The answer is:\"\n",
    "    response_text = example['prompt']\n",
    "    answer_start = response_text.find(\"###The answer is:\") + len(\"###The answer is:\")\n",
    "    answer = response_text[answer_start:].strip()\n",
    "    unique_answers.add(answer)\n",
    "\n",
    "# Display all unique answers\n",
    "print(\"Unique Answers in PrOntoQA Dataset:\")\n",
    "for answer in unique_answers:\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc85d30-7a9c-42ea-a9a6-a0ce357f1132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.2 Combine Datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d3925d4-8eef-4d31-a52a-e708529b5e1b",
   "metadata": {},
   "source": [
    "def process_ar_lsat():\n",
    "    dataset = load_dataset(\"olegbask/AR-LSAT\")\n",
    "    # The dataset already has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['context']\n",
    "        question_text = example['question']\n",
    "        options = example['answers']\n",
    "        label = example['label']\n",
    "        source = \"AR-LSAT\"\n",
    "\n",
    "        # Create letter labels for options\n",
    "        option_labels = ['A', 'B', 'C', 'D', 'E']\n",
    "\n",
    "        # Combine the question and the options\n",
    "        question = question_text + '\\n'\n",
    "        for idx, (option_label, option_text) in enumerate(zip(option_labels, options)):\n",
    "            question += f\"{option_label}. {option_text}\\n\"\n",
    "\n",
    "        # Construct the answer with the correct option label and text\n",
    "        correct_option_label = option_labels[label]\n",
    "        correct_option_text = options[label]\n",
    "        answer = f\"{correct_option_label}. {correct_option_text}\"\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question.strip(),\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_reclor():\n",
    "    dataset = load_dataset(\"reclor\")\n",
    "    # ReClor has train and validation splits, test split is not available\n",
    "    # We will generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 500  # Since validation size is 500, we'll make test size the same\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['context']\n",
    "        question_text = example['question']\n",
    "        options = example['answers']\n",
    "        label = example['label']\n",
    "        source = \"ReClor\"\n",
    "\n",
    "        # Create letter labels for options\n",
    "        option_labels = ['A', 'B', 'C', 'D']\n",
    "\n",
    "        # Combine the question and the options\n",
    "        question = question_text + '\\n'\n",
    "        for option_label, option_text in zip(option_labels, options):\n",
    "            question += f\"{option_label}. {option_text}\\n\"\n",
    "\n",
    "        # Construct the answer with the correct option label and text\n",
    "        correct_option_label = option_labels[label]\n",
    "        correct_option_text = options[label]\n",
    "        answer = f\"{correct_option_label}. {correct_option_text}\"\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question.strip(),\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_logiqa2():\n",
    "    dataset = load_dataset(\"baber/logiqa2\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['text']\n",
    "        question_text = example['question']\n",
    "        options = example['options']\n",
    "        label = example['answer']\n",
    "        source = \"LogiQA 2.0\"\n",
    "\n",
    "        # Create letter labels for options\n",
    "        option_labels = ['A', 'B', 'C', 'D']\n",
    "\n",
    "        # Combine the question and the options\n",
    "        question = question_text + '\\n'\n",
    "        for option_label, option_text in zip(option_labels, options):\n",
    "            question += f\"{option_label}. {option_text}\\n\"\n",
    "\n",
    "        # Construct the answer with the correct option label and text\n",
    "        correct_option_label = option_labels[label]\n",
    "        correct_option_text = options[label]\n",
    "        answer = f\"{correct_option_label}. {correct_option_text}\"\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question.strip(),\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_rte():\n",
    "    dataset = load_dataset(\"SetFit/rte\")\n",
    "    # The dataset has train, validation, and test splits\n",
    "    # The test split contains unlabeled data; we'll ignore it\n",
    "\n",
    "    # Remove the original test split\n",
    "    del dataset['test']\n",
    "\n",
    "    # Get the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 277  # Create a new test split of size 277\n",
    "\n",
    "    # Shuffle the indices\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Split indices for the new test and updated train splits\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    # Create the new test and updated train splits\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['text1']\n",
    "        hypothesis = example['text2']\n",
    "        label_text = example['label_text']\n",
    "        source = \"RTE\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following statement classify as \"entailment\" or \"not entailment\"? {hypothesis}'\n",
    "\n",
    "        answer = label_text\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    # Map over the updated splits\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_folio():\n",
    "    dataset = load_dataset(\"tasksource/folio\")\n",
    "    # The dataset has train and validation splits; test split is missing\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 203  # Same size as validation split\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['premises']\n",
    "        conclusion = example['conclusion']\n",
    "        label = example['label']  # 'True' or 'False'\n",
    "        source = \"FOLIO\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following conclusion classify as \"True\", \"False\" or \"Uncertain\"? {conclusion}'\n",
    "\n",
    "        answer = label\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_prontoqa():\n",
    "    dataset = load_dataset(\"longface/prontoqa-train\")\n",
    "    # Dataset only has a train split; we'll generate validation and test splits\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)    \n",
    "    val_size = n // 10  # Using 10% of the data for validation\n",
    "    test_size = n // 10  # Using 10% of the data for test\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    val_indices = indices[:val_size]\n",
    "    test_indices = indices[val_size:val_size + test_size]\n",
    "    train_indices = indices[val_size + test_size:]\n",
    "    dataset['validation'] = train_data.select(val_indices)\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        source = \"PrOntoQA\"\n",
    "\n",
    "        # Extract the prompt\n",
    "        prompt = example['prompt']\n",
    "\n",
    "        # Split the prompt into context and response\n",
    "        context_part = prompt.split(\"###Response:\")[0].strip()\n",
    "\n",
    "        # Extract the context\n",
    "        context = context_part.replace(\"###Context:\", \"\").split(\"Is the following statement true or false?\")[0].strip()\n",
    "\n",
    "        # Extract the statement in question\n",
    "        statement = context_part.split(\"Is the following statement true or false?\")[-1].strip()\n",
    "\n",
    "        # Construct the question including the options \"True\" or \"False\"\n",
    "        question = f'Is the following statement \"True\" or \"False\"? {statement}'\n",
    "\n",
    "        # Extract the answer\n",
    "        answer_line = prompt.split(\"###The answer is:\")[-1].strip()\n",
    "        answer = answer_line.capitalize()  # Ensure it's \"True\" or \"False\"\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_tellmewhy():\n",
    "    dataset = load_dataset(\"StonyBrookNLP/tellmewhy\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['narrative']\n",
    "        original_question = example['question']\n",
    "        is_answerable = example['is_ques_answerable']  # 'Answerable' or 'Not Answerable'\n",
    "        source = \"TellMeWhy\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following question classify as \"Answerable\" or \"Not Answerable\"? {original_question}'\n",
    "\n",
    "        # Set the answer\n",
    "        answer = is_answerable\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_mrpc():\n",
    "    dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "    # The dataset has train and validation splits; test labels are not available\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 1725  # As per your provided test size\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    # Label mapping\n",
    "    label_mapping = {'0': 'not equivalent', '1': 'equivalent'}\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['sentence1']\n",
    "        sentence2 = example['sentence2']\n",
    "        label = str(example['label'])  # Convert label to string '0' or '1'\n",
    "        source = \"MRPC\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following statement classify as \"not equivalent\" or \"equivalent\"? {sentence2}'\n",
    "\n",
    "        # Map the label to the corresponding string\n",
    "        answer = label_mapping.get(label, 'not equivalent')  # Default to 'not equivalent' if label not found\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in dataset:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                map_example,\n",
    "                remove_columns=dataset[split].column_names\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_proofwriter():\n",
    "    dataset = load_dataset(\"tasksource/proofwriter\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['theory']\n",
    "        question_statement = example['question']\n",
    "        answer = example['answer']\n",
    "        source = \"ProofWriter\"\n",
    "\n",
    "        # Construct the question including the options \"True\" or \"False\"\n",
    "        question = f'Is the following statement \"True\", \"False\" or \"Unknown\"? {question_statement}'\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_multi_nli():\n",
    "    dataset = load_dataset(\"multi_nli\")\n",
    "    # The dataset has train and validation splits; test split lacks labels\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 9832  # As per the size of the test set\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    # Combine validation matched and mismatched\n",
    "    validation = concatenate_datasets([dataset['validation_matched'], dataset['validation_mismatched']])\n",
    "    dataset['validation'] = validation\n",
    "\n",
    "    # Label mapping\n",
    "    label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = example['label']  # Labels are integers {0,1,2}\n",
    "\n",
    "        # Map the label to the corresponding string\n",
    "        answer = label_mapping.get(label, 'neutral')  # Default to 'neutral' if label not found\n",
    "        source = \"MultiNLI\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? {hypothesis}'\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_anli():\n",
    "    dataset = load_dataset(\"Aivalf/NLI_adversarial_dataset\")\n",
    "    # Check which splits are available\n",
    "    splits = dataset.keys()\n",
    "    # If validation or test splits are missing, generate them from the train set\n",
    "    if 'validation' not in dataset or 'test' not in dataset:\n",
    "        train_data = dataset['train']\n",
    "        n = len(train_data)\n",
    "        val_size = n // 10  # Using 10% of the data for validation\n",
    "        test_size = n // 10  # Using 10% of the data for testing\n",
    "        indices = list(range(n))\n",
    "        random.shuffle(indices)\n",
    "        val_indices = indices[:val_size]\n",
    "        test_indices = indices[val_size:val_size + test_size]\n",
    "        train_indices = indices[val_size + test_size:]\n",
    "        dataset['validation'] = train_data.select(val_indices)\n",
    "        dataset['test'] = train_data.select(test_indices)\n",
    "        dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = example['label']  # Labels are strings like 'ENTAILMENT', 'CONTRADICTION', 'NEUTRAL'\n",
    "        source = \"Adversarial NLI\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following statement classify as \"ENTAILMENT\", \"CONTRADICTION\", or \"NEUTRAL\"? {hypothesis}'\n",
    "\n",
    "        answer = label\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in dataset:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                map_example,\n",
    "                remove_columns=dataset[split].column_names\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_control():\n",
    "    dataset = load_dataset(\"tasksource/ConTRoL-nli\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        context = example['premise']\n",
    "        hypothesis = example['hypothesis']\n",
    "        label = example['label']\n",
    "        source = \"ConTRoL\"\n",
    "\n",
    "        # Construct the question\n",
    "        question = f'Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? {hypothesis}'\n",
    "\n",
    "        answer = label\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Answer': answer,\n",
    "            'Source Dataset': source\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def combineDatasets():\n",
    "    datasets = []\n",
    "    ar_lsat = process_ar_lsat()\n",
    "    reclor = process_reclor()\n",
    "    logiqa2 = process_logiqa2()\n",
    "    rte = process_rte()\n",
    "    folio = process_folio()\n",
    "    prontoqa = process_prontoqa()\n",
    "    tellmewhy = process_tellmewhy()\n",
    "    mrpc = process_mrpc()\n",
    "    proofwriter = process_proofwriter()\n",
    "    multi_nli = process_multi_nli()\n",
    "    anli = process_anli()\n",
    "    control = process_control()\n",
    "\n",
    "    # Combine datasets\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    combined_dataset = DatasetDict()\n",
    "    for split in splits:\n",
    "        datasets_to_concat = []\n",
    "        for dataset in [ar_lsat, reclor, logiqa2, rte, folio, prontoqa, tellmewhy, mrpc, proofwriter, multi_nli, anli, control]:\n",
    "            if split in dataset:\n",
    "                datasets_to_concat.append(dataset[split])\n",
    "        combined_dataset[split] = concatenate_datasets(datasets_to_concat)\n",
    "\n",
    "    # Save the combined dataset\n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "combined_dataset = combineDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8efad542-6c59-494b-b135-d723c3f1425e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6e86af3ffd40e5a57604c2007d3c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4934 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e03b84db6143c2afca0cb75e2cbea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87866 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b40705a3191472b9572b679f85e9cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4887 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_ar_lsat():\n",
    "    dataset = load_dataset(\"olegbask/AR-LSAT\")\n",
    "    # The dataset already has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        # 'label' is an integer index (0-based) indicating the correct option\n",
    "        label_index = example['label']\n",
    "        # 'answers' is a list of option texts\n",
    "        options = example['answers']\n",
    "        # Get the text of the correct answer\n",
    "        label_text = options[label_index]\n",
    "        return {\n",
    "            'Context': example['context'],\n",
    "            'Question': example['question'],\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"MCQA\",\n",
    "            'Source Dataset': \"AR-LSAT\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "def process_logiqa2():\n",
    "    dataset = load_dataset(\"baber/logiqa2\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    def map_example(example):\n",
    "        label_index = example['answer']\n",
    "        options = example['options']\n",
    "        label_text = options[label_index]\n",
    "        return {\n",
    "            'Context': example['text'],\n",
    "            'Question': example['question'],\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"MCQA\",\n",
    "            'Source Dataset': \"LogiQA 2.0\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_control():\n",
    "    dataset = load_dataset(\"tasksource/ConTRoL-nli\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    options = ['neutral', 'entailment', 'contradiction']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['label']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['premise'],\n",
    "            'Question': f'Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? {example[\"hypothesis\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"ConTRoL\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_rte():\n",
    "    dataset = load_dataset(\"SetFit/rte\")\n",
    "    # The dataset has train, validation, and test splits\n",
    "    # The test split contains unlabeled data; we'll ignore it\n",
    "\n",
    "    # Remove the original test split\n",
    "    del dataset['test']\n",
    "\n",
    "    # Get the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 277  # Create a new test split of size 277\n",
    "\n",
    "    # Shuffle the indices\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Split indices for the new test and updated train splits\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "\n",
    "    # Create the new test and updated train splits\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    options = ['entailment', 'not entailment']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['label_text']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['text1'],\n",
    "            'Question': f'Does the following statement classify as \"entailment\" or \"not entailment\"? {example[\"text2\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"RTE\"\n",
    "        }\n",
    "\n",
    "    # Map over the updated splits\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_folio():\n",
    "    dataset = load_dataset(\"tasksource/folio\")\n",
    "    # The dataset has train and validation splits; test split is missing\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 203  # Same size as validation split\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    options = ['True', 'False', 'Uncertain']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['label']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['premises'],\n",
    "            'Question': f'Does the following conclusion classify as \"True\", \"False\" or \"Uncertain\"? {example[\"conclusion\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"FOLIO\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_prontoqa():\n",
    "    dataset = load_dataset(\"longface/prontoqa-train\")\n",
    "    # Dataset only has a train split; we'll generate validation and test splits\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    val_size = n // 10  # Using 10% of the data for validation\n",
    "    test_size = n // 10  # Using 10% of the data for test\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    val_indices = indices[:val_size]\n",
    "    test_indices = indices[val_size:val_size + test_size]\n",
    "    train_indices = indices[val_size + test_size:]\n",
    "    dataset['validation'] = train_data.select(val_indices)\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    options = ['True', 'False']\n",
    "\n",
    "    def map_example(example):\n",
    "        # Extract the prompt\n",
    "        prompt = example['prompt']\n",
    "\n",
    "        # Split the prompt into context and response\n",
    "        context_part = prompt.split(\"###Response:\")[0].strip()\n",
    "\n",
    "        # Extract the context\n",
    "        context = context_part.replace(\"###Context:\", \"\").split(\"Is the following statement true or false?\")[0].strip()\n",
    "\n",
    "        # Extract the statement in question\n",
    "        statement = context_part.split(\"Is the following statement true or false?\")[-1].strip()\n",
    "\n",
    "        # Construct the question including the options \"True\" or \"False\"\n",
    "        question = f'Is the following statement \"True\" or \"False\"? {statement}'\n",
    "\n",
    "        # Extract the answer\n",
    "        answer_line = prompt.split(\"###The answer is:\")[-1].strip()\n",
    "        label_text = answer_line.capitalize()  # Ensure it's \"True\" or \"False\"\n",
    "        label_index = options.index(label_text)\n",
    "\n",
    "        return {\n",
    "            'Context': context,\n",
    "            'Question': question,\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"PrOntoQA\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_tellmewhy():\n",
    "    dataset = load_dataset(\"StonyBrookNLP/tellmewhy\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    options = ['Answerable', 'Not Answerable']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['is_ques_answerable']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['narrative'],\n",
    "            'Question': f'Does the following question classify as \"Answerable\" or \"Not Answerable\"? {example[\"question\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"TellMeWhy\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_mrpc():\n",
    "    dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "    # The dataset has train and validation splits; test labels are not available\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 1725  # As per your provided test size\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    # Label mapping\n",
    "    label_mapping = {'0': 'not equivalent', '1': 'equivalent'}\n",
    "    options = ['not equivalent', 'equivalent']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = label_mapping.get(str(example['label']), 'not equivalent')\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['sentence1'],\n",
    "            'Question': f'Does the following statement classify as \"not equivalent\" or \"equivalent\"? {example[\"sentence2\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"MRPC\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in dataset:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                map_example,\n",
    "                remove_columns=dataset[split].column_names\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_proofwriter():\n",
    "    dataset = load_dataset(\"tasksource/proofwriter\")\n",
    "    # The dataset has train, validation, and test splits with specified sizes\n",
    "\n",
    "    options = ['True', 'False', 'Unknown']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['answer']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['theory'],\n",
    "            'Question': f'Is the following statement \"True\", \"False\" or \"Unknown\"? {example[\"question\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"ProofWriter\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_multi_nli():\n",
    "    dataset = load_dataset(\"multi_nli\")\n",
    "    # The dataset has train and validation splits; test split lacks labels\n",
    "    # We'll generate a test split from the training data\n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 9832  # As per the size of the test set\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    # Combine validation matched and mismatched\n",
    "    validation = concatenate_datasets([dataset['validation_matched'], dataset['validation_mismatched']])\n",
    "    dataset['validation'] = validation\n",
    "\n",
    "    # Label mapping\n",
    "    label_mapping = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "    options = ['entailment', 'neutral', 'contradiction']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = label_mapping.get(example['label'], 'neutral')\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['premise'],\n",
    "            'Question': f'Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? {example[\"hypothesis\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"MultiNLI\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def process_anli():\n",
    "    dataset = load_dataset(\"Aivalf/NLI_adversarial_dataset\")\n",
    "    # Check which splits are available\n",
    "    splits = dataset.keys()\n",
    "    # If validation or test splits are missing, generate them from the train set\n",
    "    if 'validation' not in dataset or 'test' not in dataset:\n",
    "        train_data = dataset['train']\n",
    "        n = len(train_data)\n",
    "        val_size = n // 10  # Using 10% of the data for validation\n",
    "        test_size = n // 10  # Using 10% of the data for testing\n",
    "        indices = list(range(n))\n",
    "        random.seed(42)\n",
    "        random.shuffle(indices)\n",
    "        val_indices = indices[:val_size]\n",
    "        test_indices = indices[val_size:val_size + test_size]\n",
    "        train_indices = indices[val_size + test_size:]\n",
    "        dataset['validation'] = train_data.select(val_indices)\n",
    "        dataset['test'] = train_data.select(test_indices)\n",
    "        dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    options = ['ENTAILMENT', 'CONTRADICTION', 'NEUTRAL']\n",
    "\n",
    "    def map_example(example):\n",
    "        label_text = example['label']\n",
    "        label_index = options.index(label_text)\n",
    "        return {\n",
    "            'Context': example['premise'],\n",
    "            'Question': f'Does the following statement classify as \"ENTAILMENT\", \"CONTRADICTION\", or \"NEUTRAL\"? {example[\"hypothesis\"]}',\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"Classification\",\n",
    "            'Source Dataset': \"Adversarial NLI\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in dataset:\n",
    "            dataset[split] = dataset[split].map(\n",
    "                map_example,\n",
    "                remove_columns=dataset[split].column_names\n",
    "            )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def process_race():\n",
    "    dataset = load_dataset(\"ehovy/race\", \"all\") \n",
    "\n",
    "    def map_example(example):\n",
    "        # RACE data includes multiple choice questions with four options\n",
    "        options = example['options']\n",
    "        # Correct answer is stored as 'answer'\n",
    "        label_text = example['answer']\n",
    "        # Convert the letter answer to an index (A=0, B=1, C=2, D=3)\n",
    "        label_index = ord(label_text) - ord('A')\n",
    "        return {\n",
    "            'Context': example['article'],\n",
    "            'Question': example['question'],\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"MCQA\",\n",
    "            'Source Dataset': \"RACE\"\n",
    "        }\n",
    "\n",
    "    # Apply the mapping function to each split\n",
    "    for split in dataset.keys():\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "def process_reclor():\n",
    "    dataset = load_dataset(\"metaeval/reclor\")\n",
    "    # ReClor has train and validation splits, test split is not available\n",
    "    # We will generate a test split from the training data        \n",
    "    train_data = dataset['train']\n",
    "    n = len(train_data)\n",
    "    test_size = 500  # Since validation size is 500, we'll make test size the same\n",
    "    indices = list(range(n))\n",
    "    random.seed(42)\n",
    "    random.shuffle(indices)\n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    dataset['test'] = train_data.select(test_indices)\n",
    "    dataset['train'] = train_data.select(train_indices)\n",
    "\n",
    "    def map_example(example):\n",
    "        label_index = example['label']\n",
    "        options = example['answers']\n",
    "        label_text = options[label_index]\n",
    "        return {\n",
    "            'Context': example['context'],\n",
    "            'Question': example['question'],\n",
    "            'Options': options,\n",
    "            'Label_Text': label_text,\n",
    "            'Label': label_index,\n",
    "            'Type': \"MCQA\",\n",
    "            'Source Dataset': \"ReClor\"\n",
    "        }\n",
    "\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        dataset[split] = dataset[split].map(\n",
    "            map_example,\n",
    "            remove_columns=dataset[split].column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def combineDatasets():\n",
    "    datasets = []\n",
    "    ar_lsat = process_ar_lsat()\n",
    "    reclor = process_reclor()\n",
    "    logiqa2 = process_logiqa2()\n",
    "    rte = process_rte()\n",
    "    folio = process_folio()\n",
    "    prontoqa = process_prontoqa()\n",
    "    tellmewhy = process_tellmewhy()\n",
    "    mrpc = process_mrpc()\n",
    "    proofwriter = process_proofwriter()\n",
    "    multi_nli = process_multi_nli()\n",
    "    anli = process_anli()\n",
    "    control = process_control()\n",
    "    race = process_race()\n",
    "\n",
    "    # Combine datasets\n",
    "    splits = ['train', 'validation', 'test']\n",
    "    combined_dataset = DatasetDict()\n",
    "    for split in splits:\n",
    "        datasets_to_concat = []\n",
    "        for dataset in [ar_lsat, reclor, logiqa2, rte, folio, prontoqa, tellmewhy, mrpc, proofwriter, multi_nli, anli, control, race]:\n",
    "            if split in dataset:\n",
    "                datasets_to_concat.append(dataset[split])\n",
    "        combined_dataset[split] = concatenate_datasets(datasets_to_concat)\n",
    "\n",
    "    # Save the combined dataset\n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "combined_dataset = combineDatasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55a8b627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1163327\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 123613\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 205891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8793704b-277c-4603-8b0f-d295c9a1e652",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a701bc64236f4cc1916d97eb3bb0ddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1163327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97275111395649ae91b22a4bed58ea56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/123613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2faf447389459b93df2d0851247bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_dataset.save_to_disk('C:/combined_dataset-2', max_shard_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec5904-4dcc-4e15-a648-c3a1ab905fa6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2.3 Explore The Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d71ad5b-d34a-4e26-9599-42b791c16fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in each split:\n",
      "Train: 1163327\n",
      "Validation: 123613\n",
      "Test: 205891\n",
      "\n",
      "Total number of examples: 1492831\n",
      "\n",
      "Source Dataset counts in each split:\n",
      "\n",
      "Train split:\n",
      "AR-LSAT: 1585\n",
      "ReClor: 4138\n",
      "LogiQA 2.0: 12567\n",
      "RTE: 2213\n",
      "FOLIO: 798\n",
      "PrOntoQA: 2304\n",
      "TellMeWhy: 71892\n",
      "MRPC: 1943\n",
      "ProofWriter: 585552\n",
      "MultiNLI: 382870\n",
      "Adversarial NLI: 2880\n",
      "ConTRoL: 6719\n",
      "RACE: 87866\n",
      "\n",
      "Validation split:\n",
      "AR-LSAT: 231\n",
      "ReClor: 500\n",
      "LogiQA 2.0: 1569\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 8976\n",
      "MRPC: 408\n",
      "ProofWriter: 85468\n",
      "MultiNLI: 19647\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 799\n",
      "RACE: 4887\n",
      "\n",
      "Test split:\n",
      "AR-LSAT: 230\n",
      "ReClor: 500\n",
      "LogiQA 2.0: 1572\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 10689\n",
      "MRPC: 1725\n",
      "ProofWriter: 174476\n",
      "MultiNLI: 9832\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 805\n",
      "RACE: 4934\n",
      "\n",
      "Total counts per Source Dataset:\n",
      "AR-LSAT: 2046\n",
      "ReClor: 5138\n",
      "LogiQA 2.0: 15708\n",
      "RTE: 2767\n",
      "FOLIO: 1204\n",
      "PrOntoQA: 2880\n",
      "TellMeWhy: 91557\n",
      "MRPC: 4076\n",
      "ProofWriter: 845496\n",
      "MultiNLI: 412349\n",
      "Adversarial NLI: 3600\n",
      "ConTRoL: 8323\n",
      "RACE: 97687\n",
      "\n",
      "Examples from each Source Dataset:\n",
      "\n",
      "Source: RACE (from train split)\n",
      "Context: Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.\n",
      "\n",
      "Question: We can know from the passage that the author works as a_.\n",
      "\n",
      "Options: ['doctor', 'model', 'teacher', 'reporter']\n",
      "\n",
      "Label_Text: C\n",
      "\n",
      "Label_Index: 2\n",
      "\n",
      "\n",
      "Source: Adversarial NLI (from train split)\n",
      "Context: Boomerang (1992 film) . Boomerang was released in the United States on July 1 , 1992 .\n",
      "\n",
      "Question: Does the following statement classify as \"ENTAILMENT\", \"CONTRADICTION\", or \"NEUTRAL\"? The United States had Boomerang ( 1992 show ) released .\n",
      "\n",
      "Options: ['ENTAILMENT', 'CONTRADICTION', 'NEUTRAL']\n",
      "\n",
      "Label_Text: ENTAILMENT\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: RTE (from train split)\n",
      "Context: Civic Alliance, the umbrella group of civic groups involved in rebuilding, hosted a large, public, town-hall meeting for 5,000 people, called \"Listening to the City\" in July 2002.\n",
      "\n",
      "Question: Does the following statement classify as \"entailment\" or \"not entailment\"? The Civic Alliance is an umbrella group.\n",
      "\n",
      "Options: ['entailment', 'not entailment']\n",
      "\n",
      "Label_Text: entailment\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: PrOntoQA (from train split)\n",
      "Context: Dumpuses are bright. Dumpuses are wumpuses. Wumpuses are not opaque. Each wumpus is a numpus. Every numpus is angry. Each numpus is a jompus. Jompuses are not hot. Jompuses are rompuses. Every rompus is spicy. Rompuses are yumpuses. Yumpuses are nervous. Every yumpus is a tumpus. Zumpuses are not bright. Tumpuses are earthy. Tumpuses are impuses. Sam is a dumpus.\n",
      "\n",
      "Question: Is the following statement \"True\" or \"False\"? Sam is bright.\n",
      "\n",
      "Options: ['True', 'False']\n",
      "\n",
      "Label_Text: True\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: TellMeWhy (from train split)\n",
      "Context: Cam ordered a pizza and took it home. He opened the box to take out a slice. Cam discovered that the store did not cut the pizza for him. He looked for his pizza cutter but did not find it. He had to use his chef knife to cut a slice.\n",
      "\n",
      "Question: Does the following question classify as \"Answerable\" or \"Not Answerable\"? Why did Cam order a pizza?\n",
      "\n",
      "Options: ['Answerable', 'Not Answerable']\n",
      "\n",
      "Label_Text: Not Answerable\n",
      "\n",
      "Label_Index: 1\n",
      "\n",
      "\n",
      "Source: AR-LSAT (from train split)\n",
      "Context: Exactly six trade representatives negotiate a treaty: Klosnik, Londi, Manley, Neri, Osata, Poirier. There are exactly six chairs evenly spaced around a circular table. The chairs are numbered 1 through 6, with successively numbered chairs next to each other and chair number 1 next to chair number 6. Each chair is occupied by exactly one of the representatives. The following conditions apply: Poirier sits immediately next to Neri. Londi sits immediately next to Manley, Neri, or both. Klosnik does not sit immediately next to Manley. If Osata sits immediately next to Poirier, Osata does not sit immediately next to Manley.\n",
      "\n",
      "Question: Which one of the following seating arrangements of the six representatives in chairs 1 through 6 would NOT violate the stated conditions?\n",
      "\n",
      "Options: ['Klosnik, Poirier, Neri, Manley, Osata, Londi', 'Klosnik, Londi, Manley, Poirier, Neri, Osata', 'Klosnik, Londi, Manley, Osata, Poirier, Neri', 'Klosnik, Osata, Poirier, Neri, Londi, Manley', 'Klosnik, Neri, Londi, Osata, Manley, Poirier']\n",
      "\n",
      "Label_Text: Klosnik, Londi, Manley, Poirier, Neri, Osata\n",
      "\n",
      "Label_Index: 1\n",
      "\n",
      "\n",
      "Source: LogiQA 2.0 (from train split)\n",
      "Context: A college will continue to implement the overseas funding plan this year. It plans to select several of the six teachers from Mr. Liu, Mr. Zhang, Mr. Wang, Mr. Ma, Mr. Niu and Mr. Zhou to visit abroad. Due to the limitations of funding, the needs of discipline development, curriculum arrangement, place and time of each student's visit, the selection shall meet the following conditions: (1) Mr. Liu is the reserve discipline leader of the college, This time we have to send out. (2) if we choose Mr. Liu, we should also choose Mr. Zhou, but we can't choose Mr. Zhang. (3) only if Mr. Niu can't choose, at least one of Mr. Wang and Mr. Ma can choose. (4) if we don't choose Mr. Wang, we don't choose Mr. Zhou either.\n",
      "\n",
      "Question: If the above statement is true, which of the followings must be true?\n",
      "\n",
      "Options: [\"Mr. Niu didn't choose, but Mr. Zhou did\", \"Mr. Liu was chose, but Mr. Ma didn't\", 'Mr. Wang and Mr. Ma were chosen', 'Neither Mr. Wang nor Mr. Niu was elected']\n",
      "\n",
      "Label_Text: Mr. Niu didn't choose, but Mr. Zhou did\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: FOLIO (from train split)\n",
      "Context: All humans are capable of abstract thoughts.\n",
      "Plants are not capable of abstract thoughts.\n",
      "All multicellular creatures that are autotrophic or digest food internally are plants and animals.\n",
      "All goats are animals.\n",
      "Dirt is not an animal.\n",
      "Hulu is a goat or a human.\n",
      "Hulu is a multicellular creature that is autotrophic or digests food internally. \n",
      "\n",
      "Question: Does the following conclusion classify as \"True\", \"False\" or \"Uncertain\"? Hulu is not capable of abstract thoughts.\n",
      "\n",
      "Options: ['True', 'False', 'Uncertain']\n",
      "\n",
      "Label_Text: Uncertain\n",
      "\n",
      "Label_Index: 2\n",
      "\n",
      "\n",
      "Source: MRPC (from train split)\n",
      "Context: \" It 's safe to assume that the Senate is prepared to pass some form of cap , \" King said .\n",
      "\n",
      "Question: Does the following statement classify as \"not equivalent\" or \"equivalent\"? Its safe to assume the Senate is prepared to pass some form of a cap .... The level of it is to be debated .\n",
      "\n",
      "Options: ['not equivalent', 'equivalent']\n",
      "\n",
      "Label_Text: not equivalent\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: MultiNLI (from train split)\n",
      "Context: Sunset Plaza (8600-8700 Sunset Boulevard at Sunset Plaza Drive) has been an elite shopping area since 1934.\n",
      "\n",
      "Question: Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? Sunset Plaza has been an exclusive shopping plaza for ages.\n",
      "\n",
      "Options: ['entailment', 'neutral', 'contradiction']\n",
      "\n",
      "Label_Text: entailment\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: ProofWriter (from train split)\n",
      "Context: Gary is furry. Gary is nice. Gary is red. Gary is rough. Gary is not smart. Gary is white. Gary is young. If Gary is nice and Gary is not white then Gary is red. If someone is white then they are red. All young people are furry. If someone is white and not red then they are furry. Smart, red people are rough. If Gary is not red and Gary is not furry then Gary is not smart. If Gary is white then Gary is not smart. If someone is rough and not white then they are not smart.\n",
      "\n",
      "Question: Is the following statement \"True\", \"False\" or \"Unknown\"? Gary is white.\n",
      "\n",
      "Options: ['True', 'False', 'Unknown']\n",
      "\n",
      "Label_Text: True\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: ConTRoL (from train split)\n",
      "Context: 100 Years of the Western Workplace Conditions in the working environment of Western countries changed significantly over the 20th century. Though not without some associated problems, these changes may be viewed generally as positive: child labour all but ceased, wages rose, the number of working hours in a week decreased, pension policies became standard, fringe benefits multiplied and concerns over health and safety issues were enforced. The collection of data relating to work conditions also became a far more exact science. In particular, there were important developments in methodology and data gathering. Additionally, there was a major expansion of the data collection effort more people became involved in learning about the workplace; and, for the first time, results started to be published. This being the case, at the end of the century, not only were most workers better off than their early 20th century predecessors had been, but they were also in a position to understand how and why this was the case. By carefully analyzing the statistical data made available, specific changes in the workplace not least regarding the concept of what work should involve became clearly discernible. The most obvious changes to the workplace involved the size and composition of the countries workforces. Registering only 24 million in 1900 (and including labourers of age ten and up) and 139 million (aged 16 and older), the size of Americas workforce, for instance, increased by almost six-fold in line with its overall population growth. At the same time, the composition of the workforce shifted from industries dominated by primary production occupations, such as farmers and foresters, to those dominated by professional, technical and, in particular, service workers. At the beginning of the 20th century, 38% of all American workers were employed on farms, by the end of the same century, that figure had fallen to less than 3 %. In Europe, much the same process occurred. In the 1930s, in every European country, bar Britain and Belgium, more than 20 per cent of the population worked in agriculture. By the 1980s, however, the farming populations of all developed countries, excluding Eastern Europe, had dropped to ten per cent and often even lower. At the same time, capital intensive farming using highly mechanized techniques dramatically reduced the numbers needed to farm there. And therein lay the problem. While the workplace became a safer and more productive environment, a world away from the harsh working conditions of our forefathers, the switch from an agricultural to a modern working environment also created massive unemployment in many countries. Fundamental to this problem was the widespread move from the countryside to the city. Having lost their livelihoods, the worlds peasant populations amassed in ever larger numbers in already crowded communities, where rates of job growth failed to keep up with internal migration. As a result, thousands were left squatting in shanty towns on the periphery of cities, waiting for jobs that might never arrive. While this was (and is) particularly true of Third World countries, the same phenomenon could also be witnessed in several American, French, English and German cities in the late 20th century. From a different and more positive perspective, in the 20th century, women became visible and active members of all sectors of the Western workplace. In 1900, only 19% of European women of working age participated in the labour force; by 1999, this figure had risen to 60%. In 1900, only 1% of the countrys lawyers and 6% of its physicians were female; by contrast, the figures were 29% and 24% in 1999. A recent survey of French teenagers, both male and female, revealed that over 50% of those polled thought that, in any job (bar those involving military service), women make better employees, as they are less likely to become riled under stress and less overtly competitive than men. The last and perhaps most significant change to the 20th-century workplace involved the introduction of technology. The list of technological improvements in the workplace is endless: communication and measuring devices, computers of all shapes and sizes, x-ray, lasers, neon lights, stainless steel, and so on and on. Such improvements led to a more productive, safer work environment. Moreover, the fact that medicine improved so dramatically led to an increase in the average lifespan among Western populations. In turn, workers of very different ages were able to work shoulder to shoulder, and continue in their jobs far longer. By the end of 20th century, the Western workplace had undergone remarkable changes. In general, both men and women worked fewer hours per day for more years under better conditions. Yet, the power of agriculture had waned as farmers and foresters moved to cities to earn greater salaries as annalists and accountants. For those who could not make this transition, however, life at the dawn of the new century seemed less appealing.\n",
      "\n",
      "Question: Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? Improvements in medicine led to workers earning more over a longer period.\n",
      "\n",
      "Options: ['neutral', 'entailment', 'contradiction']\n",
      "\n",
      "Label_Text: neutral\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: ReClor (from train split)\n",
      "Context: Two years ago, the human resources department at Harrison' s Technologies had 175 sexual harassment cases open, a number that at that time led all companies in the sector. At that time, a typical claim took six months to process and clear. In response to this high number, the CEO' s Advisory Team published a comprehensive set of guidelines for training employees in the identification and avoidance of behaviors that could constitute sexual harassment. At this writing, the human resources department has only sixty sexual harassment cases open. Clearly, the guidelines published by the CEO' s Advisory Team were effective in reducing the occurrence of new sexual harassment claims at Harrison' s Technologies.\n",
      "\n",
      "Question: Which of the following, if true, most seriously weakens the conclusion above?\n",
      "\n",
      "Options: [\"The human resources department at Anthony Information Industries, a principal rival of Harrison's with a similar number of employees, opened no new sexual harassment claims in the past year.\", \"New paperwork and procedure, also introduced two years ago, has allowed the human resources department at Harrison's Technologies to process and close almost all new claims of sexual harassment in less than one month.\", 'The majority of sexual harassment claims lead to the termination of employees found guilty; new employees hired to fill these positions need to be trained in the guidelines.', \"Independent consultants using scientifically designed surveys have found that the majority of current employees at Harrison's Technologies are not properly educated on exactly what behaviors constitute sexual harassment.\"]\n",
      "\n",
      "Label_Text: New paperwork and procedure, also introduced two years ago, has allowed the human resources department at Harrison's Technologies to process and close almost all new claims of sexual harassment in less than one month.\n",
      "\n",
      "Label_Index: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('combined_dataset-2')\n",
    "\n",
    "# Get the splits\n",
    "splits = combined_dataset.keys()\n",
    "\n",
    "# Print number of examples in each split\n",
    "print(\"Number of examples in each split:\")\n",
    "for split in splits:\n",
    "    num_examples = len(combined_dataset[split])\n",
    "    print(f\"{split.capitalize()}: {num_examples}\")\n",
    "\n",
    "# Calculate total number of examples\n",
    "total_examples = sum(len(combined_dataset[split]) for split in splits)\n",
    "print(f\"\\nTotal number of examples: {total_examples}\")\n",
    "\n",
    "# Print source dataset statistics for each split\n",
    "print(\"\\nSource Dataset counts in each split:\")\n",
    "for split in splits:\n",
    "    source_counter = Counter(combined_dataset[split]['Source Dataset'])\n",
    "    print(f\"\\n{split.capitalize()} split:\")\n",
    "    for source, count in source_counter.items():\n",
    "        print(f\"{source}: {count}\")\n",
    "\n",
    "# Print total counts per source dataset across all splits\n",
    "total_source_counter = Counter()\n",
    "for split in splits:\n",
    "    total_source_counter.update(combined_dataset[split]['Source Dataset'])\n",
    "\n",
    "print(\"\\nTotal counts per Source Dataset:\")\n",
    "for source, count in total_source_counter.items():\n",
    "    print(f\"{source}: {count}\")\n",
    "\n",
    "# Get all unique source datasets\n",
    "all_sources = set(total_source_counter.keys())\n",
    "\n",
    "# Display an example from each source dataset\n",
    "print(\"\\nExamples from each Source Dataset:\")\n",
    "for source in all_sources:\n",
    "    found = False\n",
    "    for split in splits:\n",
    "        # Get the dataset split\n",
    "        ds = combined_dataset[split]\n",
    "        # Filter to examples from this source\n",
    "        indices = [i for i, s in enumerate(ds['Source Dataset']) if s == source]\n",
    "        if indices:\n",
    "            idx = indices[0]\n",
    "            example = ds[idx]\n",
    "            print(f\"\\nSource: {source} (from {split} split)\")\n",
    "            print(f\"Context: {example['Context']}\\n\")\n",
    "            print(f\"Question: {example['Question']}\\n\")\n",
    "            print(f\"Options: {example['Options']}\\n\")\n",
    "            print(f\"Label_Text: {example['Label_Text']}\\n\")\n",
    "            print(f\"Label_Index: {example['Label']}\\n\")\n",
    "            found = True\n",
    "            break  # Move to the next source\n",
    "    if not found:\n",
    "        print(f\"No example found for source {source}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01b0b53-d940-4ef8-ad94-5b16a4c5099f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4 Clean The Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "850da193-88c3-4f9f-b31e-aae558af0d70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1163327\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 123613\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 205891\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('combined_dataset-2')\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b69ea54-e3c5-4854-a93b-de243babb754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7314f1b0b144431cba93df16712ceeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for RACE: 1110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b42354bd943e4d7eaf708dd8db1858da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for Adversarial NLI: 324\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d15f258432f41b18c21cd9b557edb51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for RTE: 303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2702c4df892f4db0aab4c57865e0b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for PrOntoQA: 156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe16a565694404899ba82fbe5aa473a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for TellMeWhy: 113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74191f70f9364f9c8c381b065a6aeb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for AR-LSAT: 206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69afcd92a68546b7992d83fa8fb23b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for LogiQA 2.0: 270\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3807b7d8c172415193d761dae31d9b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for FOLIO: 229\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107c91fb40314a45b68a59811c0ec29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for MRPC: 116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "196683b01ac84042990df0bb6d6e491c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for MultiNLI: 341\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b957c53a87e4a5faa48bc232480f4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for ProofWriter: 281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2546cf1c671d465092118016a04387a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for ConTRoL: 1702\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e925d37bf024d83a127f0f63b229015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of tokenized input for ReClor: 184\n"
     ]
    }
   ],
   "source": [
    "def find_max_length(data):\n",
    "    max_len = 0\n",
    "    # Tokenize without truncation\n",
    "    for example in data:\n",
    "        # Concatenate the context and question text\n",
    "        text = example['Context'] + \" \" + example['Question']\n",
    "        tokenized_text = tokenizer.encode(text, add_special_tokens=True)\n",
    "        max_len = max(max_len, len(tokenized_text))\n",
    "    return max_len\n",
    "\n",
    "# Loop over each source in the dataset\n",
    "unique_sources = set(combined_dataset[\"test\"][\"Source Dataset\"])\n",
    "max_lengths = {}\n",
    "\n",
    "for source in unique_sources:\n",
    "    # Filter the dataset for the current source\n",
    "    source_data = combined_dataset[\"test\"].filter(lambda x: x['Source Dataset'] == source)\n",
    "    # Calculate the max length for the current source\n",
    "    max_length = find_max_length(source_data)\n",
    "    max_lengths[source] = max_length\n",
    "    print(f\"Maximum length of tokenized input for {source}: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "694f9f6a-1641-40ec-b6e3-0f0cf966d3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Counting examples with fewer than 500 tokens in the train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da809c417c034236b5702f488cb831f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1163327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in train split with fewer than 500 tokens:\n",
      "AR-LSAT: 1585\n",
      "ReClor: 4138\n",
      "LogiQA 2.0: 12567\n",
      "RTE: 2213\n",
      "FOLIO: 798\n",
      "PrOntoQA: 2304\n",
      "TellMeWhy: 71892\n",
      "MRPC: 1943\n",
      "ProofWriter: 585552\n",
      "MultiNLI: 382870\n",
      "Adversarial NLI: 2880\n",
      "ConTRoL: 3768\n",
      "RACE: 78867\n",
      "\n",
      "Counting examples with fewer than 500 tokens in the validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fade5f734b428b8a8496d5ec397f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in validation split with fewer than 500 tokens:\n",
      "AR-LSAT: 231\n",
      "ReClor: 499\n",
      "LogiQA 2.0: 1569\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 8976\n",
      "MRPC: 408\n",
      "ProofWriter: 85468\n",
      "MultiNLI: 19647\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 595\n",
      "RACE: 4428\n",
      "\n",
      "Counting examples with fewer than 500 tokens in the test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc720492b5744abb4b5f2d4addf0ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in test split with fewer than 500 tokens:\n",
      "AR-LSAT: 230\n",
      "ReClor: 500\n",
      "LogiQA 2.0: 1571\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 10689\n",
      "MRPC: 1725\n",
      "ProofWriter: 174476\n",
      "MultiNLI: 9832\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 415\n",
      "RACE: 4473\n"
     ]
    }
   ],
   "source": [
    "# Define the callable class for computing token lengths\n",
    "class ComputeTokenLengths:\n",
    "    def __init__(self, tokenizer_name):\n",
    "        self.tokenizer_name = pretrained_model_name\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        \n",
    "         # Concatenate context, question, and options into full texts\n",
    "        full_texts = [\n",
    "            f\"{ctx} {ques} {' '.join(opts)}\" for ctx, ques, opts in zip(\n",
    "                examples['Context'], examples['Question'], examples['Options'])\n",
    "        ]\n",
    "        # Tokenize\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            full_texts, truncation=False, add_special_tokens=True\n",
    "        )\n",
    "        # Get lengths\n",
    "        examples['token_length'] = [len(ids) for ids in tokenized_inputs['input_ids']]\n",
    "        return examples\n",
    "\n",
    "# Instantiate the class\n",
    "compute_token_lengths = ComputeTokenLengths(pretrained_model_name)\n",
    "\n",
    "# Function to compute the number of examples with fewer than 500 tokens by source\n",
    "def count_examples_fewer_than_500_tokens(dataset):\n",
    "    # Add token lengths to each example\n",
    "    dataset_with_lengths = dataset.map(\n",
    "        compute_token_lengths, batched=True, batch_size=1000, num_proc=1\n",
    "    )\n",
    "    count_by_source = defaultdict(int)\n",
    "    for example in dataset_with_lengths:\n",
    "        source = example['Source Dataset']\n",
    "        if example['token_length'] < 500:\n",
    "            count_by_source[source] += 1\n",
    "    return count_by_source\n",
    "\n",
    "# Process each split and count examples with fewer than 500 tokens\n",
    "splits = combined_dataset.keys()\n",
    "for split in splits:\n",
    "    print(f\"\\nCounting examples with fewer than 500 tokens in the {split} split...\")\n",
    "    count_under_500 = count_examples_fewer_than_500_tokens(combined_dataset[split])\n",
    "    print(f\"Number of examples in {split} split with fewer than 500 tokens:\")\n",
    "    for source, count in count_under_500.items():\n",
    "        print(f\"{source}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9832b0d4-0b5a-41e6-ae94-d55351427db1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering examples longer than 500 tokens in the train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce28bd3e9a7483d97c97da6d4e61428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1163327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering examples longer than 500 tokens in the validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43becf085c7c40ea93cd48342fafaf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering examples longer than 500 tokens in the test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66a496988d644e3aa1dc5672c9b42fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the callable class for computing and filtering token lengths\n",
    "class FilterTokenLengths:\n",
    "    def __init__(self, tokenizer_name):\n",
    "        self.tokenizer_name = pretrained_model_name\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        if self.tokenizer is None:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        \n",
    "         # Concatenate context, question, and options into full texts\n",
    "        full_texts = [\n",
    "            f\"{ctx} {ques} {' '.join(opts)}\" for ctx, ques, opts in zip(\n",
    "                examples['Context'], examples['Question'], examples['Options'])\n",
    "        ]\n",
    "        # Tokenize\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            full_texts, truncation=False, add_special_tokens=True\n",
    "        )\n",
    "        # Get lengths\n",
    "        token_lengths = [len(ids) for ids in tokenized_inputs['input_ids']]\n",
    "        # Filter examples longer than 500 tokens\n",
    "        return {\n",
    "            k: [v[i] for i in range(len(v)) if token_lengths[i] <= 500] for k, v in examples.items()\n",
    "        }\n",
    "\n",
    "# Instantiate the class\n",
    "filter_token_lengths = FilterTokenLengths(pretrained_model_name)\n",
    "\n",
    "# Define function to filter out long examples in each dataset split\n",
    "def filter_long_examples(dataset):\n",
    "    return dataset.map(\n",
    "        filter_token_lengths, batched=True, batch_size=1000, num_proc=1, load_from_cache_file=False\n",
    "    )\n",
    "\n",
    "# Process each split and remove examples with more than 500 tokens\n",
    "filtered_dataset = DatasetDict()\n",
    "splits = combined_dataset.keys()\n",
    "for split in splits:\n",
    "    print(f\"\\nFiltering examples longer than 500 tokens in the {split} split...\")\n",
    "    filtered_dataset[split] = filter_long_examples(combined_dataset[split])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45ca66fb-e278-427b-bc9a-ddebabf57e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples in each split:\n",
      "Train: 1151512\n",
      "Validation: 122956\n",
      "Test: 205044\n",
      "\n",
      "Total number of examples: 1479512\n",
      "\n",
      "Source Dataset counts in each split:\n",
      "\n",
      "Train split:\n",
      "AR-LSAT: 1585\n",
      "ReClor: 4138\n",
      "LogiQA 2.0: 12567\n",
      "RTE: 2213\n",
      "FOLIO: 798\n",
      "PrOntoQA: 2304\n",
      "TellMeWhy: 71892\n",
      "MRPC: 1943\n",
      "ProofWriter: 585552\n",
      "MultiNLI: 382870\n",
      "Adversarial NLI: 2880\n",
      "ConTRoL: 3772\n",
      "RACE: 78998\n",
      "\n",
      "Validation split:\n",
      "AR-LSAT: 231\n",
      "ReClor: 499\n",
      "LogiQA 2.0: 1569\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 8976\n",
      "MRPC: 408\n",
      "ProofWriter: 85468\n",
      "MultiNLI: 19647\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 595\n",
      "RACE: 4435\n",
      "\n",
      "Test split:\n",
      "AR-LSAT: 230\n",
      "ReClor: 500\n",
      "LogiQA 2.0: 1571\n",
      "RTE: 277\n",
      "FOLIO: 203\n",
      "PrOntoQA: 288\n",
      "TellMeWhy: 10689\n",
      "MRPC: 1725\n",
      "ProofWriter: 174476\n",
      "MultiNLI: 9832\n",
      "Adversarial NLI: 360\n",
      "ConTRoL: 415\n",
      "RACE: 4478\n",
      "\n",
      "Total counts per Source Dataset:\n",
      "AR-LSAT: 2046\n",
      "ReClor: 5137\n",
      "LogiQA 2.0: 15707\n",
      "RTE: 2767\n",
      "FOLIO: 1204\n",
      "PrOntoQA: 2880\n",
      "TellMeWhy: 91557\n",
      "MRPC: 4076\n",
      "ProofWriter: 845496\n",
      "MultiNLI: 412349\n",
      "Adversarial NLI: 3600\n",
      "ConTRoL: 4782\n",
      "RACE: 87911\n",
      "\n",
      "Examples from each Source Dataset:\n",
      "\n",
      "Source: RACE (from train split)\n",
      "Context: Last week I talked with some of my students about what they wanted to do after they graduated, and what kind of job prospects  they thought they had.\n",
      "Given that I teach students who are training to be doctors, I was surprised do find that most thought that they would not be able to get the jobs they wanted without \"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me that they would need a   or family friend to help them out.\n",
      "\"Surgery ,\" one replied.\n",
      "I was pretty alarmed by that response. It seems that the graduates of today are increasingly willing to go under the knife to get ahead of others when it comes to getting a job .\n",
      "One girl told me that she was considering surgery to increase her height. \"They break your legs, put in special extending screws, and slowly expand the gap between the two ends of the bone as it re-grows, you can get at least 5 cm taller!\"\n",
      "At that point, I was shocked. I am short, I can't deny that, but I don't think I would put myself through months of agony just to be a few centimetres taller. I don't even bother to wear shoes with thick soles, as I'm not trying to hide the fact that I am just not tall!\n",
      "It seems to me that there is a trend towards wanting \"perfection\" , and that is an ideal that just does not exist in reality.\n",
      "No one is born perfect, yet magazines, TV shows and movies present images of thin, tall, beautiful people as being the norm. Advertisements for slimming aids, beauty treatments and cosmetic surgery clinics fill the pages of newspapers, further creating an idea that \"perfection\" is a requirement, and that it must be purchased, no matter what the cost. In my opinion, skills, rather than appearance, should determine how successful a person is in his/her chosen career.\n",
      "\n",
      "Question: We can know from the passage that the author works as a_.\n",
      "\n",
      "Options: ['doctor', 'model', 'teacher', 'reporter']\n",
      "\n",
      "Label_Text: C\n",
      "\n",
      "Label_Index: 2\n",
      "\n",
      "\n",
      "Source: Adversarial NLI (from train split)\n",
      "Context: Boomerang (1992 film) . Boomerang was released in the United States on July 1 , 1992 .\n",
      "\n",
      "Question: Does the following statement classify as \"ENTAILMENT\", \"CONTRADICTION\", or \"NEUTRAL\"? The United States had Boomerang ( 1992 show ) released .\n",
      "\n",
      "Options: ['ENTAILMENT', 'CONTRADICTION', 'NEUTRAL']\n",
      "\n",
      "Label_Text: ENTAILMENT\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: RTE (from train split)\n",
      "Context: Civic Alliance, the umbrella group of civic groups involved in rebuilding, hosted a large, public, town-hall meeting for 5,000 people, called \"Listening to the City\" in July 2002.\n",
      "\n",
      "Question: Does the following statement classify as \"entailment\" or \"not entailment\"? The Civic Alliance is an umbrella group.\n",
      "\n",
      "Options: ['entailment', 'not entailment']\n",
      "\n",
      "Label_Text: entailment\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: PrOntoQA (from train split)\n",
      "Context: Dumpuses are bright. Dumpuses are wumpuses. Wumpuses are not opaque. Each wumpus is a numpus. Every numpus is angry. Each numpus is a jompus. Jompuses are not hot. Jompuses are rompuses. Every rompus is spicy. Rompuses are yumpuses. Yumpuses are nervous. Every yumpus is a tumpus. Zumpuses are not bright. Tumpuses are earthy. Tumpuses are impuses. Sam is a dumpus.\n",
      "\n",
      "Question: Is the following statement \"True\" or \"False\"? Sam is bright.\n",
      "\n",
      "Options: ['True', 'False']\n",
      "\n",
      "Label_Text: True\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: TellMeWhy (from train split)\n",
      "Context: Cam ordered a pizza and took it home. He opened the box to take out a slice. Cam discovered that the store did not cut the pizza for him. He looked for his pizza cutter but did not find it. He had to use his chef knife to cut a slice.\n",
      "\n",
      "Question: Does the following question classify as \"Answerable\" or \"Not Answerable\"? Why did Cam order a pizza?\n",
      "\n",
      "Options: ['Answerable', 'Not Answerable']\n",
      "\n",
      "Label_Text: Not Answerable\n",
      "\n",
      "Label_Index: 1\n",
      "\n",
      "\n",
      "Source: AR-LSAT (from train split)\n",
      "Context: Exactly six trade representatives negotiate a treaty: Klosnik, Londi, Manley, Neri, Osata, Poirier. There are exactly six chairs evenly spaced around a circular table. The chairs are numbered 1 through 6, with successively numbered chairs next to each other and chair number 1 next to chair number 6. Each chair is occupied by exactly one of the representatives. The following conditions apply: Poirier sits immediately next to Neri. Londi sits immediately next to Manley, Neri, or both. Klosnik does not sit immediately next to Manley. If Osata sits immediately next to Poirier, Osata does not sit immediately next to Manley.\n",
      "\n",
      "Question: Which one of the following seating arrangements of the six representatives in chairs 1 through 6 would NOT violate the stated conditions?\n",
      "\n",
      "Options: ['Klosnik, Poirier, Neri, Manley, Osata, Londi', 'Klosnik, Londi, Manley, Poirier, Neri, Osata', 'Klosnik, Londi, Manley, Osata, Poirier, Neri', 'Klosnik, Osata, Poirier, Neri, Londi, Manley', 'Klosnik, Neri, Londi, Osata, Manley, Poirier']\n",
      "\n",
      "Label_Text: Klosnik, Londi, Manley, Poirier, Neri, Osata\n",
      "\n",
      "Label_Index: 1\n",
      "\n",
      "\n",
      "Source: LogiQA 2.0 (from train split)\n",
      "Context: A college will continue to implement the overseas funding plan this year. It plans to select several of the six teachers from Mr. Liu, Mr. Zhang, Mr. Wang, Mr. Ma, Mr. Niu and Mr. Zhou to visit abroad. Due to the limitations of funding, the needs of discipline development, curriculum arrangement, place and time of each student's visit, the selection shall meet the following conditions: (1) Mr. Liu is the reserve discipline leader of the college, This time we have to send out. (2) if we choose Mr. Liu, we should also choose Mr. Zhou, but we can't choose Mr. Zhang. (3) only if Mr. Niu can't choose, at least one of Mr. Wang and Mr. Ma can choose. (4) if we don't choose Mr. Wang, we don't choose Mr. Zhou either.\n",
      "\n",
      "Question: If the above statement is true, which of the followings must be true?\n",
      "\n",
      "Options: [\"Mr. Niu didn't choose, but Mr. Zhou did\", \"Mr. Liu was chose, but Mr. Ma didn't\", 'Mr. Wang and Mr. Ma were chosen', 'Neither Mr. Wang nor Mr. Niu was elected']\n",
      "\n",
      "Label_Text: Mr. Niu didn't choose, but Mr. Zhou did\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: FOLIO (from train split)\n",
      "Context: All humans are capable of abstract thoughts.\n",
      "Plants are not capable of abstract thoughts.\n",
      "All multicellular creatures that are autotrophic or digest food internally are plants and animals.\n",
      "All goats are animals.\n",
      "Dirt is not an animal.\n",
      "Hulu is a goat or a human.\n",
      "Hulu is a multicellular creature that is autotrophic or digests food internally. \n",
      "\n",
      "Question: Does the following conclusion classify as \"True\", \"False\" or \"Uncertain\"? Hulu is not capable of abstract thoughts.\n",
      "\n",
      "Options: ['True', 'False', 'Uncertain']\n",
      "\n",
      "Label_Text: Uncertain\n",
      "\n",
      "Label_Index: 2\n",
      "\n",
      "\n",
      "Source: MRPC (from train split)\n",
      "Context: \" It 's safe to assume that the Senate is prepared to pass some form of cap , \" King said .\n",
      "\n",
      "Question: Does the following statement classify as \"not equivalent\" or \"equivalent\"? Its safe to assume the Senate is prepared to pass some form of a cap .... The level of it is to be debated .\n",
      "\n",
      "Options: ['not equivalent', 'equivalent']\n",
      "\n",
      "Label_Text: not equivalent\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: MultiNLI (from train split)\n",
      "Context: Sunset Plaza (8600-8700 Sunset Boulevard at Sunset Plaza Drive) has been an elite shopping area since 1934.\n",
      "\n",
      "Question: Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? Sunset Plaza has been an exclusive shopping plaza for ages.\n",
      "\n",
      "Options: ['entailment', 'neutral', 'contradiction']\n",
      "\n",
      "Label_Text: entailment\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: ProofWriter (from train split)\n",
      "Context: Gary is furry. Gary is nice. Gary is red. Gary is rough. Gary is not smart. Gary is white. Gary is young. If Gary is nice and Gary is not white then Gary is red. If someone is white then they are red. All young people are furry. If someone is white and not red then they are furry. Smart, red people are rough. If Gary is not red and Gary is not furry then Gary is not smart. If Gary is white then Gary is not smart. If someone is rough and not white then they are not smart.\n",
      "\n",
      "Question: Is the following statement \"True\", \"False\" or \"Unknown\"? Gary is white.\n",
      "\n",
      "Options: ['True', 'False', 'Unknown']\n",
      "\n",
      "Label_Text: True\n",
      "\n",
      "Label_Index: 0\n",
      "\n",
      "\n",
      "Source: ConTRoL (from train split)\n",
      "Context: A 13-year-old boy, Gareth Jones, was taken to Downston Police Station on Saturday 11 June under the suspicion of shoplifting in a local superstore. Gareth Jones denies all of the charges made against him. It is also known that: Gareth is an orphan. The store security officer has a grudge against Gareth because he is the best friend of his son. Gareth is not shown on video recordings captured by the stores surveillance cameras. Two years ago Gareth was caught stealing police road traffic bollards. The store was extremely busy on Saturday 11 June. Gareth had not been given a receipt for the goods he had bought. Gareth was stopped after he had left the store.\n",
      "\n",
      "Question: Does the following statement classify as \"neutral,\" \"entailment,\" or \"contradiction\"? The stores security officer had a motive for accusing Gareth of shoplifting.\n",
      "\n",
      "Options: ['neutral', 'entailment', 'contradiction']\n",
      "\n",
      "Label_Text: entailment\n",
      "\n",
      "Label_Index: 1\n",
      "\n",
      "\n",
      "Source: ReClor (from train split)\n",
      "Context: Two years ago, the human resources department at Harrison' s Technologies had 175 sexual harassment cases open, a number that at that time led all companies in the sector. At that time, a typical claim took six months to process and clear. In response to this high number, the CEO' s Advisory Team published a comprehensive set of guidelines for training employees in the identification and avoidance of behaviors that could constitute sexual harassment. At this writing, the human resources department has only sixty sexual harassment cases open. Clearly, the guidelines published by the CEO' s Advisory Team were effective in reducing the occurrence of new sexual harassment claims at Harrison' s Technologies.\n",
      "\n",
      "Question: Which of the following, if true, most seriously weakens the conclusion above?\n",
      "\n",
      "Options: [\"The human resources department at Anthony Information Industries, a principal rival of Harrison's with a similar number of employees, opened no new sexual harassment claims in the past year.\", \"New paperwork and procedure, also introduced two years ago, has allowed the human resources department at Harrison's Technologies to process and close almost all new claims of sexual harassment in less than one month.\", 'The majority of sexual harassment claims lead to the termination of employees found guilty; new employees hired to fill these positions need to be trained in the guidelines.', \"Independent consultants using scientifically designed surveys have found that the majority of current employees at Harrison's Technologies are not properly educated on exactly what behaviors constitute sexual harassment.\"]\n",
      "\n",
      "Label_Text: New paperwork and procedure, also introduced two years ago, has allowed the human resources department at Harrison's Technologies to process and close almost all new claims of sexual harassment in less than one month.\n",
      "\n",
      "Label_Index: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the splits\n",
    "splits = filtered_dataset.keys()\n",
    "\n",
    "# Print number of examples in each split\n",
    "print(\"Number of examples in each split:\")\n",
    "for split in splits:\n",
    "    num_examples = len(filtered_dataset[split])\n",
    "    print(f\"{split.capitalize()}: {num_examples}\")\n",
    "\n",
    "# Calculate total number of examples\n",
    "total_examples = sum(len(filtered_dataset[split]) for split in splits)\n",
    "print(f\"\\nTotal number of examples: {total_examples}\")\n",
    "\n",
    "# Print source dataset statistics for each split\n",
    "print(\"\\nSource Dataset counts in each split:\")\n",
    "for split in splits:\n",
    "    source_counter = Counter(filtered_dataset[split]['Source Dataset'])\n",
    "    print(f\"\\n{split.capitalize()} split:\")\n",
    "    for source, count in source_counter.items():\n",
    "        print(f\"{source}: {count}\")\n",
    "\n",
    "# Print total counts per source dataset across all splits\n",
    "total_source_counter = Counter()\n",
    "for split in splits:\n",
    "    total_source_counter.update(filtered_dataset[split]['Source Dataset'])\n",
    "\n",
    "print(\"\\nTotal counts per Source Dataset:\")\n",
    "for source, count in total_source_counter.items():\n",
    "    print(f\"{source}: {count}\")\n",
    "\n",
    "# Get all unique source datasets\n",
    "all_sources = set(total_source_counter.keys())\n",
    "\n",
    "# Display an example from each source dataset\n",
    "print(\"\\nExamples from each Source Dataset:\")\n",
    "for source in all_sources:\n",
    "    found = False\n",
    "    for split in splits:\n",
    "        # Get the dataset split\n",
    "        ds = filtered_dataset[split]\n",
    "        # Filter to examples from this source\n",
    "        indices = [i for i, s in enumerate(ds['Source Dataset']) if s == source]\n",
    "        if indices:\n",
    "            idx = indices[0]\n",
    "            example = ds[idx]\n",
    "            print(f\"\\nSource: {source} (from {split} split)\")\n",
    "            print(f\"Context: {example['Context']}\\n\")\n",
    "            print(f\"Question: {example['Question']}\\n\")\n",
    "            print(f\"Options: {example['Options']}\\n\")\n",
    "            print(f\"Label_Text: {example['Label_Text']}\\n\")\n",
    "            print(f\"Label_Index: {example['Label']}\\n\")\n",
    "            found = True\n",
    "            break  # Move to the next source\n",
    "    if not found:\n",
    "        print(f\"No example found for source {source}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f314214-9f69-441b-9d17-4d3c4208f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b98775ceaad42729081676c33873312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/1151512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token lengths in Train split:\n",
      "AR-LSAT: 367\n",
      "ReClor: 393\n",
      "LogiQA 2.0: 496\n",
      "RTE: 270\n",
      "FOLIO: 290\n",
      "PrOntoQA: 158\n",
      "TellMeWhy: 120\n",
      "MRPC: 113\n",
      "ProofWriter: 284\n",
      "MultiNLI: 464\n",
      "Adversarial NLI: 446\n",
      "ConTRoL: 500\n",
      "RACE: 500\n",
      "\n",
      "Processing validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40d1eaa0104467dab50074a630bb9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/122956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token lengths in Validation split:\n",
      "AR-LSAT: 414\n",
      "ReClor: 366\n",
      "LogiQA 2.0: 447\n",
      "RTE: 265\n",
      "FOLIO: 240\n",
      "PrOntoQA: 155\n",
      "TellMeWhy: 113\n",
      "MRPC: 98\n",
      "ProofWriter: 276\n",
      "MultiNLI: 257\n",
      "Adversarial NLI: 333\n",
      "ConTRoL: 403\n",
      "RACE: 500\n",
      "\n",
      "Processing test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a481217afa7d4fa59fa6e6398268e84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/205044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token lengths in Test split:\n",
      "AR-LSAT: 421\n",
      "ReClor: 363\n",
      "LogiQA 2.0: 390\n",
      "RTE: 308\n",
      "FOLIO: 232\n",
      "PrOntoQA: 158\n",
      "TellMeWhy: 118\n",
      "MRPC: 119\n",
      "ProofWriter: 284\n",
      "MultiNLI: 345\n",
      "Adversarial NLI: 333\n",
      "ConTRoL: 478\n",
      "RACE: 500\n",
      "\n",
      "Global maximum token lengths across all splits:\n",
      "AR-LSAT: 421\n",
      "ReClor: 393\n",
      "LogiQA 2.0: 496\n",
      "RTE: 308\n",
      "FOLIO: 290\n",
      "PrOntoQA: 158\n",
      "TellMeWhy: 120\n",
      "MRPC: 119\n",
      "ProofWriter: 284\n",
      "MultiNLI: 464\n",
      "Adversarial NLI: 446\n",
      "ConTRoL: 500\n",
      "RACE: 500\n"
     ]
    }
   ],
   "source": [
    "# Define the callable class\n",
    "class ComputeTokenLengths:\n",
    "    def __init__(self, tokenizer_name):\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        if self.tokenizer is None:\n",
    "            from transformers import AutoTokenizer\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        # Handle options\n",
    "        if isinstance(examples['Options'][0], list):\n",
    "            options_texts = ['\\n'.join(opts) for opts in examples['Options']]\n",
    "        else:\n",
    "            options_texts = examples['Options']\n",
    "        # Concatenate texts\n",
    "        full_texts = [\n",
    "            f\"{ctx} {ques} {opts}\"\n",
    "            for ctx, ques, opts in zip(examples['Context'], examples['Question'], options_texts)\n",
    "        ]\n",
    "        # Tokenize\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            full_texts, truncation=False, add_special_tokens=True\n",
    "        )\n",
    "        # Get lengths\n",
    "        examples['token_length'] = [len(ids) for ids in tokenized_inputs['input_ids']]\n",
    "        return examples\n",
    "\n",
    "# Instantiate the class\n",
    "compute_token_lengths = ComputeTokenLengths(pretrained_model_name)\n",
    "\n",
    "# Function to compute max token lengths by source\n",
    "def max_token_length_by_source(dataset):\n",
    "    dataset_with_lengths = dataset.map(\n",
    "        compute_token_lengths, batched=True, batch_size=1000, num_proc=6\n",
    "    )\n",
    "    source_max_lengths = defaultdict(int)\n",
    "    for example in dataset_with_lengths:\n",
    "        source = example['Source Dataset']\n",
    "        length = example['token_length']\n",
    "        if length > source_max_lengths[source]:\n",
    "            source_max_lengths[source] = length\n",
    "    return source_max_lengths\n",
    "\n",
    "# Now, process your splits\n",
    "splits = filtered_dataset.keys()\n",
    "global_max_lengths = defaultdict(int)\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nProcessing {split} split...\")\n",
    "    max_lengths = max_token_length_by_source(filtered_dataset[split])\n",
    "    print(f\"Max token lengths in {split.capitalize()} split:\")\n",
    "    for source, max_len in max_lengths.items():\n",
    "        print(f\"{source}: {max_len}\")\n",
    "        if max_len > global_max_lengths[source]:\n",
    "            global_max_lengths[source] = max_len\n",
    "\n",
    "print(\"\\nGlobal maximum token lengths across all splits:\")\n",
    "for source, max_len in global_max_lengths.items():\n",
    "    print(f\"{source}: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ffac430-e403-4bbb-8cb1-4e2299273a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29f7015cc344763ad92ce7398667437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1151512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e228435695145779867783e20551d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/122956 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b54249927b4ed2bdd118f340e71111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/205044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the filtered dataset\n",
    "filtered_dataset.save_to_disk('C:/cleaned_dataset-2', max_shard_size=\"1GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb9f12-d022-4590-af99-611fc317aeaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# End of NoteBook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
