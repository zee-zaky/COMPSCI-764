{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71614c8c-099c-4c44-91f3-5cd0ea933243",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Imports, libraries and rusable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebea513d-21f8-4f2a-b64e-e52e6acbed93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project libraries imported!\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER is available.\n",
      "Device:cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from project_imports import *\n",
    "import use_gpu\n",
    "# Clear any cached memory to start fresh for each trial\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b95e38-f23f-4972-a8cb-b4c1849cb726",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb17c692-f274-4f42-8985-1b42f4f1cfd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Arguments and global vriables\n",
    "dataset_name=\"MCQA-Combined-3\"\n",
    "global_run_name=\"Optuna-1\"\n",
    "pretrained_model_name = \"microsoft/deberta-v3-base\"\n",
    "normalized_model_name = pretrained_model_name.replace(\"/\", \"-\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "assert isinstance( tokenizer, PreTrainedTokenizerFast )\n",
    "data_collator = DefaultDataCollator()\n",
    "max_length = 512 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "pad_on_right = right_padding = tokenizer.padding_side == 'right'\n",
    "global_counter = 0\n",
    "traing_answer_mismatches = []\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff5d0e7-f055-4058-986f-f82edc6fce8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Prepare the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38eef2e-4a62-434a-a3df-31a558fc1f42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 1072514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 118521\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Context', 'Question', 'Options', 'Label_Text', 'Label', 'Type', 'Source Dataset'],\n",
       "        num_rows: 200566\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "combined_dataset = load_from_disk('cleaned_dataset')\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b14e4b-8c9a-4cf4-817e-e8b7099bf1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize lists to hold datasets\n",
    "train_datasets = []\n",
    "val_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "# List of datasets to combine\n",
    "datasets_to_combine = ['AR-LSAT', 'ReClor', 'LogiQA 2.0']\n",
    "\n",
    "# Loop through each dataset and filter\n",
    "for ds_name in datasets_to_combine:\n",
    "    train_datasets.append(combined_dataset['train'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "    val_datasets.append(combined_dataset['validation'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "    test_datasets.append(combined_dataset['test'].filter(lambda x: x['Source Dataset'] == ds_name))\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_train = concatenate_datasets(train_datasets)\n",
    "combined_val = concatenate_datasets(val_datasets)\n",
    "combined_test = concatenate_datasets(test_datasets)\n",
    "\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "# To ensure that each training batch has a chance to contain a mix of examples from all sources. \n",
    "# This helps in reducing variance and improving the generalization of the model.\n",
    "combined_train = combined_train.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944988bb-97a9-4daf-80a6-743daea9b62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mcqa_preprocess_function(examples):\n",
    "    # Determine the maximum number of choices\n",
    "    max_num_choices = 5  # Since AR-LSAT has 5 options, we'll pad others to 5\n",
    "    contexts = examples['Context']\n",
    "    questions = examples['Question']\n",
    "    options_list = examples['Options']\n",
    "    labels = examples['Label']\n",
    "    \n",
    "    first_sentences = []\n",
    "    second_sentences = []\n",
    "    labels_adjusted = []\n",
    "    \n",
    "    for context, question, options, label in zip(contexts, questions, options_list, labels):\n",
    "        num_choices = len(options)\n",
    "        # Pad options to have max_num_choices\n",
    "        if num_choices < max_num_choices:\n",
    "            options += [''] * (max_num_choices - num_choices)\n",
    "        first_sentences.append([context] * max_num_choices)\n",
    "        second_sentences.append([f\"{question} {option}\" for option in options])\n",
    "        labels_adjusted.append(label)\n",
    "    \n",
    "    # Flatten the lists\n",
    "    first_sentences = [item for sublist in first_sentences for item in sublist]\n",
    "    second_sentences = [item for sublist in second_sentences for item in sublist]\n",
    "    \n",
    "    # Tokenize the inputs\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences,\n",
    "        second_sentences,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "    )\n",
    "    \n",
    "    # Un-flatten to shape (num_examples, max_num_choices, seq_length)\n",
    "    tokenized_inputs = {\n",
    "        k: [v[i:i + max_num_choices] for i in range(0, len(v), max_num_choices)]\n",
    "        for k, v in tokenized_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Labels\n",
    "    tokenized_inputs[\"labels\"] = labels_adjusted\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29057a47-41c8-4073-ad1c-6e08a04ed14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the combined datasets\n",
    "encoded_train = combined_train.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_val = combined_val.map(mcqa_preprocess_function, batched=True)\n",
    "encoded_test = combined_test.map(mcqa_preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27035bf4-f8bd-4e10-8b8e-80e6c33cb9ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 18290\n",
      "Number of validation examples: 2299\n",
      "Number of test examples: 2301\n"
     ]
    }
   ],
   "source": [
    "# Set the format of the datasets to PyTorch tensors\n",
    "encoded_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "encoded_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "def get_train_encoded():\n",
    "    return encoded_train\n",
    "\n",
    "def get_val_encoded():\n",
    "    return encoded_val\n",
    "\n",
    "def get_test_encoded():\n",
    "    return encoded_test\n",
    "\n",
    "print(\"Number of training examples:\", len(encoded_train))\n",
    "print(\"Number of validation examples:\", len(encoded_val))\n",
    "print(\"Number of test examples:\", len(encoded_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba1af4-7b57-4c74-bcd9-48edefba6747",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8384ce-d55e-4aa1-911b-7d8ac3ab07df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the accuracy metric\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {'eval_accuracy': acc, 'eval_f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62ae25b-b560-4182-a68d-e60ab612bc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "def create_training_args(run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=3):\n",
    "    \"\"\"\n",
    "    Generates training arguments for training a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_name (str): The name of the dataset.\n",
    "    - run_name (str): The name of the run, useful for logging and saving models.\n",
    "    - model_name (str): The name of the model, typically including its configuration.\n",
    "    - num_train_epochs (int): The number of epochs to train for.\n",
    "    - learning_rate (float): The learning rate for training.\n",
    "    - batch_size (int): The batch size used for training.\n",
    "\n",
    "    Returns:\n",
    "    - TrainingArguments: A configured TrainingArguments instance.\n",
    "    \"\"\"    \n",
    "    output_dir = f\"./{dataset_name}/{run_name}/{normalized_model_name}\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=398,\n",
    "        weight_decay=0.194,\n",
    "        adam_beta1=0.837,\n",
    "        adam_beta2=0.997,\n",
    "        adam_epsilon=5.87e-07,\n",
    "        lr_scheduler_type='cosine',\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    )\n",
    "    \n",
    "    return training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db0f0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name=pretrained_model_name):\n",
    "    return AutoModelForMultipleChoice.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fbc498-f422-4980-9a8c-bbcf0852c992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trainer(model_name=pretrained_model_name,run_name=\"Default-Run\", num_train_epochs=3, learning_rate=4.92e-05, batch_size=4):\n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name),\n",
    "        args=create_training_args(run_name=run_name, num_train_epochs=num_train_epochs, learning_rate=learning_rate, batch_size=batch_size),\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af7e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedEarlyStoppingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A callback to stop training when either the performance falls below a certain threshold\n",
    "    or if there is no improvement over a set number of epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, metric_name, patience):\n",
    "        self.metric_name = metric_name\n",
    "        self.patience = patience        \n",
    "        self.best_score = None\n",
    "        self.no_improve_epochs = 0\n",
    "        self.config_file = \"early_stopping_config.json\"  # Config file for early stopping values\n",
    "\n",
    "    def read_early_stopping_config(self):\n",
    "        \"\"\"\n",
    "        Reads the early stopping configuration from the file system.\n",
    "        Returns the configuration as a dictionary.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.config_file):\n",
    "            with open(self.config_file, 'r') as file:\n",
    "                config = json.load(file)\n",
    "            return config\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Config file not found: {self.config_file}\")\n",
    "    def reset_manual_stop_flag(self):\n",
    "        \"\"\"\n",
    "        Resets the manual stop flag to False in the early stopping config file.\n",
    "        \"\"\"\n",
    "        config = self.read_early_stopping_config()\n",
    "        config['manual_stop'] = False\n",
    "        with open(self.config_file, 'w') as file:\n",
    "            json.dump(config, file, indent=4)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric_value = kwargs['metrics'].get(self.metric_name)\n",
    "\n",
    "        if self.best_score is None or metric_value > self.best_score:\n",
    "            self.best_score = metric_value\n",
    "            self.no_improve_epochs = 0\n",
    "        else:\n",
    "            self.no_improve_epochs += 1\n",
    "\n",
    "        # Check if no improvement has been seen over the allowed patience\n",
    "        if self.no_improve_epochs >= self.patience:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: No improvement in {self.metric_name} for {self.patience} epochs\")\n",
    "\n",
    "\n",
    "        # Read the early stopping configuration\n",
    "        config = self.read_early_stopping_config()\n",
    "        min_accuracy = config.get(\"min_accuracy\", 0.35)                \n",
    "        num_epochs_min_acc = config.get(\"num_epochs_min_acc\", 2)  \n",
    "        max_variance = config.get(\"max_variance\", 0.2)  \n",
    "\n",
    "        # Check if performance is below the threshold\n",
    "        if metric_value < min_accuracy:\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Stopping training: {self.metric_name} below manual min_acc of {min_accuracy}\")\n",
    "\n",
    "         # Manual stop from config\n",
    "        if config.get(\"manual_stop\", False):\n",
    "            control.should_training_stop = True\n",
    "            print(f\"Manual early stopping triggered!!\")\n",
    "            self.reset_manual_stop_flag()  # Reset the flag for future runs\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a456ce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Clear any cached memory to start fresh for each trial\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "    model_name = trial.suggest_categorical('model_name', [pretrained_model_name, \"./squad-trained-model\", './MCQA-Combined/Optuna/trial_5/checkpoint-22865', './MCQA-Combined/Optuna/trial_0/checkpoint-30485','./MCQA-Combined/Optuna/trial_6/checkpoint-30485' ]) \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-7, 1e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [3, 4])\n",
    "    #warmup_steps = trial.suggest_int('warmup_steps', 0, 1000)\n",
    "    warmup_ratio= trial.suggest_float('warmup_ratio', 0.0, 1.0)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 0.0, 0.25)\n",
    "    adam_beta1 = trial.suggest_float('adam_beta1', 0.8, 0.95)\n",
    "    adam_beta2 = trial.suggest_float('adam_beta2', 0.990, 0.999)\n",
    "    adam_epsilon = trial.suggest_float('adam_epsilon', 1e-8, 1e-6)\n",
    "    lr_scheduler_type = trial.suggest_categorical('lr_scheduler_type', ['linear', 'cosine', 'cosine_with_restarts']) #,'constant_with_warmup'   \n",
    "    \n",
    "\n",
    "    output_dir = f\"./{dataset_name}/{global_run_name}/trial_{trial.number}\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"none\",  # Disable all integrations\n",
    "        overwrite_output_dir=True,\n",
    "        metric_for_best_model='eval_accuracy',\n",
    "        greater_is_better=True,\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=3,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=30,\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        weight_decay=weight_decay,\n",
    "        adam_beta1=adam_beta1,\n",
    "        adam_beta2=adam_beta2,\n",
    "        adam_epsilon=adam_epsilon,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        fp16=True,  # Enable mixed-precision training\n",
    "    ) \n",
    "    \n",
    "    # Print trial parameters\n",
    "    print(f\"Current Trial {trial.number} parameters: {trial.params}\")\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_init(model_name),\n",
    "        args=training_args,\n",
    "        train_dataset=get_train_encoded(),\n",
    "        eval_dataset=get_val_encoded(),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[AdvancedEarlyStoppingCallback(metric_name='eval_accuracy', patience=1)]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate()\n",
    "        \n",
    "    torch.cuda.empty_cache()  # Clear cache after evaluation\n",
    "    gc.collect()  # Collect garbage\n",
    "\n",
    "    return eval_results['eval_accuracy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9b86e-63c3-4ee8-bb91-bbb43e5a6481",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Fine-tuning DeBERTa on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5960c1-4b7d-4871-9029-bb724849439c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.1 Evaluate Vanilla DeBERTa (Acc=19.30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920052b1-e4d5-45c3-bb80-a471f35d5c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71605b3ebdc64244b3b643b030479e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96f4d5-67f3-4d84-b110-42b283a66ce7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.2 Evaluate Trained Vanilla DeBERTa (Acc=27.92%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcbb144a-c79b-4747-99c9-efd8f6131fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13719' max='13719' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13719/13719 3:00:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.464600</td>\n",
       "      <td>1.456540</td>\n",
       "      <td>0.256198</td>\n",
       "      <td>0.226952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.468400</td>\n",
       "      <td>1.446175</td>\n",
       "      <td>0.253589</td>\n",
       "      <td>0.234731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.418800</td>\n",
       "      <td>1.430362</td>\n",
       "      <td>0.300565</td>\n",
       "      <td>0.301469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2301' max='2301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2301/2301 01:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.2786\n",
      "F1 Score: 0.2792\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = create_trainer()\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d243e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5e2d485294447ab95c484c7c12dc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241016_110040-4ti0dzo9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/4ti0dzo9' target=\"_blank\">./MCQA-Combined/Default-Run2/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/4ti0dzo9' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/4ti0dzo9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8153f7207c0a432b96e2b626290a5958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4817, 'grad_norm': 10.460925102233887, 'learning_rate': 4.919968223047422e-05, 'epoch': 0.08}\n",
      "{'loss': 1.5081, 'grad_norm': 3.530348062515259, 'learning_rate': 4.918816880581236e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4864, 'grad_norm': 6.0712995529174805, 'learning_rate': 4.916019321314551e-05, 'epoch': 0.25}\n",
      "{'loss': 1.5478, 'grad_norm': 4.789742469787598, 'learning_rate': 4.911576767182646e-05, 'epoch': 0.33}\n",
      "{'loss': 1.4785, 'grad_norm': 26.84292984008789, 'learning_rate': 4.905478354129259e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5623, 'grad_norm': 3.22577166557312, 'learning_rate': 4.897752559048373e-05, 'epoch': 0.49}\n",
      "{'loss': 1.6182, 'grad_norm': 2.662576198577881, 'learning_rate': 4.8883736139178334e-05, 'epoch': 0.57}\n",
      "{'loss': 1.6135, 'grad_norm': 2.9794094562530518, 'learning_rate': 4.877361665371831e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6136, 'grad_norm': 1.978592872619629, 'learning_rate': 4.864724118592782e-05, 'epoch': 0.74}\n",
      "{'loss': 1.6108, 'grad_norm': 2.1126723289489746, 'learning_rate': 4.8504694719258075e-05, 'epoch': 0.82}\n",
      "{'loss': 1.6108, 'grad_norm': 2.08780837059021, 'learning_rate': 4.834607311163868e-05, 'epoch': 0.9}\n",
      "{'loss': 1.6119, 'grad_norm': 2.161538600921631, 'learning_rate': 4.817148303101623e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85739653650745d296b1798108253891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.2122662026968247, 'eval_f1': 0.2045627686513225, 'eval_loss': 1.609375, 'eval_runtime': 82.643, 'eval_samples_per_second': 27.818, 'eval_steps_per_second': 6.958, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m create_trainer(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault-Run2\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m      6\u001b[0m test_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mget_test_encoded())\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = create_trainer(run_name=\"Default-Run2\", num_train_epochs=10, batch_size=3)\n",
    "# Train the model\n",
    "trainer.train()\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362fbe0",
   "metadata": {},
   "source": [
    "## 4.3 Optuna Hyperparameters Tuning 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03dcd721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-16 15:03:56,781] A new study created in memory with name: no-name-0ded4bce-c54e-4ab6-8334-5b865ddd797e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 0 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241016_150404-p88p3ozd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/p88p3ozd' target=\"_blank\">./MCQA-Combined/Optuna/trial_0</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/p88p3ozd' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/p88p3ozd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5609648fc10b4b5ca2cc9eeb8e57cbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 1.8310006856918335, 'learning_rate': 2.711832744552112e-07, 'epoch': 0.08}\n",
      "{'loss': 1.6049, 'grad_norm': 2.0155208110809326, 'learning_rate': 5.423665489104224e-07, 'epoch': 0.16}\n",
      "{'loss': 1.5479, 'grad_norm': 6.674936771392822, 'learning_rate': 8.124650902678128e-07, 'epoch': 0.25}\n",
      "{'loss': 1.4779, 'grad_norm': 11.677717208862305, 'learning_rate': 1.0825636316252032e-06, 'epoch': 0.33}\n",
      "{'loss': 1.4543, 'grad_norm': 4.926329612731934, 'learning_rate': 1.3537469060804143e-06, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:02:16.440282, resuming normal operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4366, 'grad_norm': 13.124654769897461, 'learning_rate': 1.6249301805356256e-06, 'epoch': 0.49}\n",
      "{'loss': 1.4318, 'grad_norm': 12.793344497680664, 'learning_rate': 1.896113454990837e-06, 'epoch': 0.57}\n",
      "{'loss': 1.411, 'grad_norm': 15.144495964050293, 'learning_rate': 2.1667543628971375e-06, 'epoch': 0.66}\n",
      "{'loss': 1.3965, 'grad_norm': 22.932546615600586, 'learning_rate': 2.437937637352349e-06, 'epoch': 0.74}\n",
      "{'loss': 1.3741, 'grad_norm': 10.780708312988281, 'learning_rate': 2.7091209118075597e-06, 'epoch': 0.82}\n",
      "{'loss': 1.3568, 'grad_norm': 19.797386169433594, 'learning_rate': 2.980304186262771e-06, 'epoch': 0.9}\n",
      "{'loss': 1.3453, 'grad_norm': inf, 'learning_rate': 3.250945094169072e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1ba3d9db8b47b2b5a6184fedf6b5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.41365811222270554, 'eval_f1': 0.41452016409540837, 'eval_loss': 1.2694002389907837, 'eval_runtime': 85.7551, 'eval_samples_per_second': 26.809, 'eval_steps_per_second': 8.944, 'epoch': 1.0}\n",
      "{'loss': 1.31, 'grad_norm': 9.396753311157227, 'learning_rate': 3.5221283686242833e-06, 'epoch': 1.07}\n",
      "{'loss': 1.3161, 'grad_norm': 28.072525024414062, 'learning_rate': 3.7933116430794946e-06, 'epoch': 1.15}\n",
      "{'loss': 1.3015, 'grad_norm': 12.580765724182129, 'learning_rate': 4.0644949175347055e-06, 'epoch': 1.23}\n",
      "{'loss': 1.2701, 'grad_norm': 19.165576934814453, 'learning_rate': 4.335678191989917e-06, 'epoch': 1.31}\n",
      "{'loss': 1.29, 'grad_norm': 48.18876266479492, 'learning_rate': 4.6057767333473074e-06, 'epoch': 1.39}\n",
      "{'loss': 1.2402, 'grad_norm': 15.27053451538086, 'learning_rate': 4.876960007802518e-06, 'epoch': 1.48}\n",
      "{'loss': 1.2358, 'grad_norm': 27.17981719970703, 'learning_rate': 5.148143282257729e-06, 'epoch': 1.56}\n",
      "{'loss': 1.2313, 'grad_norm': 37.995967864990234, 'learning_rate': 5.419326556712941e-06, 'epoch': 1.64}\n",
      "{'loss': 1.2268, 'grad_norm': 39.485984802246094, 'learning_rate': 5.689967464619242e-06, 'epoch': 1.72}\n",
      "{'loss': 1.1985, 'grad_norm': 58.72370529174805, 'learning_rate': 5.961150739074453e-06, 'epoch': 1.8}\n",
      "{'loss': 1.2284, 'grad_norm': 40.23031234741211, 'learning_rate': 6.232334013529665e-06, 'epoch': 1.89}\n",
      "{'loss': 1.2233, 'grad_norm': 11.964823722839355, 'learning_rate': 6.503517287984876e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4902f6d9dfa453abd546017b6177c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.512396694214876, 'eval_f1': 0.5141582818892122, 'eval_loss': 1.1319924592971802, 'eval_runtime': 85.532, 'eval_samples_per_second': 26.879, 'eval_steps_per_second': 8.967, 'epoch': 2.0}\n",
      "{'loss': 1.0905, 'grad_norm': 37.540035247802734, 'learning_rate': 6.7741581958911764e-06, 'epoch': 2.05}\n",
      "{'loss': 1.0627, 'grad_norm': 57.054805755615234, 'learning_rate': 7.045341470346387e-06, 'epoch': 2.13}\n",
      "{'loss': 1.078, 'grad_norm': 88.00515747070312, 'learning_rate': 7.316524744801599e-06, 'epoch': 2.21}\n",
      "{'loss': 1.0316, 'grad_norm': 37.43239974975586, 'learning_rate': 7.58770801925681e-06, 'epoch': 2.3}\n",
      "{'loss': 1.0776, 'grad_norm': 36.924102783203125, 'learning_rate': 7.858348927163111e-06, 'epoch': 2.38}\n",
      "{'loss': 1.0004, 'grad_norm': 58.424015045166016, 'learning_rate': 8.129532201618322e-06, 'epoch': 2.46}\n",
      "{'loss': 1.0437, 'grad_norm': 14.681171417236328, 'learning_rate': 8.400715476073533e-06, 'epoch': 2.54}\n",
      "{'loss': 0.9755, 'grad_norm': 12.852766036987305, 'learning_rate': 8.671898750528744e-06, 'epoch': 2.62}\n",
      "{'loss': 1.0424, 'grad_norm': 14.686453819274902, 'learning_rate': 8.943082024983955e-06, 'epoch': 2.71}\n",
      "{'loss': 1.037, 'grad_norm': 63.5254020690918, 'learning_rate': 9.213722932890256e-06, 'epoch': 2.79}\n",
      "{'loss': 1.0179, 'grad_norm': 40.95802688598633, 'learning_rate': 9.484906207345467e-06, 'epoch': 2.87}\n",
      "{'loss': 1.0339, 'grad_norm': 52.209259033203125, 'learning_rate': 9.75608948180068e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83029b1f4a5443bb157951068b383b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.5967812092214007, 'eval_f1': 0.59856429277332, 'eval_loss': 1.042001724243164, 'eval_runtime': 82.9414, 'eval_samples_per_second': 27.718, 'eval_steps_per_second': 9.247, 'epoch': 3.0}\n",
      "{'loss': 0.9127, 'grad_norm': 10.677167892456055, 'learning_rate': 9.688438244080261e-06, 'epoch': 3.03}\n",
      "{'loss': 0.7734, 'grad_norm': 21.07221794128418, 'learning_rate': 9.285459942521869e-06, 'epoch': 3.12}\n",
      "{'loss': 0.8189, 'grad_norm': 71.03575897216797, 'learning_rate': 8.881674069216865e-06, 'epoch': 3.2}\n",
      "{'loss': 0.8349, 'grad_norm': 178.13543701171875, 'learning_rate': 8.477888195911861e-06, 'epoch': 3.28}\n",
      "{'loss': 0.7607, 'grad_norm': 44.89866638183594, 'learning_rate': 8.074102322606857e-06, 'epoch': 3.36}\n",
      "{'loss': 0.7832, 'grad_norm': 0.015755489468574524, 'learning_rate': 7.670316449301853e-06, 'epoch': 3.44}\n",
      "{'loss': 0.8392, 'grad_norm': 32.59613037109375, 'learning_rate': 7.267338147743459e-06, 'epoch': 3.53}\n",
      "{'loss': 0.779, 'grad_norm': 106.7799301147461, 'learning_rate': 6.863552274438456e-06, 'epoch': 3.61}\n",
      "{'loss': 0.8016, 'grad_norm': 10.091631889343262, 'learning_rate': 6.459766401133452e-06, 'epoch': 3.69}\n",
      "{'loss': 0.7694, 'grad_norm': 105.1423110961914, 'learning_rate': 6.055980527828447e-06, 'epoch': 3.77}\n",
      "{'loss': 0.8166, 'grad_norm': 23.745458602905273, 'learning_rate': 5.653002226270054e-06, 'epoch': 3.85}\n",
      "{'loss': 0.7465, 'grad_norm': 12.109500885009766, 'learning_rate': 5.25002392471166e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea61607f7b04900b0813121c5a2afd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6489778164419313, 'eval_f1': 0.6515209391720787, 'eval_loss': 1.2690231800079346, 'eval_runtime': 82.9168, 'eval_samples_per_second': 27.727, 'eval_steps_per_second': 9.25, 'epoch': 4.0}\n",
      "{'loss': 0.6748, 'grad_norm': 16.509668350219727, 'learning_rate': 4.846238051406657e-06, 'epoch': 4.02}\n",
      "{'loss': 0.5074, 'grad_norm': 190.19386291503906, 'learning_rate': 4.443259749848262e-06, 'epoch': 4.1}\n",
      "{'loss': 0.5464, 'grad_norm': 106.08119201660156, 'learning_rate': 4.039473876543259e-06, 'epoch': 4.18}\n",
      "{'loss': 0.5656, 'grad_norm': 64.0899429321289, 'learning_rate': 3.6356880032382542e-06, 'epoch': 4.26}\n",
      "{'loss': 0.5538, 'grad_norm': 98.51527404785156, 'learning_rate': 3.2319021299332507e-06, 'epoch': 4.35}\n",
      "{'loss': 0.4943, 'grad_norm': 273.8886413574219, 'learning_rate': 2.8281162566282466e-06, 'epoch': 4.43}\n",
      "{'loss': 0.5168, 'grad_norm': 76.48027801513672, 'learning_rate': 2.424330383323243e-06, 'epoch': 4.51}\n",
      "{'loss': 0.5182, 'grad_norm': 14.625099182128906, 'learning_rate': 2.0205445100182394e-06, 'epoch': 4.59}\n",
      "{'loss': 0.5279, 'grad_norm': 125.61223602294922, 'learning_rate': 1.6167586367132354e-06, 'epoch': 4.67}\n",
      "{'loss': 0.5528, 'grad_norm': 103.86339569091797, 'learning_rate': 1.2129727634082316e-06, 'epoch': 4.76}\n",
      "{'loss': 0.4753, 'grad_norm': 123.37701416015625, 'learning_rate': 8.091868901032277e-07, 'epoch': 4.84}\n",
      "{'loss': 0.4879, 'grad_norm': 3.9194579124450684, 'learning_rate': 4.062085885448339e-07, 'epoch': 4.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ef9874fa814dbb8df68727cd6883ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6755110917790343, 'eval_f1': 0.6790337949238184, 'eval_loss': 1.6976536512374878, 'eval_runtime': 82.9849, 'eval_samples_per_second': 27.704, 'eval_steps_per_second': 9.243, 'epoch': 5.0}\n",
      "{'train_runtime': 11908.2565, 'train_samples_per_second': 7.68, 'train_steps_per_second': 2.56, 'train_loss': 1.010124164748352, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd9d6d474ce4e06bc2ba36d86530e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-16 18:23:49,618] Trial 0 finished with value: 0.6755110917790343 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6755110917790343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 1 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.3042227136021964e-06, 'batch_size': 2, 'warmup_ratio': 0.09918174110702827, 'weight_decay': 0.014766619671606473, 'adam_beta1': 0.925261304375699, 'adam_beta2': 0.9972694011663767, 'adam_epsilon': 6.348691828190262e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed7bd610904b45867864cfaecbbd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45725 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6082, 'grad_norm': 2.585113048553467, 'learning_rate': 1.437635266316354e-07, 'epoch': 0.05}\n",
      "{'loss': 1.6092, 'grad_norm': 3.525179147720337, 'learning_rate': 2.875270532632708e-07, 'epoch': 0.11}\n",
      "{'loss': 1.6047, 'grad_norm': 2.733987808227539, 'learning_rate': 4.312905798949062e-07, 'epoch': 0.16}\n",
      "{'loss': 1.5669, 'grad_norm': 2.842104196548462, 'learning_rate': 5.747665794732784e-07, 'epoch': 0.22}\n",
      "{'loss': 1.4959, 'grad_norm': 19.781494140625, 'learning_rate': 7.179550519983873e-07, 'epoch': 0.27}\n",
      "{'loss': 1.4939, 'grad_norm': 34.2506217956543, 'learning_rate': 8.614310515767595e-07, 'epoch': 0.33}\n",
      "{'loss': 1.4744, 'grad_norm': 14.1617431640625, 'learning_rate': 1.0051945782083948e-06, 'epoch': 0.38}\n",
      "{'loss': 1.4594, 'grad_norm': 13.139301300048828, 'learning_rate': 1.14895810484003e-06, 'epoch': 0.44}\n",
      "{'loss': 1.4255, 'grad_norm': 16.428150177001953, 'learning_rate': 1.2927216314716656e-06, 'epoch': 0.49}\n",
      "{'loss': 1.4518, 'grad_norm': 23.827686309814453, 'learning_rate': 1.289688779164764e-06, 'epoch': 0.55}\n",
      "{'loss': 1.422, 'grad_norm': 10.863518714904785, 'learning_rate': 1.2738566065749682e-06, 'epoch': 0.6}\n",
      "{'loss': 1.4229, 'grad_norm': 23.648067474365234, 'learning_rate': 1.2580244339851722e-06, 'epoch': 0.66}\n",
      "{'loss': 1.3971, 'grad_norm': 32.23230743408203, 'learning_rate': 1.2421922613953767e-06, 'epoch': 0.71}\n",
      "{'loss': 1.3988, 'grad_norm': 31.1035213470459, 'learning_rate': 1.2263917531507605e-06, 'epoch': 0.77}\n",
      "{'loss': 1.4107, 'grad_norm': 17.023820877075195, 'learning_rate': 1.2105595805609645e-06, 'epoch': 0.82}\n",
      "{'loss': 1.3906, 'grad_norm': 19.91000747680664, 'learning_rate': 1.194727407971169e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4016, 'grad_norm': 40.05221176147461, 'learning_rate': 1.178895235381373e-06, 'epoch': 0.93}\n",
      "{'loss': 1.3599, 'grad_norm': 24.271709442138672, 'learning_rate': 1.1630947271367568e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3c6c6da1fb4d60b6be1294e47928f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.39712918660287083, 'eval_f1': 0.39737049706515465, 'eval_loss': 1.3019558191299438, 'eval_runtime': 85.0011, 'eval_samples_per_second': 27.047, 'eval_steps_per_second': 13.529, 'epoch': 1.0}\n",
      "{'loss': 1.3763, 'grad_norm': 16.46857452392578, 'learning_rate': 1.1472942188921408e-06, 'epoch': 1.04}\n",
      "{'loss': 1.336, 'grad_norm': 32.43342208862305, 'learning_rate': 1.1314620463023449e-06, 'epoch': 1.09}\n",
      "{'loss': 1.3594, 'grad_norm': 51.57057571411133, 'learning_rate': 1.1156298737125491e-06, 'epoch': 1.15}\n",
      "{'loss': 1.3668, 'grad_norm': 35.937095642089844, 'learning_rate': 1.0997977011227534e-06, 'epoch': 1.2}\n",
      "{'loss': 1.3233, 'grad_norm': 55.94281005859375, 'learning_rate': 1.0839971928781372e-06, 'epoch': 1.26}\n",
      "{'loss': 1.3477, 'grad_norm': 29.807373046875, 'learning_rate': 1.0681650202883414e-06, 'epoch': 1.31}\n",
      "{'loss': 1.3324, 'grad_norm': 39.78839874267578, 'learning_rate': 1.0523328476985456e-06, 'epoch': 1.37}\n",
      "{'loss': 1.314, 'grad_norm': 75.51228332519531, 'learning_rate': 1.0365323394539294e-06, 'epoch': 1.42}\n",
      "{'loss': 1.3216, 'grad_norm': 70.47726440429688, 'learning_rate': 1.0207001668641337e-06, 'epoch': 1.48}\n",
      "{'loss': 1.3247, 'grad_norm': 28.943124771118164, 'learning_rate': 1.004867994274338e-06, 'epoch': 1.53}\n",
      "{'loss': 1.3305, 'grad_norm': 20.597383499145508, 'learning_rate': 9.890358216845422e-07, 'epoch': 1.59}\n",
      "{'loss': 1.2772, 'grad_norm': 53.38262939453125, 'learning_rate': 9.732036490947464e-07, 'epoch': 1.64}\n",
      "{'loss': 1.2986, 'grad_norm': 47.654991149902344, 'learning_rate': 9.573714765049504e-07, 'epoch': 1.69}\n",
      "{'loss': 1.309, 'grad_norm': 37.57880401611328, 'learning_rate': 9.415393039151548e-07, 'epoch': 1.75}\n",
      "{'loss': 1.3009, 'grad_norm': 48.31801223754883, 'learning_rate': 9.257071313253589e-07, 'epoch': 1.8}\n",
      "{'loss': 1.3072, 'grad_norm': 28.656455993652344, 'learning_rate': 9.099066230807428e-07, 'epoch': 1.86}\n",
      "{'loss': 1.3404, 'grad_norm': 47.64892578125, 'learning_rate': 8.940744504909471e-07, 'epoch': 1.91}\n",
      "{'loss': 1.2969, 'grad_norm': 8.728014945983887, 'learning_rate': 8.782422779011512e-07, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31866a1dd92f48d184715a357ba61fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4480208786428882, 'eval_f1': 0.44956409030811867, 'eval_loss': 1.2185862064361572, 'eval_runtime': 84.858, 'eval_samples_per_second': 27.092, 'eval_steps_per_second': 13.552, 'epoch': 2.0}\n",
      "{'loss': 1.2556, 'grad_norm': 67.97032928466797, 'learning_rate': 8.624101053113556e-07, 'epoch': 2.02}\n",
      "{'loss': 1.2354, 'grad_norm': 49.313636779785156, 'learning_rate': 8.466095970667394e-07, 'epoch': 2.08}\n",
      "{'loss': 1.2561, 'grad_norm': 26.08816909790039, 'learning_rate': 8.307774244769435e-07, 'epoch': 2.13}\n",
      "{'loss': 1.2543, 'grad_norm': 26.237464904785156, 'learning_rate': 8.149452518871479e-07, 'epoch': 2.19}\n",
      "{'loss': 1.2442, 'grad_norm': 61.43034744262695, 'learning_rate': 7.99113079297352e-07, 'epoch': 2.24}\n",
      "{'loss': 1.2517, 'grad_norm': 17.540170669555664, 'learning_rate': 7.833125710527358e-07, 'epoch': 2.3}\n",
      "{'loss': 1.2646, 'grad_norm': 14.766806602478027, 'learning_rate': 7.6748039846294e-07, 'epoch': 2.35}\n",
      "{'loss': 1.2502, 'grad_norm': 31.206735610961914, 'learning_rate': 7.516482258731443e-07, 'epoch': 2.41}\n",
      "{'loss': 1.2218, 'grad_norm': 57.59080505371094, 'learning_rate': 7.358160532833484e-07, 'epoch': 2.46}\n",
      "{'loss': 1.2591, 'grad_norm': 31.48540496826172, 'learning_rate': 7.199838806935528e-07, 'epoch': 2.52}\n",
      "{'loss': 1.2223, 'grad_norm': 40.10590362548828, 'learning_rate': 7.041517081037569e-07, 'epoch': 2.57}\n",
      "{'loss': 1.2119, 'grad_norm': 33.12777328491211, 'learning_rate': 6.883195355139611e-07, 'epoch': 2.62}\n",
      "{'loss': 1.2667, 'grad_norm': 26.35072135925293, 'learning_rate': 6.724873629241654e-07, 'epoch': 2.68}\n",
      "{'loss': 1.2193, 'grad_norm': 105.47494506835938, 'learning_rate': 6.566868546795492e-07, 'epoch': 2.73}\n",
      "{'loss': 1.246, 'grad_norm': 38.99732208251953, 'learning_rate': 6.408546820897534e-07, 'epoch': 2.79}\n",
      "{'loss': 1.2487, 'grad_norm': 64.22789764404297, 'learning_rate': 6.250225094999577e-07, 'epoch': 2.84}\n",
      "{'loss': 1.2121, 'grad_norm': 50.7550163269043, 'learning_rate': 6.091903369101618e-07, 'epoch': 2.9}\n",
      "{'loss': 1.221, 'grad_norm': 61.784507751464844, 'learning_rate': 5.933898286655457e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7387cbe67d1c4dbe98ebdfbe4e12d193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4745541539799913, 'eval_f1': 0.47618466348176836, 'eval_loss': 1.1888489723205566, 'eval_runtime': 84.8967, 'eval_samples_per_second': 27.08, 'eval_steps_per_second': 13.546, 'epoch': 3.0}\n",
      "{'loss': 1.237, 'grad_norm': 28.527843475341797, 'learning_rate': 5.7755765607575e-07, 'epoch': 3.01}\n",
      "{'loss': 1.1674, 'grad_norm': 38.5905876159668, 'learning_rate': 5.617254834859541e-07, 'epoch': 3.06}\n",
      "{'loss': 1.1991, 'grad_norm': 6.073561668395996, 'learning_rate': 5.458933108961583e-07, 'epoch': 3.12}\n",
      "{'loss': 1.2135, 'grad_norm': 9.602526664733887, 'learning_rate': 5.301244669967217e-07, 'epoch': 3.17}\n",
      "{'loss': 1.2226, 'grad_norm': 66.79460144042969, 'learning_rate': 5.14292294406926e-07, 'epoch': 3.23}\n",
      "{'loss': 1.2166, 'grad_norm': 53.70254898071289, 'learning_rate': 4.984601218171302e-07, 'epoch': 3.28}\n",
      "{'loss': 1.1783, 'grad_norm': 51.066951751708984, 'learning_rate': 4.826279492273344e-07, 'epoch': 3.34}\n",
      "{'loss': 1.1833, 'grad_norm': 23.885244369506836, 'learning_rate': 4.667957766375386e-07, 'epoch': 3.39}\n",
      "{'loss': 1.195, 'grad_norm': 36.849979400634766, 'learning_rate': 4.5096360404774286e-07, 'epoch': 3.44}\n",
      "{'loss': 1.2283, 'grad_norm': 25.745559692382812, 'learning_rate': 4.351630958031267e-07, 'epoch': 3.5}\n",
      "{'loss': 1.1634, 'grad_norm': 54.00531768798828, 'learning_rate': 4.193309232133309e-07, 'epoch': 3.55}\n",
      "{'loss': 1.2055, 'grad_norm': 48.24699401855469, 'learning_rate': 4.0349875062353515e-07, 'epoch': 3.61}\n",
      "{'loss': 1.1953, 'grad_norm': 101.3050537109375, 'learning_rate': 3.876665780337394e-07, 'epoch': 3.66}\n",
      "{'loss': 1.1926, 'grad_norm': 75.16376495361328, 'learning_rate': 3.718660697891232e-07, 'epoch': 3.72}\n",
      "{'loss': 1.1384, 'grad_norm': 25.026262283325195, 'learning_rate': 3.5603389719932744e-07, 'epoch': 3.77}\n",
      "{'loss': 1.1814, 'grad_norm': 39.10886764526367, 'learning_rate': 3.402017246095317e-07, 'epoch': 3.83}\n",
      "{'loss': 1.2465, 'grad_norm': 95.16380310058594, 'learning_rate': 3.2436955201973587e-07, 'epoch': 3.88}\n",
      "{'loss': 1.1887, 'grad_norm': 41.403316497802734, 'learning_rate': 3.0853737942994006e-07, 'epoch': 3.94}\n",
      "{'loss': 1.1508, 'grad_norm': 29.59967613220215, 'learning_rate': 2.9273687118532387e-07, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612af5654914484399a904a0a682a2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4936929099608525, 'eval_f1': 0.4954601263039024, 'eval_loss': 1.1860566139221191, 'eval_runtime': 84.9502, 'eval_samples_per_second': 27.063, 'eval_steps_per_second': 13.537, 'epoch': 4.0}\n",
      "{'loss': 1.1134, 'grad_norm': 75.09806060791016, 'learning_rate': 2.769046985955281e-07, 'epoch': 4.05}\n",
      "{'loss': 1.17, 'grad_norm': 117.3279800415039, 'learning_rate': 2.6107252600573235e-07, 'epoch': 4.1}\n",
      "{'loss': 1.1759, 'grad_norm': 65.0226058959961, 'learning_rate': 2.4524035341593654e-07, 'epoch': 4.16}\n",
      "{'loss': 1.152, 'grad_norm': 47.28899383544922, 'learning_rate': 2.294398451713204e-07, 'epoch': 4.21}\n",
      "{'loss': 1.1272, 'grad_norm': 104.80470275878906, 'learning_rate': 2.136076725815246e-07, 'epoch': 4.26}\n",
      "{'loss': 1.159, 'grad_norm': 38.04895782470703, 'learning_rate': 1.9777549999172885e-07, 'epoch': 4.32}\n",
      "{'loss': 1.1884, 'grad_norm': 24.521554946899414, 'learning_rate': 1.8194332740193307e-07, 'epoch': 4.37}\n",
      "{'loss': 1.1683, 'grad_norm': 28.663904190063477, 'learning_rate': 1.6614281915731687e-07, 'epoch': 4.43}\n",
      "{'loss': 1.1756, 'grad_norm': 23.55503273010254, 'learning_rate': 1.503106465675211e-07, 'epoch': 4.48}\n",
      "{'loss': 1.1128, 'grad_norm': 46.98788070678711, 'learning_rate': 1.3447847397772533e-07, 'epoch': 4.54}\n",
      "{'loss': 1.1885, 'grad_norm': 121.79212188720703, 'learning_rate': 1.1864630138792954e-07, 'epoch': 4.59}\n",
      "{'loss': 1.1608, 'grad_norm': 38.146217346191406, 'learning_rate': 1.0281412879813377e-07, 'epoch': 4.65}\n",
      "{'loss': 1.151, 'grad_norm': 13.245527267456055, 'learning_rate': 8.70136205535176e-08, 'epoch': 4.7}\n",
      "{'loss': 1.1832, 'grad_norm': 46.08942413330078, 'learning_rate': 7.118144796372181e-08, 'epoch': 4.76}\n",
      "{'loss': 1.1707, 'grad_norm': 32.77238082885742, 'learning_rate': 5.5349275373926036e-08, 'epoch': 4.81}\n",
      "{'loss': 1.1344, 'grad_norm': 49.036949157714844, 'learning_rate': 3.951710278413026e-08, 'epoch': 4.87}\n",
      "{'loss': 1.1409, 'grad_norm': 55.23716735839844, 'learning_rate': 2.368493019433448e-08, 'epoch': 4.92}\n",
      "{'loss': 1.1934, 'grad_norm': 42.42463302612305, 'learning_rate': 7.884421949718296e-09, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c3d379b0424cdfb63abf3d2970c6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4989125706829056, 'eval_f1': 0.5003226595967735, 'eval_loss': 1.1893906593322754, 'eval_runtime': 83.5258, 'eval_samples_per_second': 27.524, 'eval_steps_per_second': 13.768, 'epoch': 5.0}\n",
      "{'train_runtime': 12375.9476, 'train_samples_per_second': 7.389, 'train_steps_per_second': 3.695, 'train_loss': 1.275919856868721, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9ca0a6dabd4917877705f8717e6112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-16 21:51:31,490] Trial 1 finished with value: 0.4989125706829056 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.3042227136021964e-06, 'batch_size': 2, 'warmup_ratio': 0.09918174110702827, 'weight_decay': 0.014766619671606473, 'adam_beta1': 0.925261304375699, 'adam_beta2': 0.9972694011663767, 'adam_epsilon': 6.348691828190262e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6755110917790343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 2 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 6.896556694878205e-07, 'batch_size': 4, 'warmup_ratio': 0.7952567679467826, 'weight_decay': 0.04803187586959426, 'adam_beta1': 0.9150496535084768, 'adam_beta2': 0.9965449660871344, 'adam_epsilon': 5.402942880312419e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0093d38d24c04419b0b7a7b5aa365480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.611, 'grad_norm': 1.9610719680786133, 'learning_rate': 1.896325532027663e-08, 'epoch': 0.11}\n",
      "{'loss': 1.6129, 'grad_norm': 2.407881021499634, 'learning_rate': 3.792651064055326e-08, 'epoch': 0.22}\n",
      "{'loss': 1.6111, 'grad_norm': 1.9186650514602661, 'learning_rate': 5.6889765960829894e-08, 'epoch': 0.33}\n",
      "{'loss': 1.611, 'grad_norm': 1.6168291568756104, 'learning_rate': 7.585302128110653e-08, 'epoch': 0.44}\n",
      "{'loss': 1.6091, 'grad_norm': 1.6504346132278442, 'learning_rate': 9.47783500907426e-08, 'epoch': 0.55}\n",
      "{'loss': 1.6091, 'grad_norm': 1.8088207244873047, 'learning_rate': 1.1374160541101924e-07, 'epoch': 0.66}\n",
      "{'loss': 1.607, 'grad_norm': 2.33923077583313, 'learning_rate': 1.3270486073129587e-07, 'epoch': 0.77}\n",
      "{'loss': 1.5897, 'grad_norm': 1.8429104089736938, 'learning_rate': 1.516681160515725e-07, 'epoch': 0.87}\n",
      "{'loss': 1.5725, 'grad_norm': 2.1944947242736816, 'learning_rate': 1.7059344486120857e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb64ccc3085c4760a1ca0cf6afbdf352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.31056981296215747, 'eval_f1': 0.31138836928944935, 'eval_loss': 1.546942114830017, 'eval_runtime': 93.8825, 'eval_samples_per_second': 24.488, 'eval_steps_per_second': 6.125, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "{'train_runtime': 3231.6143, 'train_samples_per_second': 28.299, 'train_steps_per_second': 7.075, 'train_loss': 1.6029558046208725, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c11bd7cab947e4befffe50e0c343fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-16 22:46:58,164] Trial 2 finished with value: 0.31056981296215747 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 6.896556694878205e-07, 'batch_size': 4, 'warmup_ratio': 0.7952567679467826, 'weight_decay': 0.04803187586959426, 'adam_beta1': 0.9150496535084768, 'adam_beta2': 0.9965449660871344, 'adam_epsilon': 5.402942880312419e-07, 'lr_scheduler_type': 'cosine'}. Best is trial 0 with value: 0.6755110917790343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 3 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.0149372419374312e-07, 'batch_size': 4, 'warmup_ratio': 0.7256166381179319, 'weight_decay': 0.21189248409312114, 'adam_beta1': 0.9391653610469487, 'adam_beta2': 0.9903297892881989, 'adam_epsilon': 1.392138634568796e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54e03b50e7c4dc69a22940e79c405cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 1.986233115196228, 'learning_rate': 3.0585138679406674e-09, 'epoch': 0.11}\n",
      "{'loss': 1.61, 'grad_norm': 2.2650704383850098, 'learning_rate': 6.117027735881335e-09, 'epoch': 0.22}\n",
      "{'loss': 1.611, 'grad_norm': 1.8638962507247925, 'learning_rate': 9.175541603822003e-09, 'epoch': 0.33}\n",
      "{'loss': 1.6112, 'grad_norm': 1.607124924659729, 'learning_rate': 1.223405547176267e-08, 'epoch': 0.44}\n",
      "{'loss': 1.6122, 'grad_norm': 1.6188691854476929, 'learning_rate': 1.5286452311967456e-08, 'epoch': 0.55}\n",
      "{'loss': 1.6118, 'grad_norm': 1.650319218635559, 'learning_rate': 1.8344966179908126e-08, 'epoch': 0.66}\n",
      "{'loss': 1.6099, 'grad_norm': 2.0125181674957275, 'learning_rate': 2.1403480047848793e-08, 'epoch': 0.77}\n",
      "{'loss': 1.6109, 'grad_norm': 1.726709008216858, 'learning_rate': 2.4461993915789462e-08, 'epoch': 0.87}\n",
      "{'loss': 1.6113, 'grad_norm': 2.0067360401153564, 'learning_rate': 2.7514390755994246e-08, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db1c280500d4d91a763361d181a7c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.19138755980861244, 'eval_f1': 0.2157940352311797, 'eval_loss': 1.609500765800476, 'eval_runtime': 1380.2172, 'eval_samples_per_second': 1.666, 'eval_steps_per_second': 0.417, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "{'train_runtime': 5685.4029, 'train_samples_per_second': 16.085, 'train_steps_per_second': 4.022, 'train_loss': 1.6109933325531653, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b2d900bcb442e088a9e8f88fefac87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 00:44:44,836] Trial 3 finished with value: 0.19138755980861244 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.0149372419374312e-07, 'batch_size': 4, 'warmup_ratio': 0.7256166381179319, 'weight_decay': 0.21189248409312114, 'adam_beta1': 0.9391653610469487, 'adam_beta2': 0.9903297892881989, 'adam_epsilon': 1.392138634568796e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6755110917790343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 4 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 2.2101758678517027e-07, 'batch_size': 4, 'warmup_ratio': 0.01664485287339723, 'weight_decay': 0.05229210409552859, 'adam_beta1': 0.8293091118799443, 'adam_beta2': 0.991007938374325, 'adam_epsilon': 4.4675288273328307e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4019c3fe399f46a7aef6e17157c760f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 1.9842169284820557, 'learning_rate': 2.1984781749023008e-07, 'epoch': 0.11}\n",
      "{'loss': 1.6098, 'grad_norm': 2.2542338371276855, 'learning_rate': 2.1493282045266626e-07, 'epoch': 0.22}\n",
      "{'loss': 1.6106, 'grad_norm': 1.8546525239944458, 'learning_rate': 2.1001782341510243e-07, 'epoch': 0.33}\n",
      "{'loss': 1.6105, 'grad_norm': 1.6134577989578247, 'learning_rate': 2.0510282637753858e-07, 'epoch': 0.44}\n",
      "{'loss': 1.6085, 'grad_norm': 1.8253097534179688, 'learning_rate': 2.001976593340499e-07, 'epoch': 0.55}\n",
      "{'loss': 1.5943, 'grad_norm': 1.73936128616333, 'learning_rate': 1.9528266229648605e-07, 'epoch': 0.66}\n",
      "{'loss': 1.5684, 'grad_norm': 2.2078733444213867, 'learning_rate': 1.9037749525299734e-07, 'epoch': 0.77}\n",
      "{'loss': 1.5338, 'grad_norm': 1.9535191059112549, 'learning_rate': 1.8546249821543352e-07, 'epoch': 0.87}\n",
      "{'loss': 1.4973, 'grad_norm': 4.079860210418701, 'learning_rate': 1.8055733117194483e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16274c129ce64a37a322619314d13d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.31361461505002175, 'eval_f1': 0.31443212285676564, 'eval_loss': 1.4577686786651611, 'eval_runtime': 120.0404, 'eval_samples_per_second': 19.152, 'eval_steps_per_second': 4.79, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "{'train_runtime': 3808.4581, 'train_samples_per_second': 24.012, 'train_steps_per_second': 6.004, 'train_loss': 1.5810835167012902, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3467f3d4f7ce4099ab3343590cf817b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: eval_accuracy below threshold of 0.35\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 01:50:15,399] Trial 4 finished with value: 0.31361461505002175 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 2.2101758678517027e-07, 'batch_size': 4, 'warmup_ratio': 0.01664485287339723, 'weight_decay': 0.05229210409552859, 'adam_beta1': 0.8293091118799443, 'adam_beta2': 0.991007938374325, 'adam_epsilon': 4.4675288273328307e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6755110917790343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 5 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe944a66f7146b8a05aef40051803b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 1.9778715372085571, 'learning_rate': 5.766490247568446e-07, 'epoch': 0.11}\n",
      "{'loss': 1.6044, 'grad_norm': 2.3704445362091064, 'learning_rate': 1.1532980495136891e-06, 'epoch': 0.22}\n",
      "{'loss': 1.4953, 'grad_norm': 10.23387336730957, 'learning_rate': 1.7264871801219926e-06, 'epoch': 0.33}\n",
      "{'loss': 1.4417, 'grad_norm': 11.970279693603516, 'learning_rate': 2.303136204878837e-06, 'epoch': 0.44}\n",
      "{'loss': 1.4174, 'grad_norm': 13.763654708862305, 'learning_rate': 2.8797852296356817e-06, 'epoch': 0.55}\n",
      "{'loss': 1.4072, 'grad_norm': 14.317955017089844, 'learning_rate': 3.4552809563430125e-06, 'epoch': 0.66}\n",
      "{'loss': 1.3841, 'grad_norm': 16.474056243896484, 'learning_rate': 4.031929981099857e-06, 'epoch': 0.77}\n",
      "{'loss': 1.3625, 'grad_norm': 13.519970893859863, 'learning_rate': 4.608579005856701e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3217, 'grad_norm': 19.089096069335938, 'learning_rate': 5.185228030613547e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0284cdfcae4f1fb251a6bcc8f984b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.432361896476729, 'eval_f1': 0.43306610856151, 'eval_loss': 1.2588419914245605, 'eval_runtime': 97.0358, 'eval_samples_per_second': 23.692, 'eval_steps_per_second': 5.926, 'epoch': 1.0}\n",
      "{'loss': 1.2863, 'grad_norm': 24.355783462524414, 'learning_rate': 5.7607237573208775e-06, 'epoch': 1.09}\n",
      "{'loss': 1.2862, 'grad_norm': 13.579124450683594, 'learning_rate': 6.3373727820777214e-06, 'epoch': 1.2}\n",
      "{'loss': 1.2524, 'grad_norm': 22.594913482666016, 'learning_rate': 6.914021806834566e-06, 'epoch': 1.31}\n",
      "{'loss': 1.251, 'grad_norm': 38.666847229003906, 'learning_rate': 7.490670831591411e-06, 'epoch': 1.42}\n",
      "{'loss': 1.1994, 'grad_norm': 17.02745819091797, 'learning_rate': 8.067319856348255e-06, 'epoch': 1.53}\n",
      "{'loss': 1.1836, 'grad_norm': 52.12561798095703, 'learning_rate': 8.6439688811051e-06, 'epoch': 1.64}\n",
      "{'loss': 1.1796, 'grad_norm': 22.33612632751465, 'learning_rate': 9.21946460781243e-06, 'epoch': 1.75}\n",
      "{'loss': 1.1744, 'grad_norm': 22.792207717895508, 'learning_rate': 9.796113632569277e-06, 'epoch': 1.86}\n",
      "{'loss': 1.1707, 'grad_norm': 17.62858009338379, 'learning_rate': 1.037276265732612e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16245cd15e3f4977bacf554504fbb639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.526750761200522, 'eval_f1': 0.529128899201623, 'eval_loss': 1.0664671659469604, 'eval_runtime': 96.9607, 'eval_samples_per_second': 23.711, 'eval_steps_per_second': 5.93, 'epoch': 2.0}\n",
      "{'loss': 1.0124, 'grad_norm': 9.407689094543457, 'learning_rate': 1.0949411682082965e-05, 'epoch': 2.08}\n",
      "{'loss': 0.9633, 'grad_norm': 24.681081771850586, 'learning_rate': 1.152606070683981e-05, 'epoch': 2.19}\n",
      "{'loss': 0.9638, 'grad_norm': 30.624353408813477, 'learning_rate': 1.2100403135497626e-05, 'epoch': 2.3}\n",
      "{'loss': 0.9796, 'grad_norm': 60.10416793823242, 'learning_rate': 1.2677052160254471e-05, 'epoch': 2.41}\n",
      "{'loss': 0.9645, 'grad_norm': 17.69792366027832, 'learning_rate': 1.3253701185011316e-05, 'epoch': 2.51}\n",
      "{'loss': 0.9201, 'grad_norm': 67.5637435913086, 'learning_rate': 1.383035020976816e-05, 'epoch': 2.62}\n",
      "{'loss': 0.9832, 'grad_norm': 19.638622283935547, 'learning_rate': 1.4406999234525005e-05, 'epoch': 2.73}\n",
      "{'loss': 0.957, 'grad_norm': 32.09751892089844, 'learning_rate': 1.4983648259281848e-05, 'epoch': 2.84}\n",
      "{'loss': 0.9613, 'grad_norm': 40.62754440307617, 'learning_rate': 1.5560297284038695e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e250ed59d0425cb5e7cbb3fabc6cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6207046541974771, 'eval_f1': 0.6225858799770051, 'eval_loss': 1.0125455856323242, 'eval_runtime': 96.9248, 'eval_samples_per_second': 23.719, 'eval_steps_per_second': 5.932, 'epoch': 3.0}\n",
      "{'loss': 0.766, 'grad_norm': 77.90558624267578, 'learning_rate': 1.57691034101029e-05, 'epoch': 3.06}\n",
      "{'loss': 0.7241, 'grad_norm': 15.309263229370117, 'learning_rate': 1.5522322889961592e-05, 'epoch': 3.17}\n",
      "{'loss': 0.7278, 'grad_norm': 16.297252655029297, 'learning_rate': 1.5051732904565056e-05, 'epoch': 3.28}\n",
      "{'loss': 0.683, 'grad_norm': 28.09387969970703, 'learning_rate': 1.4372963891239031e-05, 'epoch': 3.39}\n",
      "{'loss': 0.725, 'grad_norm': 22.27782440185547, 'learning_rate': 1.3503217961023888e-05, 'epoch': 3.5}\n",
      "{'loss': 0.6945, 'grad_norm': 81.18882751464844, 'learning_rate': 1.2469170719621684e-05, 'epoch': 3.61}\n",
      "{'loss': 0.6719, 'grad_norm': 23.0159854888916, 'learning_rate': 1.1301162423864706e-05, 'epoch': 3.72}\n",
      "{'loss': 0.6976, 'grad_norm': 50.78999710083008, 'learning_rate': 1.0033463917730498e-05, 'epoch': 3.83}\n",
      "{'loss': 0.6337, 'grad_norm': 12.062256813049316, 'learning_rate': 8.705968087797533e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195a6950f3ba48fa80a0a936e0e049e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6698564593301436, 'eval_f1': 0.6717159944115402, 'eval_loss': 1.168811559677124, 'eval_runtime': 96.7147, 'eval_samples_per_second': 23.771, 'eval_steps_per_second': 5.945, 'epoch': 4.0}\n",
      "{'loss': 0.5326, 'grad_norm': 9.531551361083984, 'learning_rate': 7.352317793748273e-06, 'epoch': 4.05}\n",
      "{'loss': 0.4342, 'grad_norm': 44.113094329833984, 'learning_rate': 6.01484139636786e-06, 'epoch': 4.15}\n",
      "{'loss': 0.4232, 'grad_norm': 33.592132568359375, 'learning_rate': 4.732782148555083e-06, 'epoch': 4.26}\n",
      "{'loss': 0.4256, 'grad_norm': 4.531754016876221, 'learning_rate': 3.5437572937930357e-06, 'epoch': 4.37}\n",
      "{'loss': 0.4175, 'grad_norm': 3.8217132091522217, 'learning_rate': 2.484627454241376e-06, 'epoch': 4.48}\n",
      "{'loss': 0.3758, 'grad_norm': 32.461368560791016, 'learning_rate': 1.5822342634285167e-06, 'epoch': 4.59}\n",
      "{'loss': 0.373, 'grad_norm': 4.0994672775268555, 'learning_rate': 8.653165342848131e-07, 'epoch': 4.7}\n",
      "{'loss': 0.3716, 'grad_norm': 14.042684555053711, 'learning_rate': 3.549095417560572e-07, 'epoch': 4.81}\n",
      "{'loss': 0.401, 'grad_norm': 24.127723693847656, 'learning_rate': 6.59892740005084e-08, 'epoch': 4.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3087454e74d4988970d4a3a4a1c4632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6824706394084384, 'eval_f1': 0.685762574950659, 'eval_loss': 1.8748891353607178, 'eval_runtime': 97.2477, 'eval_samples_per_second': 23.641, 'eval_steps_per_second': 5.913, 'epoch': 5.0}\n",
      "{'train_runtime': 19770.9209, 'train_samples_per_second': 4.625, 'train_steps_per_second': 1.156, 'train_loss': 0.9428062133655594, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cef05a3fcfd4de5b42a41ce770d84e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 07:21:25,018] Trial 5 finished with value: 0.6824706394084384 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 5 with value: 0.6824706394084384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 6 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1d1b436cb840b5a44ddecf1cec851d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30485 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5661, 'grad_norm': 11.711294174194336, 'learning_rate': 3.949385270879653e-06, 'epoch': 0.08}\n",
      "{'loss': 1.4489, 'grad_norm': 10.554149627685547, 'learning_rate': 7.914663440233671e-06, 'epoch': 0.16}\n",
      "{'loss': 1.4098, 'grad_norm': 11.79371166229248, 'learning_rate': 1.0050708238967809e-05, 'epoch': 0.25}\n",
      "{'loss': 1.3533, 'grad_norm': 19.431964874267578, 'learning_rate': 1.0036743199974956e-05, 'epoch': 0.33}\n",
      "{'loss': 1.3473, 'grad_norm': 24.68314552307129, 'learning_rate': 1.0008301590380624e-05, 'epoch': 0.41}\n",
      "{'loss': 1.3161, 'grad_norm': 25.692615509033203, 'learning_rate': 9.965565545397013e-06, 'epoch': 0.49}\n",
      "{'loss': 1.3102, 'grad_norm': 10.560772895812988, 'learning_rate': 9.908487291771306e-06, 'epoch': 0.57}\n",
      "{'loss': 1.2576, 'grad_norm': 11.055680274963379, 'learning_rate': 9.83730302206875e-06, 'epoch': 0.66}\n",
      "{'loss': 1.2612, 'grad_norm': 17.875919342041016, 'learning_rate': 9.752218400524156e-06, 'epoch': 0.74}\n",
      "{'loss': 1.2479, 'grad_norm': 17.603532791137695, 'learning_rate': 9.653690174395407e-06, 'epoch': 0.82}\n",
      "{'loss': 1.215, 'grad_norm': 17.349590301513672, 'learning_rate': 9.541608202141372e-06, 'epoch': 0.9}\n",
      "{'loss': 1.1844, 'grad_norm': 27.897993087768555, 'learning_rate': 9.41648019388126e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85628cc14f94e7c9e3156642775291c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.5289256198347108, 'eval_f1': 0.5308621643875746, 'eval_loss': 1.0884441137313843, 'eval_runtime': 82.5838, 'eval_samples_per_second': 27.838, 'eval_steps_per_second': 9.288, 'epoch': 1.0}\n",
      "{'loss': 1.0655, 'grad_norm': 31.870115280151367, 'learning_rate': 9.278955692616672e-06, 'epoch': 1.07}\n",
      "{'loss': 1.0042, 'grad_norm': 46.548980712890625, 'learning_rate': 9.12888096412232e-06, 'epoch': 1.15}\n",
      "{'loss': 1.0272, 'grad_norm': 28.26569175720215, 'learning_rate': 8.966952643288838e-06, 'epoch': 1.23}\n",
      "{'loss': 0.9901, 'grad_norm': 21.341144561767578, 'learning_rate': 8.793638570333422e-06, 'epoch': 1.31}\n",
      "{'loss': 0.9863, 'grad_norm': 66.85433197021484, 'learning_rate': 8.609439480971216e-06, 'epoch': 1.39}\n",
      "{'loss': 0.9759, 'grad_norm': 31.788536071777344, 'learning_rate': 8.41488755969923e-06, 'epoch': 1.48}\n",
      "{'loss': 0.9488, 'grad_norm': 55.67478561401367, 'learning_rate': 8.210544902219039e-06, 'epoch': 1.56}\n",
      "{'loss': 0.9794, 'grad_norm': 58.71052551269531, 'learning_rate': 7.997001891440597e-06, 'epoch': 1.64}\n",
      "{'loss': 0.9647, 'grad_norm': 46.2634162902832, 'learning_rate': 7.775327888167305e-06, 'epoch': 1.72}\n",
      "{'loss': 0.9502, 'grad_norm': 91.31755065917969, 'learning_rate': 7.545275090146866e-06, 'epoch': 1.8}\n",
      "{'loss': 0.9291, 'grad_norm': 63.38766860961914, 'learning_rate': 7.308425497077418e-06, 'epoch': 1.89}\n",
      "{'loss': 0.96, 'grad_norm': 21.791929244995117, 'learning_rate': 7.064514342401982e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892e1fb73a374ecab1eb924a2c7c36bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6133101348412353, 'eval_f1': 0.6170144726614226, 'eval_loss': 1.0821328163146973, 'eval_runtime': 82.5989, 'eval_samples_per_second': 27.833, 'eval_steps_per_second': 9.286, 'epoch': 2.0}\n",
      "{'loss': 0.7402, 'grad_norm': 31.724708557128906, 'learning_rate': 6.814713924071282e-06, 'epoch': 2.05}\n",
      "{'loss': 0.7246, 'grad_norm': 82.34474182128906, 'learning_rate': 6.5602605664758775e-06, 'epoch': 2.13}\n",
      "{'loss': 0.7114, 'grad_norm': 241.4402313232422, 'learning_rate': 6.300869824375521e-06, 'epoch': 2.21}\n",
      "{'loss': 0.6798, 'grad_norm': 59.85123062133789, 'learning_rate': 6.037796125801591e-06, 'epoch': 2.3}\n",
      "{'loss': 0.7688, 'grad_norm': 161.48974609375, 'learning_rate': 5.771799538268193e-06, 'epoch': 2.38}\n",
      "{'loss': 0.6825, 'grad_norm': 29.87432861328125, 'learning_rate': 5.503648574044489e-06, 'epoch': 2.46}\n",
      "{'loss': 0.7242, 'grad_norm': 13.752564430236816, 'learning_rate': 5.234117969783694e-06, 'epoch': 2.54}\n",
      "{'loss': 0.6424, 'grad_norm': 11.418335914611816, 'learning_rate': 4.963986448168718e-06, 'epoch': 2.62}\n",
      "{'loss': 0.7269, 'grad_norm': 14.561441421508789, 'learning_rate': 4.6940344680414305e-06, 'epoch': 2.71}\n",
      "{'loss': 0.7381, 'grad_norm': 59.68691635131836, 'learning_rate': 4.425578479705464e-06, 'epoch': 2.79}\n",
      "{'loss': 0.6533, 'grad_norm': 41.354942321777344, 'learning_rate': 4.1583183848291735e-06, 'epoch': 2.87}\n",
      "{'loss': 0.7464, 'grad_norm': 0.5689562559127808, 'learning_rate': 3.893565552248151e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323192c784d84f068dbf45fd6cb6f550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6528925619834711, 'eval_f1': 0.6548668627985723, 'eval_loss': 1.3163256645202637, 'eval_runtime': 82.5619, 'eval_samples_per_second': 27.846, 'eval_steps_per_second': 9.29, 'epoch': 3.0}\n",
      "{'loss': 0.5888, 'grad_norm': 19.721939086914062, 'learning_rate': 3.632084900798461e-06, 'epoch': 3.03}\n",
      "{'loss': 0.4963, 'grad_norm': 3.8470921516418457, 'learning_rate': 3.375142285260742e-06, 'epoch': 3.12}\n",
      "{'loss': 0.5199, 'grad_norm': 138.81040954589844, 'learning_rate': 3.1224504777332095e-06, 'epoch': 3.2}\n",
      "{'loss': 0.5351, 'grad_norm': 79.50592041015625, 'learning_rate': 2.8752587423080687e-06, 'epoch': 3.28}\n",
      "{'loss': 0.5114, 'grad_norm': 42.25868225097656, 'learning_rate': 2.634281260634312e-06, 'epoch': 3.36}\n",
      "{'loss': 0.4829, 'grad_norm': 0.0002444460988044739, 'learning_rate': 2.400214260257463e-06, 'epoch': 3.44}\n",
      "{'loss': 0.5622, 'grad_norm': 1.629701018333435, 'learning_rate': 2.1746239364265767e-06, 'epoch': 3.53}\n",
      "{'loss': 0.4635, 'grad_norm': 103.116943359375, 'learning_rate': 1.9563505280620042e-06, 'epoch': 3.61}\n",
      "{'loss': 0.5117, 'grad_norm': 13.641437530517578, 'learning_rate': 1.746946265541333e-06, 'epoch': 3.69}\n",
      "{'loss': 0.5113, 'grad_norm': 0.10459696501493454, 'learning_rate': 1.5470161556672953e-06, 'epoch': 3.77}\n",
      "{'loss': 0.5304, 'grad_norm': 81.70208740234375, 'learning_rate': 1.3571378327014336e-06, 'epoch': 3.85}\n",
      "{'loss': 0.4739, 'grad_norm': 0.021094920113682747, 'learning_rate': 1.1778598894742043e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c187affdbd1543bfb6090048122fd5e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6702914310569813, 'eval_f1': 0.6729194953170986, 'eval_loss': 1.876667857170105, 'eval_runtime': 82.6202, 'eval_samples_per_second': 27.826, 'eval_steps_per_second': 9.283, 'epoch': 4.0}\n",
      "{'loss': 0.508, 'grad_norm': 17.28328514099121, 'learning_rate': 1.0100251870693411e-06, 'epoch': 4.02}\n",
      "{'loss': 0.367, 'grad_norm': 149.6023712158203, 'learning_rate': 8.534461084606884e-07, 'epoch': 4.1}\n",
      "{'loss': 0.3991, 'grad_norm': 183.7718963623047, 'learning_rate': 7.089226661168624e-07, 'epoch': 4.18}\n",
      "{'loss': 0.4, 'grad_norm': 0.056669510900974274, 'learning_rate': 5.76872414411971e-07, 'epoch': 4.26}\n",
      "{'loss': 0.4622, 'grad_norm': 64.31035614013672, 'learning_rate': 4.5767687041554545e-07, 'epoch': 4.35}\n",
      "{'loss': 0.3811, 'grad_norm': 53.15037155151367, 'learning_rate': 3.516804116206939e-07, 'epoch': 4.43}\n",
      "{'loss': 0.3977, 'grad_norm': 18.294050216674805, 'learning_rate': 2.5953197557920225e-07, 'epoch': 4.51}\n",
      "{'loss': 0.4058, 'grad_norm': 22.009178161621094, 'learning_rate': 1.8075782668106538e-07, 'epoch': 4.59}\n",
      "{'loss': 0.3643, 'grad_norm': 193.48355102539062, 'learning_rate': 1.1598283220586932e-07, 'epoch': 4.67}\n",
      "{'loss': 0.4211, 'grad_norm': 26.29131507873535, 'learning_rate': 6.53941388326394e-08, 'epoch': 4.76}\n",
      "{'loss': 0.3561, 'grad_norm': 55.79053497314453, 'learning_rate': 2.91379064497358e-08, 'epoch': 4.84}\n",
      "{'loss': 0.4016, 'grad_norm': 0.036162566393613815, 'learning_rate': 7.318885872491524e-09, 'epoch': 4.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b80c2d816e84e51937fa05fe544cebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6733362331448456, 'eval_f1': 0.6759954248736463, 'eval_loss': 2.0366921424865723, 'eval_runtime': 82.0636, 'eval_samples_per_second': 28.015, 'eval_steps_per_second': 9.346, 'epoch': 5.0}\n",
      "{'train_runtime': 11591.6283, 'train_samples_per_second': 7.889, 'train_steps_per_second': 2.63, 'train_loss': 0.7826298665351236, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957ac4f5e7054ff1900d8fd70e078fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-17 10:36:00,662] Trial 6 finished with value: 0.6733362331448456 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 5 with value: 0.6824706394084384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 7 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 2.7102291331781023e-06, 'batch_size': 4, 'warmup_ratio': 0.0871495018092514, 'weight_decay': 0.08376945393642041, 'adam_beta1': 0.8976771551751836, 'adam_beta2': 0.9922976863312725, 'adam_epsilon': 6.461136389064594e-07, 'lr_scheduler_type': 'cosine'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0bf5fbfb6140e39a1596109c54f69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22865 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6107, 'grad_norm': 1.969455361366272, 'learning_rate': 6.799370630150783e-07, 'epoch': 0.11}\n",
      "{'loss': 1.5503, 'grad_norm': 4.61126708984375, 'learning_rate': 1.3585142519041265e-06, 'epoch': 0.22}\n",
      "{'loss': 1.4651, 'grad_norm': 18.674701690673828, 'learning_rate': 2.0343716925411145e-06, 'epoch': 0.33}\n",
      "{'loss': 1.43, 'grad_norm': 10.058382034301758, 'learning_rate': 2.7102289950251617e-06, 'epoch': 0.44}\n",
      "{'loss': 1.415, 'grad_norm': 12.71101188659668, 'learning_rate': 2.706347217095712e-06, 'epoch': 0.55}\n",
      "{'loss': 1.4003, 'grad_norm': 11.993844985961914, 'learning_rate': 2.6948158737756076e-06, 'epoch': 0.66}\n",
      "{'loss': 1.3736, 'grad_norm': 21.316970825195312, 'learning_rate': 2.675700246024967e-06, 'epoch': 0.77}\n",
      "{'loss': 1.3569, 'grad_norm': 12.652982711791992, 'learning_rate': 2.6491691021865293e-06, 'epoch': 0.87}\n",
      "{'loss': 1.3312, 'grad_norm': 15.776825904846191, 'learning_rate': 2.6152663527710614e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41d04464c104d4eabaf1f4dfbb72ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4197477163984341, 'eval_f1': 0.418994982174714, 'eval_loss': 1.2638765573501587, 'eval_runtime': 122.7548, 'eval_samples_per_second': 18.728, 'eval_steps_per_second': 4.684, 'epoch': 1.0}\n",
      "{'loss': 1.299, 'grad_norm': 33.74323654174805, 'learning_rate': 2.5742296627370517e-06, 'epoch': 1.09}\n",
      "{'loss': 1.3051, 'grad_norm': 13.195956230163574, 'learning_rate': 2.526291347995245e-06, 'epoch': 1.2}\n",
      "{'loss': 1.2894, 'grad_norm': 21.00675392150879, 'learning_rate': 2.4718383505988317e-06, 'epoch': 1.31}\n",
      "{'loss': 1.2724, 'grad_norm': 37.94491958618164, 'learning_rate': 2.4109607932708704e-06, 'epoch': 1.42}\n",
      "{'loss': 1.2248, 'grad_norm': 24.363128662109375, 'learning_rate': 2.344105904984801e-06, 'epoch': 1.53}\n",
      "{'loss': 1.2372, 'grad_norm': 46.987159729003906, 'learning_rate': 2.271652163006873e-06, 'epoch': 1.64}\n",
      "{'loss': 1.2278, 'grad_norm': 36.3315315246582, 'learning_rate': 2.194169916339691e-06, 'epoch': 1.75}\n",
      "{'loss': 1.2346, 'grad_norm': 21.15753746032715, 'learning_rate': 2.1117874023878438e-06, 'epoch': 1.86}\n",
      "{'loss': 1.2496, 'grad_norm': 12.734779357910156, 'learning_rate': 2.025121230596168e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fc58b9cb5a4912ac2889b835c11223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.4810787298825576, 'eval_f1': 0.480769738520469, 'eval_loss': 1.1494512557983398, 'eval_runtime': 108.5904, 'eval_samples_per_second': 21.171, 'eval_steps_per_second': 5.295, 'epoch': 2.0}\n",
      "{'loss': 1.1635, 'grad_norm': 12.913232803344727, 'learning_rate': 1.9346620333868667e-06, 'epoch': 2.08}\n",
      "{'loss': 1.1512, 'grad_norm': 24.09821128845215, 'learning_rate': 1.841112320828192e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1273, 'grad_norm': 42.238067626953125, 'learning_rate': 1.7446269228696428e-06, 'epoch': 2.3}\n",
      "{'loss': 1.1456, 'grad_norm': 35.595550537109375, 'learning_rate': 1.6459364270408495e-06, 'epoch': 2.41}\n",
      "{'loss': 1.1086, 'grad_norm': 27.671863555908203, 'learning_rate': 1.5455995375745695e-06, 'epoch': 2.51}\n",
      "{'loss': 1.0868, 'grad_norm': 28.301483154296875, 'learning_rate': 1.4441842792271468e-06, 'epoch': 2.62}\n",
      "{'loss': 1.1415, 'grad_norm': 31.132102966308594, 'learning_rate': 1.342468740463127e-06, 'epoch': 2.73}\n",
      "{'loss': 1.1231, 'grad_norm': 64.490234375, 'learning_rate': 1.2406212661261176e-06, 'epoch': 2.84}\n",
      "{'loss': 1.1202, 'grad_norm': 50.35445022583008, 'learning_rate': 1.139421958474804e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce537d2cfe6410e8206da49bcb42960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.5176163549369291, 'eval_f1': 0.518378985518002, 'eval_loss': 1.1023606061935425, 'eval_runtime': 130.0531, 'eval_samples_per_second': 17.677, 'eval_steps_per_second': 4.421, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-17 14:46:12,377] Trial 7 failed with parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 2.7102291331781023e-06, 'batch_size': 4, 'warmup_ratio': 0.0871495018092514, 'weight_decay': 0.08376945393642041, 'adam_beta1': 0.8976771551751836, 'adam_beta2': 0.9922976863312725, 'adam_epsilon': 6.461136389064594e-07, 'lr_scheduler_type': 'cosine'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\OEM\\AppData\\Local\\Temp\\ipykernel_27760\\1569860036.py\", line 53, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 2279, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 3349, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py\", line 2192, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\_tensor.py\", line 521, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 289, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\graph.py\", line 769, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-17 14:46:12,383] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m study\u001b[38;5;241m.\u001b[39menqueue_trial({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v3-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m9.891138752479374e-06\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5982282303832456\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.17633588993115804\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8747290421857349\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9927786970263835\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m9.90768817706196e-07\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_scheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 4\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 53\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     42\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     43\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init(model_name),\n\u001b[0;32m     44\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[AdvancedEarlyStoppingCallback(metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.35\u001b[39m)]\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     56\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.enqueue_trial({'model_name':\"microsoft/deberta-v3-base\", 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'})\n",
    "study.optimize(objective, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4db3c9",
   "metadata": {},
   "source": [
    "## 4.3 Optuna Hyperparameters Tuning 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597dbbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 01:48:33,486] A new study created in memory with name: no-name-48d19931-bfa9-4c3d-b317-7013bfdd5859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 0 parameters: {'model_name': './MCQA-Combined/Optuna/trial_5/checkpoint-22865', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5314ce67c4a143f49daa885e8a51d04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2705, 'grad_norm': 99.15497589111328, 'learning_rate': 9.514940314205971e-08, 'epoch': 0.11}\n",
      "{'loss': 0.2782, 'grad_norm': 0.18223179876804352, 'learning_rate': 1.9125991136636245e-07, 'epoch': 0.22}\n",
      "{'loss': 0.3224, 'grad_norm': 1.258844256401062, 'learning_rate': 2.873704195906652e-07, 'epoch': 0.33}\n",
      "{'loss': 0.302, 'grad_norm': 67.60765838623047, 'learning_rate': 3.8348092781496795e-07, 'epoch': 0.44}\n",
      "{'loss': 0.2937, 'grad_norm': 0.0005371381412260234, 'learning_rate': 4.793992150228221e-07, 'epoch': 0.55}\n",
      "{'loss': 0.3094, 'grad_norm': 0.0005397977656684816, 'learning_rate': 5.755097232471248e-07, 'epoch': 0.66}\n",
      "{'loss': 0.3291, 'grad_norm': 14.216509819030762, 'learning_rate': 6.716202314714275e-07, 'epoch': 0.77}\n",
      "{'loss': 0.3382, 'grad_norm': 1.9903104305267334, 'learning_rate': 7.677307396957303e-07, 'epoch': 0.87}\n",
      "{'loss': 0.3188, 'grad_norm': 0.03658886253833771, 'learning_rate': 8.636490269035844e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5754e48d17c748bda9842bd43302719f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.683340582862114, 'eval_f1': 0.6857499996570497, 'eval_loss': 1.9124321937561035, 'eval_runtime': 94.5023, 'eval_samples_per_second': 24.327, 'eval_steps_per_second': 6.085, 'epoch': 1.0}\n",
      "{'loss': 0.3088, 'grad_norm': 16.608346939086914, 'learning_rate': 9.597595351278871e-07, 'epoch': 1.09}\n",
      "{'loss': 0.3221, 'grad_norm': 22.766817092895508, 'learning_rate': 1.05587004335219e-06, 'epoch': 1.2}\n",
      "{'loss': 0.3189, 'grad_norm': 10.460604667663574, 'learning_rate': 1.1519805515764925e-06, 'epoch': 1.31}\n",
      "{'loss': 0.3254, 'grad_norm': 1.3408642189460807e-05, 'learning_rate': 1.2480910598007954e-06, 'epoch': 1.42}\n",
      "{'loss': 0.3043, 'grad_norm': 1.1166165769793679e-09, 'learning_rate': 1.3442015680250982e-06, 'epoch': 1.53}\n",
      "{'loss': 0.2966, 'grad_norm': 50.4769401550293, 'learning_rate': 1.4403120762494008e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2969, 'grad_norm': 10.994010925292969, 'learning_rate': 1.5364225844737036e-06, 'epoch': 1.75}\n",
      "{'loss': 0.2914, 'grad_norm': 0.002418143441900611, 'learning_rate': 1.6325330926980064e-06, 'epoch': 1.86}\n",
      "{'loss': 0.3092, 'grad_norm': 19.943387985229492, 'learning_rate': 1.7284513799058604e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51400a0e76d14814a82a512fc8e61b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6963897346672466, 'eval_f1': 0.6987776690853735, 'eval_loss': 2.284477472305298, 'eval_runtime': 94.5198, 'eval_samples_per_second': 24.323, 'eval_steps_per_second': 6.083, 'epoch': 2.0}\n",
      "{'loss': 0.2865, 'grad_norm': 289.2450256347656, 'learning_rate': 1.8245618881301632e-06, 'epoch': 2.08}\n",
      "{'loss': 0.253, 'grad_norm': 0.013860290870070457, 'learning_rate': 1.9206723963544663e-06, 'epoch': 2.19}\n",
      "{'loss': 0.2606, 'grad_norm': 19.288745880126953, 'learning_rate': 2.016782904578769e-06, 'epoch': 2.3}\n",
      "{'loss': 0.2748, 'grad_norm': 26.506500244140625, 'learning_rate': 2.1128934128030715e-06, 'epoch': 2.41}\n",
      "{'loss': 0.267, 'grad_norm': 76.48756408691406, 'learning_rate': 2.209003921027374e-06, 'epoch': 2.51}\n",
      "{'loss': 0.2425, 'grad_norm': 35.718902587890625, 'learning_rate': 2.305114429251677e-06, 'epoch': 2.62}\n",
      "{'loss': 0.2879, 'grad_norm': 5.830728054046631, 'learning_rate': 2.4012249374759797e-06, 'epoch': 2.73}\n",
      "{'loss': 0.321, 'grad_norm': 0.0029808077961206436, 'learning_rate': 2.4973354457002827e-06, 'epoch': 2.84}\n",
      "{'loss': 0.3373, 'grad_norm': 0.8553935289382935, 'learning_rate': 2.593061511891688e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6334e8681204ef7b6ac0896edb6cfa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6911700739451936, 'eval_f1': 0.6930771667119316, 'eval_loss': 2.2938365936279297, 'eval_runtime': 94.5937, 'eval_samples_per_second': 24.304, 'eval_steps_per_second': 6.079, 'epoch': 3.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 10293.2707, 'train_samples_per_second': 53.307, 'train_steps_per_second': 13.328, 'train_loss': 0.29866341761123233, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba811ecd60240a3b414cce5ebc23efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 04:41:41,547] Trial 0 finished with value: 0.6963897346672466 and parameters: {'model_name': './MCQA-Combined/Optuna/trial_5/checkpoint-22865', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.6963897346672466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Current Trial 1 parameters: {'model_name': './MCQA-Combined/Optuna/trial_0/checkpoint-30485', 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c31f8fbd23743e39192dae145b01daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.375, 'grad_norm': 2.2848993808111118e-07, 'learning_rate': 4.465484586029145e-08, 'epoch': 0.08}\n",
      "{'loss': 0.3694, 'grad_norm': 264.77593994140625, 'learning_rate': 8.985205826949333e-08, 'epoch': 0.16}\n",
      "{'loss': 0.3431, 'grad_norm': 126.29911041259766, 'learning_rate': 1.350492706786952e-07, 'epoch': 0.25}\n",
      "{'loss': 0.3766, 'grad_norm': 0.0011504247086122632, 'learning_rate': 1.8024648308789708e-07, 'epoch': 0.33}\n",
      "{'loss': 0.45, 'grad_norm': 2.490936040878296, 'learning_rate': 2.2544369549709894e-07, 'epoch': 0.41}\n",
      "{'loss': 0.3445, 'grad_norm': 0.02701166644692421, 'learning_rate': 2.706409079063008e-07, 'epoch': 0.49}\n",
      "{'loss': 0.3742, 'grad_norm': 22.46174430847168, 'learning_rate': 3.158381203155027e-07, 'epoch': 0.57}\n",
      "{'loss': 0.4265, 'grad_norm': 3.9365717384498566e-05, 'learning_rate': 3.610353327247045e-07, 'epoch': 0.66}\n",
      "{'loss': 0.4518, 'grad_norm': 23.836828231811523, 'learning_rate': 4.060517562842696e-07, 'epoch': 0.74}\n",
      "{'loss': 0.3876, 'grad_norm': 0.36213865876197815, 'learning_rate': 4.5124896869347147e-07, 'epoch': 0.82}\n",
      "{'loss': 0.4224, 'grad_norm': 0.21893170475959778, 'learning_rate': 4.964461811026734e-07, 'epoch': 0.9}\n",
      "{'loss': 0.3923, 'grad_norm': 0.1287842094898224, 'learning_rate': 5.416433935118752e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fef5e2290f44e1b6b7e26513e1454c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6746411483253588, 'eval_f1': 0.6783092901670232, 'eval_loss': 1.972519040107727, 'eval_runtime': 82.756, 'eval_samples_per_second': 27.78, 'eval_steps_per_second': 9.268, 'epoch': 1.0}\n",
      "{'loss': 0.4086, 'grad_norm': 168.22952270507812, 'learning_rate': 5.867502114962587e-07, 'epoch': 1.07}\n",
      "{'loss': 0.3838, 'grad_norm': 0.47670459747314453, 'learning_rate': 6.319474239054606e-07, 'epoch': 1.15}\n",
      "{'loss': 0.4116, 'grad_norm': 183.18838500976562, 'learning_rate': 6.771446363146624e-07, 'epoch': 1.23}\n",
      "{'loss': 0.4401, 'grad_norm': 35.64690017700195, 'learning_rate': 7.223418487238643e-07, 'epoch': 1.31}\n",
      "{'loss': 0.3961, 'grad_norm': 0.5133939385414124, 'learning_rate': 7.675390611330661e-07, 'epoch': 1.39}\n",
      "{'loss': 0.3798, 'grad_norm': 5.298861651681364e-06, 'learning_rate': 8.126458791174496e-07, 'epoch': 1.48}\n",
      "{'loss': 0.4052, 'grad_norm': 38.95729446411133, 'learning_rate': 8.578430915266515e-07, 'epoch': 1.56}\n",
      "{'loss': 0.3824, 'grad_norm': 23.23221778869629, 'learning_rate': 9.030403039358533e-07, 'epoch': 1.64}\n",
      "{'loss': 0.3679, 'grad_norm': 0.08535587787628174, 'learning_rate': 9.482375163450552e-07, 'epoch': 1.72}\n",
      "{'loss': 0.5002, 'grad_norm': 40.1038703918457, 'learning_rate': 9.93434728754257e-07, 'epoch': 1.8}\n",
      "{'loss': 0.3919, 'grad_norm': 1.598442554473877, 'learning_rate': 1.038631941163459e-06, 'epoch': 1.89}\n",
      "{'loss': 0.3901, 'grad_norm': 0.0006696955533698201, 'learning_rate': 1.0837387591478424e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aedf8dcd16e44fb8a62d19ab3a9f855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6755110917790343, 'eval_f1': 0.6781748576728076, 'eval_loss': 2.0128607749938965, 'eval_runtime': 82.5231, 'eval_samples_per_second': 27.859, 'eval_steps_per_second': 9.294, 'epoch': 2.0}\n",
      "{'loss': 0.4027, 'grad_norm': 52.395896911621094, 'learning_rate': 1.1289359715570443e-06, 'epoch': 2.05}\n",
      "{'loss': 0.392, 'grad_norm': 2.4953598976135254, 'learning_rate': 1.1741331839662462e-06, 'epoch': 2.13}\n",
      "{'loss': 0.3836, 'grad_norm': 24.53590202331543, 'learning_rate': 1.219330396375448e-06, 'epoch': 2.21}\n",
      "{'loss': 0.3746, 'grad_norm': 35.88473892211914, 'learning_rate': 1.26452760878465e-06, 'epoch': 2.3}\n",
      "{'loss': 0.375, 'grad_norm': 7.686323165893555, 'learning_rate': 1.3097248211938518e-06, 'epoch': 2.38}\n",
      "{'loss': 0.3613, 'grad_norm': 0.017652036622166634, 'learning_rate': 1.3548316391782352e-06, 'epoch': 2.46}\n",
      "{'loss': 0.3839, 'grad_norm': 0.1621805727481842, 'learning_rate': 1.4000288515874371e-06, 'epoch': 2.54}\n",
      "{'loss': 0.3689, 'grad_norm': 16.461877822875977, 'learning_rate': 1.445226063996639e-06, 'epoch': 2.62}\n",
      "{'loss': 0.3773, 'grad_norm': 75.58336639404297, 'learning_rate': 1.490423276405841e-06, 'epoch': 2.71}\n",
      "{'loss': 0.462, 'grad_norm': 0.037843331694602966, 'learning_rate': 1.5355300943902243e-06, 'epoch': 2.79}\n",
      "{'loss': 0.4008, 'grad_norm': 1.1928247213363647, 'learning_rate': 1.580727306799426e-06, 'epoch': 2.87}\n",
      "{'loss': 0.4276, 'grad_norm': 4.909253402729519e-05, 'learning_rate': 1.6259245192086279e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70785f5e28bb4deea91bd75db15cc335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6859504132231405, 'eval_f1': 0.6884094783241329, 'eval_loss': 2.1828103065490723, 'eval_runtime': 82.6416, 'eval_samples_per_second': 27.819, 'eval_steps_per_second': 9.281, 'epoch': 3.0}\n",
      "{'loss': 0.3601, 'grad_norm': 0.02080305479466915, 'learning_rate': 1.6711217316178298e-06, 'epoch': 3.03}\n",
      "{'loss': 0.329, 'grad_norm': 0.0005194161203689873, 'learning_rate': 1.7162285496022134e-06, 'epoch': 3.12}\n",
      "{'loss': 0.3386, 'grad_norm': 138.39730834960938, 'learning_rate': 1.7614257620114153e-06, 'epoch': 3.2}\n",
      "{'loss': 0.3487, 'grad_norm': 15.076576232910156, 'learning_rate': 1.8066229744206172e-06, 'epoch': 3.28}\n",
      "{'loss': 0.3628, 'grad_norm': 0.3702249526977539, 'learning_rate': 1.851820186829819e-06, 'epoch': 3.36}\n",
      "{'loss': 0.2977, 'grad_norm': 26.885936737060547, 'learning_rate': 1.8969270048142025e-06, 'epoch': 3.44}\n",
      "{'loss': 0.3142, 'grad_norm': 30.011932373046875, 'learning_rate': 1.942124217223404e-06, 'epoch': 3.53}\n",
      "{'loss': 0.3755, 'grad_norm': 46.05055618286133, 'learning_rate': 1.9873214296326063e-06, 'epoch': 3.61}\n",
      "{'loss': 0.391, 'grad_norm': 92.55567169189453, 'learning_rate': 2.032518642041808e-06, 'epoch': 3.69}\n",
      "{'loss': 0.3675, 'grad_norm': 8.8368558883667, 'learning_rate': 2.0776254600261914e-06, 'epoch': 3.77}\n",
      "{'loss': 0.4355, 'grad_norm': 2.171920277760364e-05, 'learning_rate': 2.1228226724353935e-06, 'epoch': 3.85}\n",
      "{'loss': 0.4001, 'grad_norm': 82.46470642089844, 'learning_rate': 2.167929490419777e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac811a9121244b83a5c5ca719844a0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6911700739451936, 'eval_f1': 0.6935897092331728, 'eval_loss': 2.3350729942321777, 'eval_runtime': 82.6281, 'eval_samples_per_second': 27.823, 'eval_steps_per_second': 9.283, 'epoch': 4.0}\n",
      "{'loss': 0.3271, 'grad_norm': 40.06705856323242, 'learning_rate': 2.213126702828979e-06, 'epoch': 4.02}\n",
      "{'loss': 0.3236, 'grad_norm': 30.957353591918945, 'learning_rate': 2.2583239152381807e-06, 'epoch': 4.1}\n",
      "{'loss': 0.321, 'grad_norm': 5.337359887391813e-09, 'learning_rate': 2.3035211276473828e-06, 'epoch': 4.18}\n",
      "{'loss': 0.329, 'grad_norm': 27.053117752075195, 'learning_rate': 2.3487183400565844e-06, 'epoch': 4.26}\n",
      "{'loss': 0.3242, 'grad_norm': 0.3099638521671295, 'learning_rate': 2.3939155524657865e-06, 'epoch': 4.35}\n",
      "{'loss': 0.2886, 'grad_norm': 0.00022847039508633316, 'learning_rate': 2.439112764874988e-06, 'epoch': 4.43}\n",
      "{'loss': 0.324, 'grad_norm': 1.5601292848587036, 'learning_rate': 2.4843099772841903e-06, 'epoch': 4.51}\n",
      "{'loss': 0.3594, 'grad_norm': 0.0007065070676617324, 'learning_rate': 2.5294167952685733e-06, 'epoch': 4.59}\n",
      "{'loss': 0.3275, 'grad_norm': 232.66778564453125, 'learning_rate': 2.5746140076777754e-06, 'epoch': 4.67}\n",
      "{'loss': 0.3773, 'grad_norm': 25.536983489990234, 'learning_rate': 2.619811220086977e-06, 'epoch': 4.76}\n",
      "{'loss': 0.3642, 'grad_norm': 3.494884159849221e-09, 'learning_rate': 2.665008432496179e-06, 'epoch': 4.84}\n",
      "{'loss': 0.3686, 'grad_norm': 64.76029205322266, 'learning_rate': 2.7101152504805626e-06, 'epoch': 4.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee1d8f70eb348c0af60f08db8c8a9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6911700739451936, 'eval_f1': 0.6939035399771951, 'eval_loss': 2.580587387084961, 'eval_runtime': 82.6908, 'eval_samples_per_second': 27.802, 'eval_steps_per_second': 9.276, 'epoch': 5.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 11595.5841, 'train_samples_per_second': 47.32, 'train_steps_per_second': 15.774, 'train_loss': 0.3771073816720826, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3975ad9324449d7b30b3063d982aeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 07:56:21,309] Trial 1 finished with value: 0.6911700739451936 and parameters: {'model_name': './MCQA-Combined/Optuna/trial_0/checkpoint-30485', 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6963897346672466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Current Trial 2 parameters: {'model_name': './MCQA-Combined/Optuna/trial_6/checkpoint-30485', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528b779719654a3eb46c3c0962595843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.445, 'grad_norm': 0.0, 'learning_rate': 6.542576538613509e-07, 'epoch': 0.08}\n",
      "{'loss': 0.3755, 'grad_norm': 7.25158166885376, 'learning_rate': 1.3164617569598844e-06, 'epoch': 0.16}\n",
      "{'loss': 0.3561, 'grad_norm': 125.56159210205078, 'learning_rate': 1.9786658600584178e-06, 'epoch': 0.25}\n",
      "{'loss': 0.4039, 'grad_norm': 43.95442199707031, 'learning_rate': 2.6408699631569514e-06, 'epoch': 0.33}\n",
      "{'loss': 0.4612, 'grad_norm': 2.5171901143039577e-05, 'learning_rate': 3.3017496580492876e-06, 'epoch': 0.41}\n",
      "{'loss': 0.374, 'grad_norm': 0.1422954499721527, 'learning_rate': 3.963953761147821e-06, 'epoch': 0.49}\n",
      "{'loss': 0.4267, 'grad_norm': 36.26209259033203, 'learning_rate': 4.626157864246355e-06, 'epoch': 0.57}\n",
      "{'loss': 0.4454, 'grad_norm': 0.000985930673778057, 'learning_rate': 5.288361967344887e-06, 'epoch': 0.66}\n",
      "{'loss': 0.5264, 'grad_norm': 31.491241455078125, 'learning_rate': 5.95056607044342e-06, 'epoch': 0.74}\n",
      "{'loss': 0.466, 'grad_norm': 19.953073501586914, 'learning_rate': 6.612770173541955e-06, 'epoch': 0.82}\n",
      "{'loss': 0.5157, 'grad_norm': 16.81511116027832, 'learning_rate': 7.274974276640488e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5209, 'grad_norm': 0.0014625568874180317, 'learning_rate': 7.937178379739021e-06, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e832795e90044b9a4774e0e06a08147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6681165724227925, 'eval_f1': 0.6712466084338798, 'eval_loss': 2.0135676860809326, 'eval_runtime': 84.2165, 'eval_samples_per_second': 27.299, 'eval_steps_per_second': 9.107, 'epoch': 1.0}\n",
      "{'loss': 0.456, 'grad_norm': 5.765288352966309, 'learning_rate': 8.598058074631356e-06, 'epoch': 1.07}\n",
      "{'loss': 0.4585, 'grad_norm': 0.012237763032317162, 'learning_rate': 9.260262177729892e-06, 'epoch': 1.15}\n",
      "{'loss': 0.5119, 'grad_norm': 150.01853942871094, 'learning_rate': 9.922466280828424e-06, 'epoch': 1.23}\n",
      "{'loss': 0.5092, 'grad_norm': 19.789443969726562, 'learning_rate': 1.0052127881100565e-05, 'epoch': 1.31}\n",
      "{'loss': 0.578, 'grad_norm': 0.06391139328479767, 'learning_rate': 1.0051603225735257e-05, 'epoch': 1.39}\n",
      "{'loss': 0.4865, 'grad_norm': 0.017514964565634727, 'learning_rate': 1.0050674508406943e-05, 'epoch': 1.48}\n",
      "{'loss': 0.5037, 'grad_norm': 38.18337631225586, 'learning_rate': 1.0049342451633382e-05, 'epoch': 1.56}\n",
      "{'loss': 0.5309, 'grad_norm': 110.04790496826172, 'learning_rate': 1.004760716234386e-05, 'epoch': 1.64}\n",
      "{'loss': 0.5632, 'grad_norm': 1.016189694404602, 'learning_rate': 1.0045473458780014e-05, 'epoch': 1.72}\n",
      "{'loss': 0.5676, 'grad_norm': 43.34209442138672, 'learning_rate': 1.0042932960361606e-05, 'epoch': 1.8}\n",
      "{'loss': 0.527, 'grad_norm': 96.03111267089844, 'learning_rate': 1.0039989743941577e-05, 'epoch': 1.89}\n",
      "{'loss': 0.5706, 'grad_norm': 0.0011687733931466937, 'learning_rate': 1.0036644045783152e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8a2a110bf74fbc92dff0f65b848087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6733362331448456, 'eval_f1': 0.6743090396682621, 'eval_loss': 2.1987509727478027, 'eval_runtime': 84.2601, 'eval_samples_per_second': 27.285, 'eval_steps_per_second': 9.103, 'epoch': 2.0}\n",
      "{'loss': 0.5713, 'grad_norm': 42.225181579589844, 'learning_rate': 1.00328961344583e-05, 'epoch': 2.05}\n",
      "{'loss': 0.4159, 'grad_norm': 0.39606156945228577, 'learning_rate': 1.0028746310826174e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4835, 'grad_norm': 0.4710776209831238, 'learning_rate': 1.0024194908008965e-05, 'epoch': 2.21}\n",
      "{'loss': 0.4383, 'grad_norm': 54.79790115356445, 'learning_rate': 1.001924229136516e-05, 'epoch': 2.3}\n",
      "{'loss': 0.422, 'grad_norm': 87.28656768798828, 'learning_rate': 1.0013899965060368e-05, 'epoch': 2.38}\n",
      "{'loss': 0.4584, 'grad_norm': 1.9011500626220368e-05, 'learning_rate': 1.0008146945957569e-05, 'epoch': 2.46}\n",
      "{'loss': 0.3971, 'grad_norm': 1.7128010988235474, 'learning_rate': 1.0001994001259376e-05, 'epoch': 2.54}\n",
      "{'loss': 0.4821, 'grad_norm': 6.297253131866455, 'learning_rate': 9.995441624886154e-06, 'epoch': 2.62}\n",
      "{'loss': 0.4876, 'grad_norm': 193.06771850585938, 'learning_rate': 9.988504643128629e-06, 'epoch': 2.71}\n",
      "{'loss': 0.522, 'grad_norm': 1.7583880424499512, 'learning_rate': 9.98115580949708e-06, 'epoch': 2.79}\n",
      "{'loss': 0.4934, 'grad_norm': 175.49725341796875, 'learning_rate': 9.973409216951743e-06, 'epoch': 2.87}\n",
      "{'loss': 0.5207, 'grad_norm': 0.0020698816515505314, 'learning_rate': 9.96528217071398e-06, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed46996dd6fe47c3b9a4db3d2899eaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6859504132231405, 'eval_f1': 0.689047034019059, 'eval_loss': 2.972546100616455, 'eval_runtime': 96.1115, 'eval_samples_per_second': 23.92, 'eval_steps_per_second': 7.98, 'epoch': 3.0}\n",
      "{'loss': 0.4082, 'grad_norm': 0.004036388825625181, 'learning_rate': 9.956742750059558e-06, 'epoch': 3.03}\n",
      "{'loss': 0.3907, 'grad_norm': 0.017696648836135864, 'learning_rate': 9.947807530220855e-06, 'epoch': 3.12}\n",
      "{'loss': 0.4468, 'grad_norm': 0.003038992639631033, 'learning_rate': 9.938477228462098e-06, 'epoch': 3.2}\n",
      "{'loss': 0.4029, 'grad_norm': 47.63961410522461, 'learning_rate': 9.928752593762235e-06, 'epoch': 3.28}\n",
      "{'loss': 0.3898, 'grad_norm': 341.1247253417969, 'learning_rate': 9.918655035359445e-06, 'epoch': 3.36}\n",
      "{'loss': 0.4115, 'grad_norm': 104.11070251464844, 'learning_rate': 9.908144892918616e-06, 'epoch': 3.44}\n",
      "{'loss': 0.4193, 'grad_norm': 54.144039154052734, 'learning_rate': 9.897242852429097e-06, 'epoch': 3.53}\n",
      "{'loss': 0.3899, 'grad_norm': 85.18841552734375, 'learning_rate': 9.885949789039312e-06, 'epoch': 3.61}\n",
      "{'loss': 0.3605, 'grad_norm': 203.65823364257812, 'learning_rate': 9.874266609286588e-06, 'epoch': 3.69}\n",
      "{'loss': 0.3434, 'grad_norm': 2.570380797095595e-09, 'learning_rate': 9.862194251024382e-06, 'epoch': 3.77}\n",
      "{'loss': 0.3989, 'grad_norm': 0.0, 'learning_rate': 9.849733683346997e-06, 'epoch': 3.85}\n",
      "{'loss': 0.4503, 'grad_norm': 86.49512481689453, 'learning_rate': 9.836885906511783e-06, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a82c1373934f68a1c73ab7c23a2f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.6759460635058722, 'eval_f1': 0.6767204573821777, 'eval_loss': 3.629800319671631, 'eval_runtime': 86.6431, 'eval_samples_per_second': 26.534, 'eval_steps_per_second': 8.852, 'epoch': 4.0}\n",
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "{'train_runtime': 9487.1783, 'train_samples_per_second': 57.836, 'train_steps_per_second': 19.28, 'train_loss': 0.4590179983936452, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31210245093743f99ce6c53a3278e127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 10:35:55,111] Trial 2 finished with value: 0.6859504132231405 and parameters: {'model_name': './MCQA-Combined/Optuna/trial_6/checkpoint-30485', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.6963897346672466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Current Trial 3 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b46df85de74d388bc56c4add27338d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 2.020512819290161, 'learning_rate': 9.611050822430274e-08, 'epoch': 0.11}\n",
      "{'loss': 1.6101, 'grad_norm': 2.085341691970825, 'learning_rate': 1.9222101644860547e-07, 'epoch': 0.22}\n",
      "{'loss': 1.6122, 'grad_norm': 1.6762933731079102, 'learning_rate': 2.883315246729082e-07, 'epoch': 0.33}\n",
      "{'loss': 1.6112, 'grad_norm': 2.2332630157470703, 'learning_rate': 3.8444203289721095e-07, 'epoch': 0.44}\n",
      "{'loss': 1.5889, 'grad_norm': 1.8620704412460327, 'learning_rate': 4.803603201050651e-07, 'epoch': 0.55}\n",
      "{'loss': 1.5185, 'grad_norm': 6.934852123260498, 'learning_rate': 5.762786073129193e-07, 'epoch': 0.66}\n",
      "{'loss': 1.4754, 'grad_norm': 10.272505760192871, 'learning_rate': 6.720046735043248e-07, 'epoch': 0.77}\n",
      "{'loss': 1.4694, 'grad_norm': 8.192824363708496, 'learning_rate': 7.681151817286275e-07, 'epoch': 0.87}\n",
      "{'loss': 1.4503, 'grad_norm': 9.931807518005371, 'learning_rate': 8.640334689364817e-07, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6f991ba03a4a75b1da566d44fd1859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.3501522401043932, 'eval_f1': 0.3514140656822659, 'eval_loss': 1.3924517631530762, 'eval_runtime': 108.3799, 'eval_samples_per_second': 21.212, 'eval_steps_per_second': 5.305, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.4\n",
      "{'train_runtime': 3702.9615, 'train_samples_per_second': 148.179, 'train_steps_per_second': 37.049, 'train_loss': 1.5478570347095042, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db94076c9c3c43db86960850990bb0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 11:39:21,823] Trial 3 finished with value: 0.3501522401043932 and parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'}. Best is trial 0 with value: 0.6963897346672466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Trial 4 parameters: {'model_name': './squad-trained-model', 'learning_rate': 2.2913924741371357e-07, 'batch_size': 3, 'warmup_ratio': 0.6782879296196984, 'weight_decay': 0.04860407678647671, 'adam_beta1': 0.8164084430629853, 'adam_beta2': 0.9906390554740927, 'adam_epsilon': 6.695655421893776e-07, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3258258386343bfa764934c5306526f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/182910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.61, 'grad_norm': 2.1761417388916016, 'learning_rate': 9.19763232569998e-10, 'epoch': 0.08}\n",
      "{'loss': 1.6096, 'grad_norm': 2.805415153503418, 'learning_rate': 1.8432202933832488e-09, 'epoch': 0.16}\n",
      "{'loss': 1.6099, 'grad_norm': 2.8612136840820312, 'learning_rate': 2.7666773541964996e-09, 'epoch': 0.25}\n",
      "{'loss': 1.6118, 'grad_norm': 2.628831386566162, 'learning_rate': 3.6901344150097503e-09, 'epoch': 0.33}\n",
      "{'loss': 1.6102, 'grad_norm': 3.427438259124756, 'learning_rate': 4.613591475823001e-09, 'epoch': 0.41}\n",
      "{'loss': 1.613, 'grad_norm': 3.6097707748413086, 'learning_rate': 5.535201622514626e-09, 'epoch': 0.49}\n",
      "{'loss': 1.6107, 'grad_norm': 2.4153101444244385, 'learning_rate': 6.458658683327877e-09, 'epoch': 0.57}\n",
      "{'loss': 1.6116, 'grad_norm': 3.0242533683776855, 'learning_rate': 7.3821157441411286e-09, 'epoch': 0.66}\n",
      "{'loss': 1.61, 'grad_norm': 2.5526537895202637, 'learning_rate': 8.305572804954378e-09, 'epoch': 0.74}\n",
      "{'loss': 1.6113, 'grad_norm': 6.2558794021606445, 'learning_rate': 9.229029865767629e-09, 'epoch': 0.82}\n",
      "{'loss': 1.6116, 'grad_norm': 2.766200542449951, 'learning_rate': 1.015248692658088e-08, 'epoch': 0.9}\n",
      "{'loss': 1.6137, 'grad_norm': 2.5826902389526367, 'learning_rate': 1.1075943987394131e-08, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b3ce9dc908410eae947494547ac292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.1644193127446716, 'eval_f1': 0.1907047402973053, 'eval_loss': 1.6099488735198975, 'eval_runtime': 83.3643, 'eval_samples_per_second': 27.578, 'eval_steps_per_second': 9.201, 'epoch': 1.0}\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.4\n",
      "{'train_runtime': 2326.3998, 'train_samples_per_second': 235.858, 'train_steps_per_second': 78.624, 'train_loss': 1.6111839360754392, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011e33410e4849e6a715ca1ca09c336b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/767 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-18 12:19:34,622] Trial 4 finished with value: 0.1644193127446716 and parameters: {'model_name': './squad-trained-model', 'learning_rate': 2.2913924741371357e-07, 'batch_size': 3, 'warmup_ratio': 0.6782879296196984, 'weight_decay': 0.04860407678647671, 'adam_beta1': 0.8164084430629853, 'adam_beta2': 0.9906390554740927, 'adam_epsilon': 6.695655421893776e-07, 'lr_scheduler_type': 'linear'}. Best is trial 0 with value: 0.6963897346672466.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping training: No improvement in eval_accuracy for 1 epochs\n",
      "Stopping training: eval_accuracy below manual min_acc of 0.4\n",
      "Current Trial 5 parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.2781743075162943e-06, 'batch_size': 4, 'warmup_ratio': 0.6682042061326278, 'weight_decay': 0.11855116033570309, 'adam_beta1': 0.8482673079863498, 'adam_beta2': 0.9938674227791334, 'adam_epsilon': 8.072037956756882e-08, 'lr_scheduler_type': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37adc986e09c433eacb431559911ff3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6108, 'grad_norm': 1.8368643522262573, 'learning_rate': 6.971530295929434e-09, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-10-18 12:28:51,841] Trial 5 failed with parameters: {'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.2781743075162943e-06, 'batch_size': 4, 'warmup_ratio': 0.6682042061326278, 'weight_decay': 0.11855116033570309, 'adam_beta1': 0.8482673079863498, 'adam_beta2': 0.9938674227791334, 'adam_epsilon': 8.072037956756882e-08, 'lr_scheduler_type': 'linear'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\OEM\\AppData\\Local\\Temp\\ipykernel_4464\\1873140353.py\", line 59, in objective\n",
      "    trainer.train()\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 1938, in train\n",
      "    return inner_training_loop(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 2279, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 3318, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py\", line 3363, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 820, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py\", line 808, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 43, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 1599, in forward\n",
      "    outputs = self.deberta(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 1063, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 507, in forward\n",
      "    output_states = layer_module(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 355, in forward\n",
      "    attention_output = self.attention(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 286, in forward\n",
      "    self_output = self.self(\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 699, in forward\n",
      "    query_layer = self.transpose_for_scores(self.query_proj(query_states), self.num_attention_heads)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-18 12:28:51,846] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m study\u001b[38;5;241m.\u001b[39menqueue_trial({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./MCQA-Combined/Optuna/trial_6/checkpoint-30485\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.0052258285035737e-05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.04149176551014211\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.006685281279171756\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9429922176765829\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9918592948813898\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8.867767549079712e-08\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_scheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_with_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m      7\u001b[0m study\u001b[38;5;241m.\u001b[39menqueue_trial({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v3-base\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.5807103066634623e-05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5994150649377659\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.12506835879573128\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta1\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.8136227307274486\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_beta2\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9924116710027883\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.9858068243318367e-07\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_scheduler_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine_with_restarts\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m----> 9\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[13], line 59\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     49\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init(model_name),\n\u001b[0;32m     50\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[AdvancedEarlyStoppingCallback(metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     62\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3318\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3318\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3323\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3324\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3363\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3362\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3363\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\utils\\operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\amp\\autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1599\u001b[0m, in \u001b[0;36mDebertaV2ForMultipleChoice.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1592\u001b[0m flat_attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attention_mask\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1593\u001b[0m flat_inputs_embeds \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1594\u001b[0m     inputs_embeds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), inputs_embeds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m )\n\u001b[1;32m-> 1599\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1610\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1611\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1063\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1055\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1056\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1057\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1061\u001b[0m )\n\u001b[1;32m-> 1063\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:507\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    497\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    498\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    499\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m         output_attentions,\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    517\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:355\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    348\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m ):\n\u001b[1;32m--> 355\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    364\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:286\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    285\u001b[0m ):\n\u001b[1;32m--> 286\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    295\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:699\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    698\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 699\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[0;32m    700\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_proj(hidden_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n\u001b[0;32m    701\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_proj(hidden_states), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a study object and optimize the objective\n",
    "study = optuna.create_study(direction='maximize')\n",
    "#study.enqueue_trial({'model_name': './squad-trained-model', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 3, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.enqueue_trial({'model_name': './MCQA-Combined/Optuna/trial_5/checkpoint-22865', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.enqueue_trial({'model_name': './MCQA-Combined/Optuna/trial_0/checkpoint-30485', 'learning_rate': 9.891138752479374e-06, 'batch_size': 3, 'warmup_ratio': 0.5982282303832456, 'weight_decay': 0.17633588993115804, 'adam_beta1': 0.8747290421857349, 'adam_beta2': 0.9927786970263835, 'adam_epsilon': 9.90768817706196e-07, 'lr_scheduler_type': 'linear'})\n",
    "study.enqueue_trial({'model_name': './MCQA-Combined/Optuna/trial_6/checkpoint-30485', 'learning_rate': 1.0052258285035737e-05, 'batch_size': 3, 'warmup_ratio': 0.04149176551014211, 'weight_decay': 0.006685281279171756, 'adam_beta1': 0.9429922176765829, 'adam_beta2': 0.9918592948813898, 'adam_epsilon': 8.867767549079712e-08, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "study.enqueue_trial({'model_name': 'microsoft/deberta-v3-base', 'learning_rate': 1.5807103066634623e-05, 'batch_size': 4, 'warmup_ratio': 0.5994150649377659, 'weight_decay': 0.12506835879573128, 'adam_beta1': 0.8136227307274486, 'adam_beta2': 0.9924116710027883, 'adam_epsilon': 1.9858068243318367e-07, 'lr_scheduler_type': 'cosine_with_restarts'})\n",
    "\n",
    "study.optimize(objective, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0629740",
   "metadata": {},
   "source": [
    "## 4.4 Optuna Hyperparameters Tuning 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b567c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a446eb39",
   "metadata": {},
   "source": [
    "## 4.4 Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c58679df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88acdf74d9a84523b31b136f3262f4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.6484\n",
      "F1 Score: 0.6501\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = create_trainer(model_name=\"./MCQA-Combined/Optuna/trial_5/checkpoint-22865\", run_name=\"Optuna\")\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02783c0c-6c08-42fc-a741-705ff1bac74b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 4.3 Evaluate SQUAD DeBERTa (Acc=21.9%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90877cd6-3ca5-4cc4-8a77-a755a0d6896a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fbc41000154328a25323df4a1a83e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241016_105610-d1a17evf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/d1a17evf' target=\"_blank\">./MCQA-Combined/Squad-Run/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/d1a17evf' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/d1a17evf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.2190\n",
      "F1 Score: 0.2305\n"
     ]
    }
   ],
   "source": [
    "# Create the Trainer\n",
    "trainer = create_trainer(model_name=\"./squad-trained-model\", run_name=\"Squad-Run\")\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33aceb-a791-4e83-a2f4-cec16d086dfe",
   "metadata": {},
   "source": [
    "## 4.4 Evaluate Trained SQUAD DeBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63fad85-dd7b-4143-aefe-ad0995cca802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at ./squad-trained-model and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241016_032050-5ptlux0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/5ptlux0n' target=\"_blank\">./MCQA-Combined/Squad-Run2/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/5ptlux0n' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/5ptlux0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b624ec312e94bc8adde7e14346a96e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5257, 'grad_norm': 7.596715450286865, 'learning_rate': 4.919968223047422e-05, 'epoch': 0.08}\n",
      "{'loss': 1.5156, 'grad_norm': 9.40099811553955, 'learning_rate': 4.918816880581236e-05, 'epoch': 0.16}\n",
      "{'loss': 1.4678, 'grad_norm': 3.4478559494018555, 'learning_rate': 4.916012062578594e-05, 'epoch': 0.25}\n",
      "{'loss': 1.5513, 'grad_norm': 17.781841278076172, 'learning_rate': 4.911566214483556e-05, 'epoch': 0.33}\n",
      "{'loss': 1.539, 'grad_norm': 2.640469789505005, 'learning_rate': 4.905492193723485e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5319, 'grad_norm': 4.278200626373291, 'learning_rate': 4.897752559048373e-05, 'epoch': 0.49}\n",
      "{'loss': 1.6115, 'grad_norm': 4.821766376495361, 'learning_rate': 4.8883736139178334e-05, 'epoch': 0.57}\n",
      "{'loss': 1.6179, 'grad_norm': 2.340773344039917, 'learning_rate': 4.877361665371831e-05, 'epoch': 0.66}\n",
      "{'loss': 1.6139, 'grad_norm': 1.792155146598816, 'learning_rate': 4.864724118592782e-05, 'epoch': 0.74}\n",
      "{'loss': 1.612, 'grad_norm': 1.8655813932418823, 'learning_rate': 4.8504694719258075e-05, 'epoch': 0.82}\n",
      "{'loss': 1.6176, 'grad_norm': 3.0023794174194336, 'learning_rate': 4.834607311163868e-05, 'epoch': 0.9}\n",
      "{'loss': 1.5723, 'grad_norm': 6.832743167877197, 'learning_rate': 4.817148303101623e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce070fae00841da93d32c27f8d2bd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 84.1784, 'eval_samples_per_second': 27.311, 'eval_steps_per_second': 6.831, 'epoch': 1.0}\n",
      "{'loss': 1.5381, 'grad_norm': 4.530053615570068, 'learning_rate': 4.7981041883623604e-05, 'epoch': 1.07}\n",
      "{'loss': 1.5394, 'grad_norm': 6.36260461807251, 'learning_rate': 4.7774877735027944e-05, 'epoch': 1.15}\n",
      "{'loss': 1.5295, 'grad_norm': 6.978030681610107, 'learning_rate': 4.755312922401073e-05, 'epoch': 1.23}\n",
      "{'loss': 1.5172, 'grad_norm': 4.914771556854248, 'learning_rate': 4.731594546933761e-05, 'epoch': 1.31}\n",
      "{'loss': 1.5025, 'grad_norm': 5.194087505340576, 'learning_rate': 4.706348596948085e-05, 'epoch': 1.39}\n",
      "{'loss': 1.5084, 'grad_norm': 6.125194549560547, 'learning_rate': 4.679592049536167e-05, 'epoch': 1.48}\n",
      "{'loss': 1.5085, 'grad_norm': 7.808128356933594, 'learning_rate': 4.651342897618479e-05, 'epoch': 1.56}\n",
      "{'loss': 1.5049, 'grad_norm': 4.580046653747559, 'learning_rate': 4.621620137844178e-05, 'epoch': 1.64}\n",
      "{'loss': 1.4862, 'grad_norm': 4.204786777496338, 'learning_rate': 4.5904437578164695e-05, 'epoch': 1.72}\n",
      "{'loss': 1.4951, 'grad_norm': 3.663379430770874, 'learning_rate': 4.557834722651582e-05, 'epoch': 1.8}\n",
      "{'loss': 1.5045, 'grad_norm': 4.053482532501221, 'learning_rate': 4.523814960880405e-05, 'epoch': 1.89}\n",
      "{'loss': 1.5099, 'grad_norm': 5.835659503936768, 'learning_rate': 4.488407349702248e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f25782a504b899331e546a23599af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 83.064, 'eval_samples_per_second': 27.677, 'eval_steps_per_second': 6.922, 'epoch': 2.0}\n",
      "{'loss': 1.5065, 'grad_norm': 3.979177474975586, 'learning_rate': 4.4516356996006614e-05, 'epoch': 2.05}\n",
      "{'loss': 1.503, 'grad_norm': 4.245189666748047, 'learning_rate': 4.413602280000609e-05, 'epoch': 2.13}\n",
      "{'loss': 1.4982, 'grad_norm': 4.549670219421387, 'learning_rate': 4.374180237162288e-05, 'epoch': 2.21}\n",
      "{'loss': 1.473, 'grad_norm': 4.50761604309082, 'learning_rate': 4.3334709694722466e-05, 'epoch': 2.3}\n",
      "{'loss': 1.491, 'grad_norm': 5.555902004241943, 'learning_rate': 4.291501852608098e-05, 'epoch': 2.38}\n",
      "{'loss': 1.4926, 'grad_norm': 7.867809772491455, 'learning_rate': 4.2483887209316017e-05, 'epoch': 2.46}\n",
      "{'loss': 1.4895, 'grad_norm': 4.849914073944092, 'learning_rate': 4.2039877782223496e-05, 'epoch': 2.54}\n",
      "{'loss': 1.4876, 'grad_norm': 6.086119651794434, 'learning_rate': 4.158414059634607e-05, 'epoch': 2.62}\n",
      "{'loss': 1.4992, 'grad_norm': 2.9808876514434814, 'learning_rate': 4.11169821203329e-05, 'epoch': 2.71}\n",
      "{'loss': 1.4849, 'grad_norm': 4.292880535125732, 'learning_rate': 4.0639683906898305e-05, 'epoch': 2.79}\n",
      "{'loss': 1.4881, 'grad_norm': 4.190824508666992, 'learning_rate': 4.01506540123251e-05, 'epoch': 2.87}\n",
      "{'loss': 1.4926, 'grad_norm': 3.2148046493530273, 'learning_rate': 3.9651166801394444e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fa3b6c72744b06bb9f5e894ead8f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23140495867768596, 'eval_f1': 0.0997437610992934, 'eval_loss': 1.6093804836273193, 'eval_runtime': 82.8693, 'eval_samples_per_second': 27.742, 'eval_steps_per_second': 6.939, 'epoch': 3.0}\n",
      "{'loss': 1.4853, 'grad_norm': 4.019048690795898, 'learning_rate': 3.914155816324496e-05, 'epoch': 3.03}\n",
      "{'loss': 1.4793, 'grad_norm': 3.2914605140686035, 'learning_rate': 3.862217079335065e-05, 'epoch': 3.12}\n",
      "{'loss': 1.475, 'grad_norm': 4.0273590087890625, 'learning_rate': 3.809442077199905e-05, 'epoch': 3.2}\n",
      "{'loss': 1.4893, 'grad_norm': 4.321653366088867, 'learning_rate': 3.755654788239679e-05, 'epoch': 3.28}\n",
      "{'loss': 1.4797, 'grad_norm': 3.2571210861206055, 'learning_rate': 3.7009962129657125e-05, 'epoch': 3.36}\n",
      "{'loss': 1.4827, 'grad_norm': 4.648759841918945, 'learning_rate': 3.6455031075178315e-05, 'epoch': 3.44}\n",
      "{'loss': 1.4983, 'grad_norm': 4.272787094116211, 'learning_rate': 3.589326140412408e-05, 'epoch': 3.53}\n",
      "{'loss': 1.4997, 'grad_norm': 4.295674800872803, 'learning_rate': 3.5323927722127364e-05, 'epoch': 3.61}\n",
      "{'loss': 1.4933, 'grad_norm': 3.5752389430999756, 'learning_rate': 3.474624906104748e-05, 'epoch': 3.69}\n",
      "{'loss': 1.4925, 'grad_norm': 3.7566475868225098, 'learning_rate': 3.416174737270529e-05, 'epoch': 3.77}\n",
      "{'loss': 1.4591, 'grad_norm': 3.874237060546875, 'learning_rate': 3.3570815715750384e-05, 'epoch': 3.85}\n",
      "{'loss': 1.4979, 'grad_norm': 2.336552858352661, 'learning_rate': 3.297385147278015e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d03f59897844600b26c9230dc07521d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 83.0768, 'eval_samples_per_second': 27.673, 'eval_steps_per_second': 6.921, 'epoch': 4.0}\n",
      "{'loss': 1.4719, 'grad_norm': 2.996605634689331, 'learning_rate': 3.23712560831127e-05, 'epoch': 4.02}\n",
      "{'loss': 1.4712, 'grad_norm': 6.47822904586792, 'learning_rate': 3.1763434772831785e-05, 'epoch': 4.1}\n",
      "{'loss': 1.4711, 'grad_norm': 4.895840644836426, 'learning_rate': 3.1152026093494284e-05, 'epoch': 4.18}\n",
      "{'loss': 1.468, 'grad_norm': 5.00419807434082, 'learning_rate': 3.0534990799659266e-05, 'epoch': 4.26}\n",
      "{'loss': 1.5023, 'grad_norm': 6.500093460083008, 'learning_rate': 2.9913964414753657e-05, 'epoch': 4.35}\n",
      "{'loss': 1.4751, 'grad_norm': 3.58735728263855, 'learning_rate': 2.9289364559114705e-05, 'epoch': 4.43}\n",
      "{'loss': 1.4775, 'grad_norm': 3.1968586444854736, 'learning_rate': 2.86616112561242e-05, 'epoch': 4.51}\n",
      "{'loss': 1.4799, 'grad_norm': 4.51552152633667, 'learning_rate': 2.8031126649756197e-05, 'epoch': 4.59}\n",
      "{'loss': 1.4673, 'grad_norm': 3.224747896194458, 'learning_rate': 2.7398334720698713e-05, 'epoch': 4.67}\n",
      "{'loss': 1.4816, 'grad_norm': 6.086941242218018, 'learning_rate': 2.6763661001240303e-05, 'epoch': 4.76}\n",
      "{'loss': 1.4838, 'grad_norm': 2.096513271331787, 'learning_rate': 2.6127532289113274e-05, 'epoch': 4.84}\n",
      "{'loss': 1.4828, 'grad_norm': 2.8201773166656494, 'learning_rate': 2.549165141280316e-05, 'epoch': 4.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af96850a9fbd4463b807285a3edccdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 82.9285, 'eval_samples_per_second': 27.723, 'eval_steps_per_second': 6.934, 'epoch': 5.0}\n",
      "{'loss': 1.4922, 'grad_norm': 4.3420538902282715, 'learning_rate': 2.485389750418722e-05, 'epoch': 5.0}\n",
      "{'loss': 1.4729, 'grad_norm': 3.0050296783447266, 'learning_rate': 2.4217248592186236e-05, 'epoch': 5.08}\n",
      "{'loss': 1.4857, 'grad_norm': 2.575566053390503, 'learning_rate': 2.3579581247030347e-05, 'epoch': 5.17}\n",
      "{'loss': 1.482, 'grad_norm': 3.6931982040405273, 'learning_rate': 2.2942600100776998e-05, 'epoch': 5.25}\n",
      "{'loss': 1.4651, 'grad_norm': 4.2977800369262695, 'learning_rate': 2.2306733502829025e-05, 'epoch': 5.33}\n",
      "{'loss': 1.4858, 'grad_norm': 3.5150718688964844, 'learning_rate': 2.167367587920281e-05, 'epoch': 5.41}\n",
      "{'loss': 1.4576, 'grad_norm': 3.327179431915283, 'learning_rate': 2.1041315778235267e-05, 'epoch': 5.49}\n",
      "{'loss': 1.4876, 'grad_norm': 4.94452428817749, 'learning_rate': 2.0411348778335624e-05, 'epoch': 5.58}\n",
      "{'loss': 1.4845, 'grad_norm': 4.331723690032959, 'learning_rate': 1.9784198512118116e-05, 'epoch': 5.66}\n",
      "{'loss': 1.4861, 'grad_norm': 3.621962547302246, 'learning_rate': 1.9160286718033917e-05, 'epoch': 5.74}\n",
      "{'loss': 1.4713, 'grad_norm': 8.679520606994629, 'learning_rate': 1.854003295676545e-05, 'epoch': 5.82}\n",
      "{'loss': 1.4689, 'grad_norm': 2.9979472160339355, 'learning_rate': 1.7923854329085348e-05, 'epoch': 5.9}\n",
      "{'loss': 1.486, 'grad_norm': 7.279735088348389, 'learning_rate': 1.7312165195369553e-05, 'epoch': 5.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce7b7e8e96614ef58340bb35a611973a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 83.1284, 'eval_samples_per_second': 27.656, 'eval_steps_per_second': 6.917, 'epoch': 6.0}\n",
      "{'loss': 1.4649, 'grad_norm': 3.833265781402588, 'learning_rate': 1.670537689695318e-05, 'epoch': 6.07}\n",
      "{'loss': 1.4657, 'grad_norm': 3.4999027252197266, 'learning_rate': 1.6105094870624732e-05, 'epoch': 6.15}\n",
      "{'loss': 1.4825, 'grad_norm': 5.763112545013428, 'learning_rate': 1.5509316981925158e-05, 'epoch': 6.23}\n",
      "{'loss': 1.4832, 'grad_norm': 2.866100549697876, 'learning_rate': 1.491965228615947e-05, 'epoch': 6.31}\n",
      "{'loss': 1.4803, 'grad_norm': 2.8617475032806396, 'learning_rate': 1.4337656865767735e-05, 'epoch': 6.4}\n",
      "{'loss': 1.4757, 'grad_norm': 7.2106709480285645, 'learning_rate': 1.376138957802862e-05, 'epoch': 6.48}\n",
      "{'loss': 1.4663, 'grad_norm': 5.259767532348633, 'learning_rate': 1.3192410908373847e-05, 'epoch': 6.56}\n",
      "{'loss': 1.4785, 'grad_norm': 4.387902736663818, 'learning_rate': 1.2631103476720539e-05, 'epoch': 6.64}\n",
      "{'loss': 1.461, 'grad_norm': 5.7543840408325195, 'learning_rate': 1.207894298097188e-05, 'epoch': 6.72}\n",
      "{'loss': 1.4969, 'grad_norm': 3.725315570831299, 'learning_rate': 1.1534087787517397e-05, 'epoch': 6.81}\n",
      "{'loss': 1.4745, 'grad_norm': 3.377579689025879, 'learning_rate': 1.0998019001206713e-05, 'epoch': 6.89}\n",
      "{'loss': 1.4687, 'grad_norm': 2.346468687057495, 'learning_rate': 1.0471097111115277e-05, 'epoch': 6.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e502ce8474e442e8ba5517f922c9bdc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23183993040452372, 'eval_f1': 0.10354681077762767, 'eval_loss': 1.6093758344650269, 'eval_runtime': 83.0025, 'eval_samples_per_second': 27.698, 'eval_steps_per_second': 6.928, 'epoch': 7.0}\n",
      "{'loss': 1.4672, 'grad_norm': 4.07132625579834, 'learning_rate': 9.95367645532406e-06, 'epoch': 7.05}\n",
      "{'loss': 1.4828, 'grad_norm': 4.484358787536621, 'learning_rate': 9.44610498263851e-06, 'epoch': 7.13}\n",
      "{'loss': 1.455, 'grad_norm': 5.40916109085083, 'learning_rate': 8.94872401860413e-06, 'epoch': 7.22}\n",
      "{'loss': 1.4818, 'grad_norm': 3.5796735286712646, 'learning_rate': 8.461868035976024e-06, 'epoch': 7.3}\n",
      "{'loss': 1.4889, 'grad_norm': 7.282806873321533, 'learning_rate': 7.986805392342239e-06, 'epoch': 7.38}\n",
      "{'loss': 1.4453, 'grad_norm': 3.1540772914886475, 'learning_rate': 7.522869949373102e-06, 'epoch': 7.46}\n",
      "{'loss': 1.464, 'grad_norm': 3.1888012886047363, 'learning_rate': 7.069477321731563e-06, 'epoch': 7.54}\n",
      "{'loss': 1.4721, 'grad_norm': 4.016075611114502, 'learning_rate': 6.627873408661936e-06, 'epoch': 7.63}\n",
      "{'loss': 1.466, 'grad_norm': 2.3176751136779785, 'learning_rate': 6.19835517464046e-06, 'epoch': 7.71}\n",
      "{'loss': 1.4775, 'grad_norm': 4.624432563781738, 'learning_rate': 5.782033206725376e-06, 'epoch': 7.79}\n",
      "{'loss': 1.4929, 'grad_norm': 4.270590305328369, 'learning_rate': 5.3775189366746675e-06, 'epoch': 7.87}\n",
      "{'loss': 1.466, 'grad_norm': 4.205439567565918, 'learning_rate': 4.985931168975241e-06, 'epoch': 7.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392cb859ee3a4d8fae5aba2b96e6f97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 83.0326, 'eval_samples_per_second': 27.688, 'eval_steps_per_second': 6.925, 'epoch': 8.0}\n",
      "{'loss': 1.4533, 'grad_norm': 3.5164966583251953, 'learning_rate': 4.607533233848777e-06, 'epoch': 8.04}\n",
      "{'loss': 1.477, 'grad_norm': 3.4276087284088135, 'learning_rate': 4.2425795917773645e-06, 'epoch': 8.12}\n",
      "{'loss': 1.4612, 'grad_norm': 6.394713401794434, 'learning_rate': 3.891315662386942e-06, 'epoch': 8.2}\n",
      "{'loss': 1.4453, 'grad_norm': 4.075328826904297, 'learning_rate': 3.5539776594105037e-06, 'epoch': 8.28}\n",
      "{'loss': 1.4689, 'grad_norm': 3.991068124771118, 'learning_rate': 3.230792431841896e-06, 'epoch': 8.36}\n",
      "{'loss': 1.4974, 'grad_norm': 4.339132785797119, 'learning_rate': 2.9219773113871763e-06, 'epoch': 8.45}\n",
      "{'loss': 1.4617, 'grad_norm': 7.477202415466309, 'learning_rate': 2.628313759225937e-06, 'epoch': 8.53}\n",
      "{'loss': 1.455, 'grad_norm': 3.3329036235809326, 'learning_rate': 2.348822312553423e-06, 'epoch': 8.61}\n",
      "{'loss': 1.4795, 'grad_norm': 3.4248876571655273, 'learning_rate': 2.0848080729143932e-06, 'epoch': 8.69}\n",
      "{'loss': 1.4632, 'grad_norm': 3.5132253170013428, 'learning_rate': 1.8353904675385906e-06, 'epoch': 8.77}\n",
      "{'loss': 1.4626, 'grad_norm': 2.535428762435913, 'learning_rate': 1.6012813323689981e-06, 'epoch': 8.86}\n",
      "{'loss': 1.4693, 'grad_norm': 4.5602545738220215, 'learning_rate': 1.3826380982947946e-06, 'epoch': 8.94}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d30381a94934c6d967cbb5cf0645468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.6093724966049194, 'eval_runtime': 83.0172, 'eval_samples_per_second': 27.693, 'eval_steps_per_second': 6.926, 'epoch': 9.0}\n",
      "{'loss': 1.4614, 'grad_norm': 4.2972731590271, 'learning_rate': 1.1799981822759164e-06, 'epoch': 9.02}\n",
      "{'loss': 1.4505, 'grad_norm': 4.579303741455078, 'learning_rate': 9.926857147363265e-07, 'epoch': 9.1}\n",
      "{'loss': 1.4744, 'grad_norm': 2.0948286056518555, 'learning_rate': 8.212484093368643e-07, 'epoch': 9.18}\n",
      "{'loss': 1.4763, 'grad_norm': 2.1727755069732666, 'learning_rate': 6.658015521700958e-07, 'epoch': 9.27}\n",
      "{'loss': 1.4798, 'grad_norm': 4.078769683837891, 'learning_rate': 5.264496762647245e-07, 'epoch': 9.35}\n",
      "{'loss': 1.4521, 'grad_norm': 5.194500923156738, 'learning_rate': 4.0328649129046553e-07, 'epoch': 9.43}\n",
      "{'loss': 1.4733, 'grad_norm': 4.355737686157227, 'learning_rate': 2.9639482054129713e-07, 'epoch': 9.51}\n",
      "{'loss': 1.4531, 'grad_norm': 2.2465782165527344, 'learning_rate': 2.0601128877500422e-07, 'epoch': 9.59}\n",
      "{'loss': 1.4717, 'grad_norm': 3.7174625396728516, 'learning_rate': 1.3183443771773224e-07, 'epoch': 9.68}\n",
      "{'loss': 1.4652, 'grad_norm': 2.2173144817352295, 'learning_rate': 7.411164369036499e-08, 'epoch': 9.76}\n",
      "{'loss': 1.4634, 'grad_norm': 3.692753314971924, 'learning_rate': 3.288172342162399e-08, 'epoch': 9.84}\n",
      "{'loss': 1.4653, 'grad_norm': 2.8942439556121826, 'learning_rate': 8.172402711287496e-09, 'epoch': 9.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b4b9a1fa9a440a8c4374c4d9fec317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': 0.23053501522401043, 'eval_f1': 0.08637932701924746, 'eval_loss': 1.609375, 'eval_runtime': 82.9839, 'eval_samples_per_second': 27.704, 'eval_steps_per_second': 6.929, 'epoch': 10.0}\n",
      "{'train_runtime': 23204.0495, 'train_samples_per_second': 7.882, 'train_steps_per_second': 2.628, 'train_loss': 1.4887918260501452, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60970, training_loss=1.4887918260501452, metrics={'train_runtime': 23204.0495, 'train_samples_per_second': 7.882, 'train_steps_per_second': 2.628, 'total_flos': 2.40617214893568e+17, 'train_loss': 1.4887918260501452, 'epoch': 10.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./squad-trained-model\"\n",
    "model =  (path)\n",
    "# Create the Trainer\n",
    "squad_trainer = create_trainer(run_name=\"Squad-Run2\", batch_size=3, num_train_epochs=10)\n",
    "\n",
    "# Train the model\n",
    "squad_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f14c4e-6eeb-4757-97d1-bf3c1cd300e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de07d417e5b349aab4e694941bf8ed28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.2190\n",
      "F1 Score: 0.0898\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_results = squad_trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "772a934d-4ad5-459c-8f36-3afc382bdabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OEM\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "wandb: Currently logged in as: mzak071 (COMPSCI714). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\OEM\\Notebooks\\COMPSCI764\\Project\\wandb\\run-20241015_215034-xfygdyim</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/COMPSCI714/huggingface/runs/xfygdyim' target=\"_blank\">./MCQA-Combined/Squad-Run2/microsoft-deberta-v3-base</a></strong> to <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/COMPSCI714/huggingface' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/COMPSCI714/huggingface/runs/xfygdyim' target=\"_blank\">https://wandb.ai/COMPSCI714/huggingface/runs/xfygdyim</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "Accuracy: 0.2873\n",
      "F1 Score: 0.2872\n"
     ]
    }
   ],
   "source": [
    "path = \"./MCQA-Combined/Squad-Run/microsoft-deberta-v3-base/checkpoint-18291\"\n",
    "model =  AutoModelForMultipleChoice.from_pretrained(path)\n",
    "# Create the Trainer\n",
    "squad_trainer = create_trainer(run_name=\"Squad-Run2\", batch_size=3)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = squad_trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4cf0ff6-3794-4543-baab-3073e3b0341f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='18291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  102/18291 00:40 < 2:04:11, 2.44 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43msquad_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_results \u001b[38;5;241m=\u001b[39m squad_trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mget_test_encoded())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\transformers\\trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\accelerate\\accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\compsci714win\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "squad_trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = squad_trainer.evaluate(eval_dataset=get_test_encoded())\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb9f12-d022-4590-af99-611fc317aeaf",
   "metadata": {},
   "source": [
    "# End of NoteBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521a6a10-fee5-4be2-b061-29591435f60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compsci714win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
